[
    {
        "file_name": "code_review.py",
        "diff_content": "@@ -0,0 +1,229 @@\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n+\n+PRIVATE_TOKEN = os.getenv(\"GITLAB_TOKEN\")\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+HEADERS = {\n+    \"PRIVATE-TOKEN\": PRIVATE_TOKEN\n+}\n+\n+CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n+TIKTOKEN_ENCODING = tiktoken.encoding_for_model(TIKTOKEN_MODEL)\n+\n+def load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n+    \"\"\"\n+    レビュー観点をJSONファイルから読み込む。\n+\n+    Args:\n+        file_path (str): レビュー観点が記載されたJSONファイルのパス。\n+\n+    Returns:\n+        list: レビュー観点のリスト。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+def load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n+    \"\"\"\n+    システムプロンプトをテキストファイルから読み込む。\n+\n+    Args:\n+        file_path (str): システムプロンプトが記載されたテキストファイルのパス。\n+\n+    Returns:\n+        str: システムプロンプトの内容。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return f.read()\n+\n+def num_tokens_from_messages(messages) -> int:\n+    \"\"\"\n+    メッセージからトークン数を計算する。\n+\n+    Args:\n+        messages (list): メッセージのリスト。\n+\n+    Returns:\n+        int: メッセージに含まれるトークン数。\n+    \"\"\"\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += TOKENS_PER_MESSAGE\n+        for key, value in message.items():\n+            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n+            if key == \"name\":\n+                num_tokens += TOKENS_PER_NAME\n+    num_tokens += 3  # assistantの返信の開始トークン\n+    return num_tokens\n+\n+def _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n+    \"\"\"\n+    OpenAIのChat APIを呼び出す関数。\n+\n+    Args:\n+        messages (list): APIに送信するメッセージリスト。\n+        model (str): 使用するモデル名。\n+        temperature (float): 出力の多様性を制御する温度パラメータ。\n+        max_tokens (int): 最大トークン数。\n+\n+    Returns:\n+        str: OpenAI APIからのレスポンス。\n+    \"\"\"\n+    response = CLIENT.chat.completions.create(\n+        model=model,\n+        messages=messages,\n+        temperature=temperature,\n+        max_tokens=max_tokens\n+    )\n+    return response.choices[0].message.content\n+\n+def review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n+    \"\"\"\n+    指定された観点でコードレビューを実行し、結果をログに記録する。\n+\n+    Args:\n+        review_perspective (str): レビュー観点。\n+        file_name (str): 対象ファイル名。\n+        diff_info (str): 差分情報。\n+        log_file (str): ログを保存するファイル名。\n+\n+    Returns:\n+        str: レビュー結果。\n+    \"\"\"\n+    # ファイル名と差分情報でシステムプロンプトをフォーマット\n+    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n+    messages = [\n+        {\"role\": \"system\", \"content\": current_system_prompt},\n+        {\"role\": \"user\", \"content\": review_perspective}\n+    ]\n+\n+    start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+    # OpenAI APIを呼び出してレビュー結果を取得\n+    response_str = _call_openai_chat_api(\n+        messages=messages,\n+        model=MODEL_NAME,\n+        temperature=TEMPERATURE,\n+    )\n+    end_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+\n+    log_data = {\n+        \"file_name\": file_name,\n+        \"diff_content\": diff_info,\n+        \"review_perspective\": review_perspective,\n+        \"request_tokens\": num_tokens_from_messages(messages),\n+        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n+        \"response\": response_str,\n+        \"response_tokens\": num_tokens_from_messages(response_str),\n+        \"start_time\": start_time,\n+        \"end_time\": end_time,\n+    }\n+\n+    # 既存のログデータをロード（なければ空リストを使う）\n+    if os.path.exists(log_file):\n+        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n+            try:\n+                existing_logs = json.load(f)\n+            except json.JSONDecodeError:\n+                existing_logs = []\n+    else:\n+        existing_logs = []\n+\n+    # 新しいログデータを追加\n+    existing_logs.append(log_data)\n+    \n+    # ログをJSON形式で保存\n+    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n+        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n+    return response_str\n+\n+def get_mr_info(mr_iid) -> list:\n+    \"\"\"\n+    GitLabのマージリクエスト情報を取得する。\n+\n+    Args:\n+        mr_iid (int): マージリクエストのIID。\n+\n+    Returns:\n+        list: マージリクエストの差分情報リスト。\n+    \"\"\"\n+    base = f\"{GITLAB_URL}/api/v4\"\n+    # 1. MR基本情報\n+    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n+    # 2. 差分情報（Diff）\n+    #    MRの差分情報を取得するエンドポイント\n+    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n+\n+    all_diffs = []\n+    current_page = 1\n+\n+    while True:\n+        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n+        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n+        try:\n+            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n+        except requests.exceptions.HTTPError as e:\n+            print(f\"APIリクエストに失敗しました: {e}\")\n+            break \n+        try:\n+            page_data = response.json()\n+        except requests.exceptions.JSONDecodeError:\n+            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n+            break\n+        if not isinstance(page_data, list):\n+            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n+            break\n+        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n+            break\n+\n+        all_diffs.extend(page_data)\n+\n+        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n+        total_pages_header = response.headers.get(\"x-total-pages\")\n+        if total_pages_header and current_page >= int(total_pages_header):\n+            break # 最後のページまで到達\n+        current_page += 1\n+    return all_diffs\n+\n+def main():\n+    \"\"\"\n+    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n+    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n+    \"\"\"\n+    # コマンドライン引数を解析\n+    parser = argparse.ArgumentParser(description=\"GitLab MR情報を取得してコードレビューを実行します。\")\n+    parser.add_argument(\"--mr_iid\", type=int, required=True, help=\"マージリクエストのIIDを指定してください。\")\n+    args = parser.parse_args()\n+\n+    # GitLab MR情報を取得\n+    diffs = get_mr_info(args.mr_iid)\n+    # レビュー観点をロード\n+    review_perspectives = load_review_perspectives()\n+    \n+    # 出力ファイルの末尾に付与する現在時刻を取得してフォーマット\n+    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n+    # レビュー結果出力ファイル名とログファイル名を生成\n+    output_file = f\"review_results_{timestamp}.md\"\n+    log_file = f\"api_call_logs_{timestamp}.json\"\n+    \n+    # 各DIFFに対してレビューを実行\n+    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n+        for diff in diffs:\n+            file_name = diff['new_path']\n+            diff_info = diff['diff']\n+            if not diff_info:\n+                continue\n+            f.write(f\"# レビュー対象：{file_name}\\n\")\n+            for perspective in review_perspectives:\n+                f.write(f\"## レビュー観点：{perspective}\\n\")\n+                review_result = review_code(perspective, file_name, diff_info, log_file)\n+                f.write(f\"{review_result}\\n\")\n+\n+if __name__ == \"__main__\":\n+    main()\n",
        "review_perspective": "PEP8に準拠しているか（インデント、空白、命名規則）",
        "request_tokens": 3194,
        "api_params": {
            "model": "gpt-4.1-nano",
            "temperature": 0.2
        },
        "response": "### 指摘対象コード:\n```python\n+def load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n+    \"\"\"\n+    レビュー観点をJSONファイルから読み込む。\n+\n+    Args:\n+        file_path (str): レビュー観点が記載されたJSONファイルのパス。\n+\n+    Returns:\n+        list: レビュー観点のリスト。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+def load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n+    \"\"\"\n+    システムプロンプトをテキストファイルから読み込む。\n+\n+    Args:\n+        file_path (str): システムプロンプトが記載されたテキストファイルのパス。\n+\n+    Returns:\n+        str: システムプロンプトの内容。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return f.read()\n+\n+def num_tokens_from_messages(messages) -> int:\n+    \"\"\"\n+    メッセージからトークン数を計算する。\n+\n+    Args:\n+        messages (list): メッセージのリスト。\n+\n+    Returns:\n+        int: メッセージに含まれるトークン数。\n+    \"\"\"\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += TOKENS_PER_MESSAGE\n+        for key, value in message.items():\n+            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n+            if key == \"name\":\n+                num_tokens += TOKENS_PER_NAME\n+    num_tokens += 3  # assistantの返信の開始トークン\n+    return num_tokens\n+\n+def _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n+    \"\"\"\n+    OpenAIのChat APIを呼び出す関数。\n+\n+    Args:\n+        messages (list): APIに送信するメッセージリスト。\n+        model (str): 使用するモデル名。\n+        temperature (float): 出力の多様性を制御する温度パラメータ。\n+        max_tokens (int): 最大トークン数。\n+\n+    Returns:\n+        str: OpenAI APIからのレスポンス。\n+    \"\"\"\n+    response = CLIENT.chat.completions.create(\n+        model=model,\n+        messages=messages,\n+        temperature=temperature,\n+        max_tokens=max_tokens\n+    )\n+    return response.choices[0].message.content\n+\n+def review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n+    \"\"\"\n+    指定された観点でコードレビューを実行し、結果をログに記録する。\n+\n+    Args:\n+        review_perspective (str): レビュー観点。\n+        file_name (str): 対象ファイル名。\n+        diff_info (str): 差分情報。\n+        log_file (str): ログを保存するファイル名。\n+\n+    Returns:\n+        str: レビュー結果。\n+    \"\"\"\n+    # ファイル名と差分情報でシステムプロンプトをフォーマット\n+    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n+    messages = [\n+        {\"role\": \"system\", \"content\": current_system_prompt},\n+        {\"role\": \"user\", \"content\": review_perspective}\n+    ]\n+\n+    start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+    # OpenAI APIを呼び出してレビュー結果を取得\n+    response_str = _call_openai_chat_api(\n+        messages=messages,\n+        model=MODEL_NAME,\n+        temperature=TEMPERATURE,\n+    )\n+    end_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+\n+    log_data = {\n+        \"file_name\": file_name,\n+        \"diff_content\": diff_info,\n+        \"review_perspective\": review_perspective,\n+        \"request_tokens\": num_tokens_from_messages(messages),\n+        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n+        \"response\": response_str,\n+        \"response_tokens\": num_tokens_from_messages(response_str),\n+        \"start_time\": start_time,\n+        \"end_time\": end_time,\n+    }\n+\n+    # 既存のログデータをロード（なければ空リストを使う）\n+    if os.path.exists(log_file):\n+        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n+            try:\n+                existing_logs = json.load(f)\n+            except json.JSONDecodeError:\n+                existing_logs = []\n+    else:\n+        existing_logs = []\n+\n+    # 新しいログデータを追加\n+    existing_logs.append(log_data)\n+    \n+    # ログをJSON形式で保存\n+    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n+        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n+    return response_str\n+\n+def get_mr_info(mr_iid) -> list:\n+    \"\"\"\n+    GitLabのマージリクエスト情報を取得する。\n+\n+    Args:\n+        mr_iid (int): マージリクエストのIID。\n+\n+    Returns:\n+        list: マージリクエストの差分情報リスト。\n+    \"\"\"\n+    base = f\"{GITLAB_URL}/api/v4\"\n+    # 1. MR基本情報\n+    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n+    # 2. 差分情報（Diff）\n+    #    MRの差分情報を取得するエンドポイント\n+    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n+\n+    all_diffs = []\n+    current_page = 1\n+\n+    while True:\n+        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n+        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n+        try:\n+            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n+        except requests.exceptions.HTTPError as e:\n+            print(f\"APIリクエストに失敗しました: {e}\")\n+            break \n+        try:\n+            page_data = response.json()\n+        except requests.exceptions.JSONDecodeError:\n+            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n+            break\n+        if not isinstance(page_data, list):\n+            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n+            break\n+        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n+            break\n+\n+        all_diffs.extend(page_data)\n+\n+        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n+        total_pages_header = response.headers.get(\"x-total-pages\")\n+        if total_pages_header and current_page >= int(total_pages_header):\n+            break # 最後のページまで到達\n+        current_page += 1\n+    return all_diffs\n+\n+def main():\n+    \"\"\"\n+    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n+    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n+    \"\"\"\n+    # コマンドライン引数を解析\n+    parser = argparse.ArgumentParser(description=\"GitLab MR情報を取得してコードレビューを実行します。\")\n+    parser.add_argument(\"--mr_iid\", type=int, required=True, help=\"マージリクエストのIIDを指定してください。\")\n+    args = parser.parse_args()\n+\n+    # GitLab MR情報を取得\n+    diffs = get_mr_info(args.mr_iid)\n+    # レビュー観点をロード\n+    review_perspectives = load_review_perspectives()\n+    \n+    # 出力ファイルの末尾に付与する現在時刻を取得してフォーマット\n+    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n+    # レビュー結果出力ファイル名とログファイル名を生成\n+    output_file = f\"review_results_{timestamp}.md\"\n+    log_file = f\"api_call_logs_{timestamp}.json\"\n+    \n+    # 各DIFFに対してレビューを実行\n+    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n+        for diff in diffs:\n+            file_name = diff['new_path']\n+            diff_info = diff['diff']\n+            if not diff_info:\n+                continue\n+            f.write(f\"# レビュー対象：{file_name}\\n\")\n+            for perspective in review_perspectives:\n+                f.write(f\"## レビュー観点：{perspective}\\n\")\n+                review_result = review_code(perspective, file_name, diff_info, log_file)\n+                f.write(f\"{review_result}\\n\")\n+\n+if __name__ == \"__main__\":\n+    main()\n```\n\n### 指摘内容:\n- 変数名や関数名の命名規則はPEP8に準拠しているが、インデントや空白の使い方に一部改善の余地がある。\n\n### 修正提案:\n- 例えば、関数の引数の型アノテーションと関数の間に空白を入れるのが一般的です（例：`def get_mr_info(mr_iid: int) -> list:`）。\n- `messages`リスト内の辞書のキーと値の間に空白を入れるとよりPEP8に沿います（例：`{\"role\": \"system\", \"content\": current_system_prompt}`）。\n- `if not diff_info:`のインデントや空白は問題ありませんが、全体的に空白やインデントはPEP8に準拠しています。\n\n---\n\nこのコードはPEP8にほぼ準拠していますが、型アノテーションと空白の使い方を少し調整すればより良くなるでしょう。",
        "response_tokens": 2677,
        "start_time": "2025/05/26 04:47:54.398061",
        "end_time": "2025/05/26 04:48:16.305772"
    },
    {
        "file_name": "code_review.py",
        "diff_content": "@@ -0,0 +1,229 @@\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n+\n+PRIVATE_TOKEN = os.getenv(\"GITLAB_TOKEN\")\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+HEADERS = {\n+    \"PRIVATE-TOKEN\": PRIVATE_TOKEN\n+}\n+\n+CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n+TIKTOKEN_ENCODING = tiktoken.encoding_for_model(TIKTOKEN_MODEL)\n+\n+def load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n+    \"\"\"\n+    レビュー観点をJSONファイルから読み込む。\n+\n+    Args:\n+        file_path (str): レビュー観点が記載されたJSONファイルのパス。\n+\n+    Returns:\n+        list: レビュー観点のリスト。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+def load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n+    \"\"\"\n+    システムプロンプトをテキストファイルから読み込む。\n+\n+    Args:\n+        file_path (str): システムプロンプトが記載されたテキストファイルのパス。\n+\n+    Returns:\n+        str: システムプロンプトの内容。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return f.read()\n+\n+def num_tokens_from_messages(messages) -> int:\n+    \"\"\"\n+    メッセージからトークン数を計算する。\n+\n+    Args:\n+        messages (list): メッセージのリスト。\n+\n+    Returns:\n+        int: メッセージに含まれるトークン数。\n+    \"\"\"\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += TOKENS_PER_MESSAGE\n+        for key, value in message.items():\n+            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n+            if key == \"name\":\n+                num_tokens += TOKENS_PER_NAME\n+    num_tokens += 3  # assistantの返信の開始トークン\n+    return num_tokens\n+\n+def _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n+    \"\"\"\n+    OpenAIのChat APIを呼び出す関数。\n+\n+    Args:\n+        messages (list): APIに送信するメッセージリスト。\n+        model (str): 使用するモデル名。\n+        temperature (float): 出力の多様性を制御する温度パラメータ。\n+        max_tokens (int): 最大トークン数。\n+\n+    Returns:\n+        str: OpenAI APIからのレスポンス。\n+    \"\"\"\n+    response = CLIENT.chat.completions.create(\n+        model=model,\n+        messages=messages,\n+        temperature=temperature,\n+        max_tokens=max_tokens\n+    )\n+    return response.choices[0].message.content\n+\n+def review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n+    \"\"\"\n+    指定された観点でコードレビューを実行し、結果をログに記録する。\n+\n+    Args:\n+        review_perspective (str): レビュー観点。\n+        file_name (str): 対象ファイル名。\n+        diff_info (str): 差分情報。\n+        log_file (str): ログを保存するファイル名。\n+\n+    Returns:\n+        str: レビュー結果。\n+    \"\"\"\n+    # ファイル名と差分情報でシステムプロンプトをフォーマット\n+    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n+    messages = [\n+        {\"role\": \"system\", \"content\": current_system_prompt},\n+        {\"role\": \"user\", \"content\": review_perspective}\n+    ]\n+\n+    start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+    # OpenAI APIを呼び出してレビュー結果を取得\n+    response_str = _call_openai_chat_api(\n+        messages=messages,\n+        model=MODEL_NAME,\n+        temperature=TEMPERATURE,\n+    )\n+    end_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+\n+    log_data = {\n+        \"file_name\": file_name,\n+        \"diff_content\": diff_info,\n+        \"review_perspective\": review_perspective,\n+        \"request_tokens\": num_tokens_from_messages(messages),\n+        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n+        \"response\": response_str,\n+        \"response_tokens\": num_tokens_from_messages(response_str),\n+        \"start_time\": start_time,\n+        \"end_time\": end_time,\n+    }\n+\n+    # 既存のログデータをロード（なければ空リストを使う）\n+    if os.path.exists(log_file):\n+        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n+            try:\n+                existing_logs = json.load(f)\n+            except json.JSONDecodeError:\n+                existing_logs = []\n+    else:\n+        existing_logs = []\n+\n+    # 新しいログデータを追加\n+    existing_logs.append(log_data)\n+    \n+    # ログをJSON形式で保存\n+    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n+        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n+    return response_str\n+\n+def get_mr_info(mr_iid) -> list:\n+    \"\"\"\n+    GitLabのマージリクエスト情報を取得する。\n+\n+    Args:\n+        mr_iid (int): マージリクエストのIID。\n+\n+    Returns:\n+        list: マージリクエストの差分情報リスト。\n+    \"\"\"\n+    base = f\"{GITLAB_URL}/api/v4\"\n+    # 1. MR基本情報\n+    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n+    # 2. 差分情報（Diff）\n+    #    MRの差分情報を取得するエンドポイント\n+    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n+\n+    all_diffs = []\n+    current_page = 1\n+\n+    while True:\n+        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n+        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n+        try:\n+            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n+        except requests.exceptions.HTTPError as e:\n+            print(f\"APIリクエストに失敗しました: {e}\")\n+            break \n+        try:\n+            page_data = response.json()\n+        except requests.exceptions.JSONDecodeError:\n+            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n+            break\n+        if not isinstance(page_data, list):\n+            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n+            break\n+        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n+            break\n+\n+        all_diffs.extend(page_data)\n+\n+        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n+        total_pages_header = response.headers.get(\"x-total-pages\")\n+        if total_pages_header and current_page >= int(total_pages_header):\n+            break # 最後のページまで到達\n+        current_page += 1\n+    return all_diffs\n+\n+def main():\n+    \"\"\"\n+    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n+    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n+    \"\"\"\n+    # コマンドライン引数を解析\n+    parser = argparse.ArgumentParser(description=\"GitLab MR情報を取得してコードレビューを実行します。\")\n+    parser.add_argument(\"--mr_iid\", type=int, required=True, help=\"マージリクエストのIIDを指定してください。\")\n+    args = parser.parse_args()\n+\n+    # GitLab MR情報を取得\n+    diffs = get_mr_info(args.mr_iid)\n+    # レビュー観点をロード\n+    review_perspectives = load_review_perspectives()\n+    \n+    # 出力ファイルの末尾に付与する現在時刻を取得してフォーマット\n+    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n+    # レビュー結果出力ファイル名とログファイル名を生成\n+    output_file = f\"review_results_{timestamp}.md\"\n+    log_file = f\"api_call_logs_{timestamp}.json\"\n+    \n+    # 各DIFFに対してレビューを実行\n+    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n+        for diff in diffs:\n+            file_name = diff['new_path']\n+            diff_info = diff['diff']\n+            if not diff_info:\n+                continue\n+            f.write(f\"# レビュー対象：{file_name}\\n\")\n+            for perspective in review_perspectives:\n+                f.write(f\"## レビュー観点：{perspective}\\n\")\n+                review_result = review_code(perspective, file_name, diff_info, log_file)\n+                f.write(f\"{review_result}\\n\")\n+\n+if __name__ == \"__main__\":\n+    main()\n",
        "review_perspective": "変数名・関数名が意味を持っているか",
        "request_tokens": 3184,
        "api_params": {
            "model": "gpt-4.1-nano",
            "temperature": 0.2
        },
        "response": "### 指摘対象コード:\n```python\ndef load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n# ...\ndef load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n# ...\ndef num_tokens_from_messages(messages) -> int:\n# ...\ndef _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n# ...\ndef review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n# ...\ndef get_mr_info(mr_iid) -> list:\n# ...\n```\n\n### 指摘内容:\n関数名や変数名は、処理内容や役割を適切に表現しており、全体的に意味が明確です。ただし、「_call_openai_chat_api」のプレフィックスのアンダースコアは、内部使用を示唆しているため、外部から呼び出す場合は適切ですが、公開APIとして使う場合は少し曖昧になる可能性があります。\n\n### 修正提案:\n特に問題はありませんが、より明確にしたい場合は、「call_openai_chat_api」などの命名に変更しても良いでしょう。ただし、現状の命名も十分に意味が伝わるため、必要に応じて検討してください。",
        "response_tokens": 370,
        "start_time": "2025/05/26 04:48:16.313016",
        "end_time": "2025/05/26 04:48:19.293387"
    },
    {
        "file_name": "code_review.py",
        "diff_content": "@@ -0,0 +1,229 @@\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n+\n+PRIVATE_TOKEN = os.getenv(\"GITLAB_TOKEN\")\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+HEADERS = {\n+    \"PRIVATE-TOKEN\": PRIVATE_TOKEN\n+}\n+\n+CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n+TIKTOKEN_ENCODING = tiktoken.encoding_for_model(TIKTOKEN_MODEL)\n+\n+def load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n+    \"\"\"\n+    レビュー観点をJSONファイルから読み込む。\n+\n+    Args:\n+        file_path (str): レビュー観点が記載されたJSONファイルのパス。\n+\n+    Returns:\n+        list: レビュー観点のリスト。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+def load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n+    \"\"\"\n+    システムプロンプトをテキストファイルから読み込む。\n+\n+    Args:\n+        file_path (str): システムプロンプトが記載されたテキストファイルのパス。\n+\n+    Returns:\n+        str: システムプロンプトの内容。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return f.read()\n+\n+def num_tokens_from_messages(messages) -> int:\n+    \"\"\"\n+    メッセージからトークン数を計算する。\n+\n+    Args:\n+        messages (list): メッセージのリスト。\n+\n+    Returns:\n+        int: メッセージに含まれるトークン数。\n+    \"\"\"\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += TOKENS_PER_MESSAGE\n+        for key, value in message.items():\n+            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n+            if key == \"name\":\n+                num_tokens += TOKENS_PER_NAME\n+    num_tokens += 3  # assistantの返信の開始トークン\n+    return num_tokens\n+\n+def _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n+    \"\"\"\n+    OpenAIのChat APIを呼び出す関数。\n+\n+    Args:\n+        messages (list): APIに送信するメッセージリスト。\n+        model (str): 使用するモデル名。\n+        temperature (float): 出力の多様性を制御する温度パラメータ。\n+        max_tokens (int): 最大トークン数。\n+\n+    Returns:\n+        str: OpenAI APIからのレスポンス。\n+    \"\"\"\n+    response = CLIENT.chat.completions.create(\n+        model=model,\n+        messages=messages,\n+        temperature=temperature,\n+        max_tokens=max_tokens\n+    )\n+    return response.choices[0].message.content\n+\n+def review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n+    \"\"\"\n+    指定された観点でコードレビューを実行し、結果をログに記録する。\n+\n+    Args:\n+        review_perspective (str): レビュー観点。\n+        file_name (str): 対象ファイル名。\n+        diff_info (str): 差分情報。\n+        log_file (str): ログを保存するファイル名。\n+\n+    Returns:\n+        str: レビュー結果。\n+    \"\"\"\n+    # ファイル名と差分情報でシステムプロンプトをフォーマット\n+    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n+    messages = [\n+        {\"role\": \"system\", \"content\": current_system_prompt},\n+        {\"role\": \"user\", \"content\": review_perspective}\n+    ]\n+\n+    start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+    # OpenAI APIを呼び出してレビュー結果を取得\n+    response_str = _call_openai_chat_api(\n+        messages=messages,\n+        model=MODEL_NAME,\n+        temperature=TEMPERATURE,\n+    )\n+    end_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+\n+    log_data = {\n+        \"file_name\": file_name,\n+        \"diff_content\": diff_info,\n+        \"review_perspective\": review_perspective,\n+        \"request_tokens\": num_tokens_from_messages(messages),\n+        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n+        \"response\": response_str,\n+        \"response_tokens\": num_tokens_from_messages(response_str),\n+        \"start_time\": start_time,\n+        \"end_time\": end_time,\n+    }\n+\n+    # 既存のログデータをロード（なければ空リストを使う）\n+    if os.path.exists(log_file):\n+        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n+            try:\n+                existing_logs = json.load(f)\n+            except json.JSONDecodeError:\n+                existing_logs = []\n+    else:\n+        existing_logs = []\n+\n+    # 新しいログデータを追加\n+    existing_logs.append(log_data)\n+    \n+    # ログをJSON形式で保存\n+    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n+        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n+    return response_str\n+\n+def get_mr_info(mr_iid) -> list:\n+    \"\"\"\n+    GitLabのマージリクエスト情報を取得する。\n+\n+    Args:\n+        mr_iid (int): マージリクエストのIID。\n+\n+    Returns:\n+        list: マージリクエストの差分情報リスト。\n+    \"\"\"\n+    base = f\"{GITLAB_URL}/api/v4\"\n+    # 1. MR基本情報\n+    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n+    # 2. 差分情報（Diff）\n+    #    MRの差分情報を取得するエンドポイント\n+    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n+\n+    all_diffs = []\n+    current_page = 1\n+\n+    while True:\n+        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n+        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n+        try:\n+            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n+        except requests.exceptions.HTTPError as e:\n+            print(f\"APIリクエストに失敗しました: {e}\")\n+            break \n+        try:\n+            page_data = response.json()\n+        except requests.exceptions.JSONDecodeError:\n+            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n+            break\n+        if not isinstance(page_data, list):\n+            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n+            break\n+        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n+            break\n+\n+        all_diffs.extend(page_data)\n+\n+        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n+        total_pages_header = response.headers.get(\"x-total-pages\")\n+        if total_pages_header and current_page >= int(total_pages_header):\n+            break # 最後のページまで到達\n+        current_page += 1\n+    return all_diffs\n+\n+def main():\n+    \"\"\"\n+    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n+    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n+    \"\"\"\n+    # コマンドライン引数を解析\n+    parser = argparse.ArgumentParser(description=\"GitLab MR情報を取得してコードレビューを実行します。\")\n+    parser.add_argument(\"--mr_iid\", type=int, required=True, help=\"マージリクエストのIIDを指定してください。\")\n+    args = parser.parse_args()\n+\n+    # GitLab MR情報を取得\n+    diffs = get_mr_info(args.mr_iid)\n+    # レビュー観点をロード\n+    review_perspectives = load_review_perspectives()\n+    \n+    # 出力ファイルの末尾に付与する現在時刻を取得してフォーマット\n+    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n+    # レビュー結果出力ファイル名とログファイル名を生成\n+    output_file = f\"review_results_{timestamp}.md\"\n+    log_file = f\"api_call_logs_{timestamp}.json\"\n+    \n+    # 各DIFFに対してレビューを実行\n+    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n+        for diff in diffs:\n+            file_name = diff['new_path']\n+            diff_info = diff['diff']\n+            if not diff_info:\n+                continue\n+            f.write(f\"# レビュー対象：{file_name}\\n\")\n+            for perspective in review_perspectives:\n+                f.write(f\"## レビュー観点：{perspective}\\n\")\n+                review_result = review_code(perspective, file_name, diff_info, log_file)\n+                f.write(f\"{review_result}\\n\")\n+\n+if __name__ == \"__main__\":\n+    main()\n",
        "review_perspective": "コメントが適切に記述されているか",
        "request_tokens": 3180,
        "api_params": {
            "model": "gpt-4.1-nano",
            "temperature": 0.2
        },
        "response": "この観点では問題ありません。",
        "response_tokens": 13,
        "start_time": "2025/05/26 04:48:19.303060",
        "end_time": "2025/05/26 04:48:19.807044"
    },
    {
        "file_name": "code_review.py",
        "diff_content": "@@ -0,0 +1,229 @@\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n+\n+PRIVATE_TOKEN = os.getenv(\"GITLAB_TOKEN\")\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+HEADERS = {\n+    \"PRIVATE-TOKEN\": PRIVATE_TOKEN\n+}\n+\n+CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n+TIKTOKEN_ENCODING = tiktoken.encoding_for_model(TIKTOKEN_MODEL)\n+\n+def load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n+    \"\"\"\n+    レビュー観点をJSONファイルから読み込む。\n+\n+    Args:\n+        file_path (str): レビュー観点が記載されたJSONファイルのパス。\n+\n+    Returns:\n+        list: レビュー観点のリスト。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+def load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n+    \"\"\"\n+    システムプロンプトをテキストファイルから読み込む。\n+\n+    Args:\n+        file_path (str): システムプロンプトが記載されたテキストファイルのパス。\n+\n+    Returns:\n+        str: システムプロンプトの内容。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return f.read()\n+\n+def num_tokens_from_messages(messages) -> int:\n+    \"\"\"\n+    メッセージからトークン数を計算する。\n+\n+    Args:\n+        messages (list): メッセージのリスト。\n+\n+    Returns:\n+        int: メッセージに含まれるトークン数。\n+    \"\"\"\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += TOKENS_PER_MESSAGE\n+        for key, value in message.items():\n+            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n+            if key == \"name\":\n+                num_tokens += TOKENS_PER_NAME\n+    num_tokens += 3  # assistantの返信の開始トークン\n+    return num_tokens\n+\n+def _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n+    \"\"\"\n+    OpenAIのChat APIを呼び出す関数。\n+\n+    Args:\n+        messages (list): APIに送信するメッセージリスト。\n+        model (str): 使用するモデル名。\n+        temperature (float): 出力の多様性を制御する温度パラメータ。\n+        max_tokens (int): 最大トークン数。\n+\n+    Returns:\n+        str: OpenAI APIからのレスポンス。\n+    \"\"\"\n+    response = CLIENT.chat.completions.create(\n+        model=model,\n+        messages=messages,\n+        temperature=temperature,\n+        max_tokens=max_tokens\n+    )\n+    return response.choices[0].message.content\n+\n+def review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n+    \"\"\"\n+    指定された観点でコードレビューを実行し、結果をログに記録する。\n+\n+    Args:\n+        review_perspective (str): レビュー観点。\n+        file_name (str): 対象ファイル名。\n+        diff_info (str): 差分情報。\n+        log_file (str): ログを保存するファイル名。\n+\n+    Returns:\n+        str: レビュー結果。\n+    \"\"\"\n+    # ファイル名と差分情報でシステムプロンプトをフォーマット\n+    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n+    messages = [\n+        {\"role\": \"system\", \"content\": current_system_prompt},\n+        {\"role\": \"user\", \"content\": review_perspective}\n+    ]\n+\n+    start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+    # OpenAI APIを呼び出してレビュー結果を取得\n+    response_str = _call_openai_chat_api(\n+        messages=messages,\n+        model=MODEL_NAME,\n+        temperature=TEMPERATURE,\n+    )\n+    end_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+\n+    log_data = {\n+        \"file_name\": file_name,\n+        \"diff_content\": diff_info,\n+        \"review_perspective\": review_perspective,\n+        \"request_tokens\": num_tokens_from_messages(messages),\n+        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n+        \"response\": response_str,\n+        \"response_tokens\": num_tokens_from_messages(response_str),\n+        \"start_time\": start_time,\n+        \"end_time\": end_time,\n+    }\n+\n+    # 既存のログデータをロード（なければ空リストを使う）\n+    if os.path.exists(log_file):\n+        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n+            try:\n+                existing_logs = json.load(f)\n+            except json.JSONDecodeError:\n+                existing_logs = []\n+    else:\n+        existing_logs = []\n+\n+    # 新しいログデータを追加\n+    existing_logs.append(log_data)\n+    \n+    # ログをJSON形式で保存\n+    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n+        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n+    return response_str\n+\n+def get_mr_info(mr_iid) -> list:\n+    \"\"\"\n+    GitLabのマージリクエスト情報を取得する。\n+\n+    Args:\n+        mr_iid (int): マージリクエストのIID。\n+\n+    Returns:\n+        list: マージリクエストの差分情報リスト。\n+    \"\"\"\n+    base = f\"{GITLAB_URL}/api/v4\"\n+    # 1. MR基本情報\n+    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n+    # 2. 差分情報（Diff）\n+    #    MRの差分情報を取得するエンドポイント\n+    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n+\n+    all_diffs = []\n+    current_page = 1\n+\n+    while True:\n+        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n+        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n+        try:\n+            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n+        except requests.exceptions.HTTPError as e:\n+            print(f\"APIリクエストに失敗しました: {e}\")\n+            break \n+        try:\n+            page_data = response.json()\n+        except requests.exceptions.JSONDecodeError:\n+            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n+            break\n+        if not isinstance(page_data, list):\n+            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n+            break\n+        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n+            break\n+\n+        all_diffs.extend(page_data)\n+\n+        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n+        total_pages_header = response.headers.get(\"x-total-pages\")\n+        if total_pages_header and current_page >= int(total_pages_header):\n+            break # 最後のページまで到達\n+        current_page += 1\n+    return all_diffs\n+\n+def main():\n+    \"\"\"\n+    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n+    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n+    \"\"\"\n+    # コマンドライン引数を解析\n+    parser = argparse.ArgumentParser(description=\"GitLab MR情報を取得してコードレビューを実行します。\")\n+    parser.add_argument(\"--mr_iid\", type=int, required=True, help=\"マージリクエストのIIDを指定してください。\")\n+    args = parser.parse_args()\n+\n+    # GitLab MR情報を取得\n+    diffs = get_mr_info(args.mr_iid)\n+    # レビュー観点をロード\n+    review_perspectives = load_review_perspectives()\n+    \n+    # 出力ファイルの末尾に付与する現在時刻を取得してフォーマット\n+    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n+    # レビュー結果出力ファイル名とログファイル名を生成\n+    output_file = f\"review_results_{timestamp}.md\"\n+    log_file = f\"api_call_logs_{timestamp}.json\"\n+    \n+    # 各DIFFに対してレビューを実行\n+    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n+        for diff in diffs:\n+            file_name = diff['new_path']\n+            diff_info = diff['diff']\n+            if not diff_info:\n+                continue\n+            f.write(f\"# レビュー対象：{file_name}\\n\")\n+            for perspective in review_perspectives:\n+                f.write(f\"## レビュー観点：{perspective}\\n\")\n+                review_result = review_code(perspective, file_name, diff_info, log_file)\n+                f.write(f\"{review_result}\\n\")\n+\n+if __name__ == \"__main__\":\n+    main()\n",
        "review_perspective": "関数・クラスが適切な長さに保たれているか",
        "request_tokens": 3189,
        "api_params": {
            "model": "gpt-4.1-nano",
            "temperature": 0.2
        },
        "response": "### 指摘対象コード:\n```python\ndef load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n    \"\"\"\n    レビュー観点をJSONファイルから読み込む。\n    ...\n    \"\"\"\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        return json.load(f)\n\ndef load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n    \"\"\"\n    システムプロンプトをテキストファイルから読み込む。\n    ...\n    \"\"\"\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        return f.read()\n\ndef num_tokens_from_messages(messages) -> int:\n    \"\"\"\n    メッセージからトークン数を計算する。\n    ...\n    \"\"\"\n    num_tokens = 0\n    for message in messages:\n        num_tokens += TOKENS_PER_MESSAGE\n        for key, value in message.items():\n            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n            if key == \"name\":\n                num_tokens += TOKENS_PER_NAME\n    num_tokens += 3  # assistantの返信の開始トークン\n    return num_tokens\n\ndef review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n    \"\"\"\n    指定された観点でコードレビューを実行し、結果をログに記録する。\n    ...\n    \"\"\"\n    # 省略\n    # ログ保存処理\n    ...\n    return response_str\n\ndef get_mr_info(mr_iid) -> list:\n    \"\"\"\n    GitLabのマージリクエスト情報を取得する。\n    ...\n    \"\"\"\n    # 省略\n    while True:\n        # APIリクエスト\n        ...\n        if not isinstance(page_data, list):\n            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n            break\n        if not page_data:\n            break\n        all_diffs.extend(page_data)\n        # ページング処理\n        ...\n    return all_diffs\n```\n\n### 指摘内容:\nこれらの関数はそれぞれの責務を適切に分離しており、関数の長さも適切です。ただし、`review_code`関数はログ保存処理やAPI呼び出し処理など複数の責務を持っているため、長さがやや長くなっています。\n\n### 修正提案:\n`review_code`関数の中で、ログ保存処理やAPI呼び出し部分を別の小さな関数に分割し、責務ごとに短く保つことを推奨します。これにより、関数の可読性と保守性が向上します。",
        "response_tokens": 702,
        "start_time": "2025/05/26 04:48:19.815505",
        "end_time": "2025/05/26 04:48:25.540954"
    },
    {
        "file_name": "code_review.py",
        "diff_content": "@@ -0,0 +1,229 @@\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n+\n+PRIVATE_TOKEN = os.getenv(\"GITLAB_TOKEN\")\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+HEADERS = {\n+    \"PRIVATE-TOKEN\": PRIVATE_TOKEN\n+}\n+\n+CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n+TIKTOKEN_ENCODING = tiktoken.encoding_for_model(TIKTOKEN_MODEL)\n+\n+def load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n+    \"\"\"\n+    レビュー観点をJSONファイルから読み込む。\n+\n+    Args:\n+        file_path (str): レビュー観点が記載されたJSONファイルのパス。\n+\n+    Returns:\n+        list: レビュー観点のリスト。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+def load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n+    \"\"\"\n+    システムプロンプトをテキストファイルから読み込む。\n+\n+    Args:\n+        file_path (str): システムプロンプトが記載されたテキストファイルのパス。\n+\n+    Returns:\n+        str: システムプロンプトの内容。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return f.read()\n+\n+def num_tokens_from_messages(messages) -> int:\n+    \"\"\"\n+    メッセージからトークン数を計算する。\n+\n+    Args:\n+        messages (list): メッセージのリスト。\n+\n+    Returns:\n+        int: メッセージに含まれるトークン数。\n+    \"\"\"\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += TOKENS_PER_MESSAGE\n+        for key, value in message.items():\n+            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n+            if key == \"name\":\n+                num_tokens += TOKENS_PER_NAME\n+    num_tokens += 3  # assistantの返信の開始トークン\n+    return num_tokens\n+\n+def _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n+    \"\"\"\n+    OpenAIのChat APIを呼び出す関数。\n+\n+    Args:\n+        messages (list): APIに送信するメッセージリスト。\n+        model (str): 使用するモデル名。\n+        temperature (float): 出力の多様性を制御する温度パラメータ。\n+        max_tokens (int): 最大トークン数。\n+\n+    Returns:\n+        str: OpenAI APIからのレスポンス。\n+    \"\"\"\n+    response = CLIENT.chat.completions.create(\n+        model=model,\n+        messages=messages,\n+        temperature=temperature,\n+        max_tokens=max_tokens\n+    )\n+    return response.choices[0].message.content\n+\n+def review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n+    \"\"\"\n+    指定された観点でコードレビューを実行し、結果をログに記録する。\n+\n+    Args:\n+        review_perspective (str): レビュー観点。\n+        file_name (str): 対象ファイル名。\n+        diff_info (str): 差分情報。\n+        log_file (str): ログを保存するファイル名。\n+\n+    Returns:\n+        str: レビュー結果。\n+    \"\"\"\n+    # ファイル名と差分情報でシステムプロンプトをフォーマット\n+    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n+    messages = [\n+        {\"role\": \"system\", \"content\": current_system_prompt},\n+        {\"role\": \"user\", \"content\": review_perspective}\n+    ]\n+\n+    start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+    # OpenAI APIを呼び出してレビュー結果を取得\n+    response_str = _call_openai_chat_api(\n+        messages=messages,\n+        model=MODEL_NAME,\n+        temperature=TEMPERATURE,\n+    )\n+    end_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+\n+    log_data = {\n+        \"file_name\": file_name,\n+        \"diff_content\": diff_info,\n+        \"review_perspective\": review_perspective,\n+        \"request_tokens\": num_tokens_from_messages(messages),\n+        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n+        \"response\": response_str,\n+        \"response_tokens\": num_tokens_from_messages(response_str),\n+        \"start_time\": start_time,\n+        \"end_time\": end_time,\n+    }\n+\n+    # 既存のログデータをロード（なければ空リストを使う）\n+    if os.path.exists(log_file):\n+        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n+            try:\n+                existing_logs = json.load(f)\n+            except json.JSONDecodeError:\n+                existing_logs = []\n+    else:\n+        existing_logs = []\n+\n+    # 新しいログデータを追加\n+    existing_logs.append(log_data)\n+    \n+    # ログをJSON形式で保存\n+    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n+        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n+    return response_str\n+\n+def get_mr_info(mr_iid) -> list:\n+    \"\"\"\n+    GitLabのマージリクエスト情報を取得する。\n+\n+    Args:\n+        mr_iid (int): マージリクエストのIID。\n+\n+    Returns:\n+        list: マージリクエストの差分情報リスト。\n+    \"\"\"\n+    base = f\"{GITLAB_URL}/api/v4\"\n+    # 1. MR基本情報\n+    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n+    # 2. 差分情報（Diff）\n+    #    MRの差分情報を取得するエンドポイント\n+    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n+\n+    all_diffs = []\n+    current_page = 1\n+\n+    while True:\n+        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n+        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n+        try:\n+            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n+        except requests.exceptions.HTTPError as e:\n+            print(f\"APIリクエストに失敗しました: {e}\")\n+            break \n+        try:\n+            page_data = response.json()\n+        except requests.exceptions.JSONDecodeError:\n+            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n+            break\n+        if not isinstance(page_data, list):\n+            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n+            break\n+        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n+            break\n+\n+        all_diffs.extend(page_data)\n+\n+        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n+        total_pages_header = response.headers.get(\"x-total-pages\")\n+        if total_pages_header and current_page >= int(total_pages_header):\n+            break # 最後のページまで到達\n+        current_page += 1\n+    return all_diffs\n+\n+def main():\n+    \"\"\"\n+    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n+    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n+    \"\"\"\n+    # コマンドライン引数を解析\n+    parser = argparse.ArgumentParser(description=\"GitLab MR情報を取得してコードレビューを実行します。\")\n+    parser.add_argument(\"--mr_iid\", type=int, required=True, help=\"マージリクエストのIIDを指定してください。\")\n+    args = parser.parse_args()\n+\n+    # GitLab MR情報を取得\n+    diffs = get_mr_info(args.mr_iid)\n+    # レビュー観点をロード\n+    review_perspectives = load_review_perspectives()\n+    \n+    # 出力ファイルの末尾に付与する現在時刻を取得してフォーマット\n+    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n+    # レビュー結果出力ファイル名とログファイル名を生成\n+    output_file = f\"review_results_{timestamp}.md\"\n+    log_file = f\"api_call_logs_{timestamp}.json\"\n+    \n+    # 各DIFFに対してレビューを実行\n+    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n+        for diff in diffs:\n+            file_name = diff['new_path']\n+            diff_info = diff['diff']\n+            if not diff_info:\n+                continue\n+            f.write(f\"# レビュー対象：{file_name}\\n\")\n+            for perspective in review_perspectives:\n+                f.write(f\"## レビュー観点：{perspective}\\n\")\n+                review_result = review_code(perspective, file_name, diff_info, log_file)\n+                f.write(f\"{review_result}\\n\")\n+\n+if __name__ == \"__main__\":\n+    main()\n",
        "review_perspective": "ネストが深すぎないか",
        "request_tokens": 3178,
        "api_params": {
            "model": "gpt-4.1-nano",
            "temperature": 0.2
        },
        "response": "### 指摘対象コード:\n```python\ndef get_mr_info(mr_iid) -> list:\n    \"\"\"\n    GitLabのマージリクエスト情報を取得する。\n    ...\n    \"\"\"\n    base = f\"{GITLAB_URL}/api/v4\"\n    # 1. MR基本情報\n    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n    # 2. 差分情報（Diff）\n    #    MRの差分情報を取得するエンドポイント\n    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n\n    all_diffs = []\n    current_page = 1\n\n    while True:\n        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n        try:\n            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n        except requests.exceptions.HTTPError as e:\n            print(f\"APIリクエストに失敗しました: {e}\")\n            break \n        try:\n            page_data = response.json()\n        except requests.exceptions.JSONDecodeError:\n            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n            break\n        if not isinstance(page_data, list):\n            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n            break\n        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n            break\n        all_diffs.extend(page_data)\n        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n        total_pages_header = response.headers.get(\"x-total-pages\")\n        if total_pages_header and current_page >= int(total_pages_header):\n            break # 最後のページまで到達\n        current_page += 1\n    return all_diffs\n```\n\n### 指摘内容:\nこの関数は、APIから差分情報をページネーションを用いて取得し、全ページの差分をリストに格納して返す処理を行っています。ただし、whileループ内の処理が深くネストされており、例外処理や条件分岐が複雑になっています。\n\n### 修正提案:\nループ内の処理を関数に分割したり、条件判定を早期リターンにしてネストを浅くすることで、可読性を向上させることができます。例えば、例外処理やレスポンスの検証部分を個別の関数に切り出すか、ループの中の条件判定を整理して、ネストを浅くすることを検討してください。",
        "response_tokens": 743,
        "start_time": "2025/05/26 04:48:25.548330",
        "end_time": "2025/05/26 04:48:31.480704"
    },
    {
        "file_name": "code_review.py",
        "diff_content": "@@ -0,0 +1,229 @@\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n+\n+PRIVATE_TOKEN = os.getenv(\"GITLAB_TOKEN\")\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+HEADERS = {\n+    \"PRIVATE-TOKEN\": PRIVATE_TOKEN\n+}\n+\n+CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n+TIKTOKEN_ENCODING = tiktoken.encoding_for_model(TIKTOKEN_MODEL)\n+\n+def load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n+    \"\"\"\n+    レビュー観点をJSONファイルから読み込む。\n+\n+    Args:\n+        file_path (str): レビュー観点が記載されたJSONファイルのパス。\n+\n+    Returns:\n+        list: レビュー観点のリスト。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+def load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n+    \"\"\"\n+    システムプロンプトをテキストファイルから読み込む。\n+\n+    Args:\n+        file_path (str): システムプロンプトが記載されたテキストファイルのパス。\n+\n+    Returns:\n+        str: システムプロンプトの内容。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return f.read()\n+\n+def num_tokens_from_messages(messages) -> int:\n+    \"\"\"\n+    メッセージからトークン数を計算する。\n+\n+    Args:\n+        messages (list): メッセージのリスト。\n+\n+    Returns:\n+        int: メッセージに含まれるトークン数。\n+    \"\"\"\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += TOKENS_PER_MESSAGE\n+        for key, value in message.items():\n+            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n+            if key == \"name\":\n+                num_tokens += TOKENS_PER_NAME\n+    num_tokens += 3  # assistantの返信の開始トークン\n+    return num_tokens\n+\n+def _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n+    \"\"\"\n+    OpenAIのChat APIを呼び出す関数。\n+\n+    Args:\n+        messages (list): APIに送信するメッセージリスト。\n+        model (str): 使用するモデル名。\n+        temperature (float): 出力の多様性を制御する温度パラメータ。\n+        max_tokens (int): 最大トークン数。\n+\n+    Returns:\n+        str: OpenAI APIからのレスポンス。\n+    \"\"\"\n+    response = CLIENT.chat.completions.create(\n+        model=model,\n+        messages=messages,\n+        temperature=temperature,\n+        max_tokens=max_tokens\n+    )\n+    return response.choices[0].message.content\n+\n+def review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n+    \"\"\"\n+    指定された観点でコードレビューを実行し、結果をログに記録する。\n+\n+    Args:\n+        review_perspective (str): レビュー観点。\n+        file_name (str): 対象ファイル名。\n+        diff_info (str): 差分情報。\n+        log_file (str): ログを保存するファイル名。\n+\n+    Returns:\n+        str: レビュー結果。\n+    \"\"\"\n+    # ファイル名と差分情報でシステムプロンプトをフォーマット\n+    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n+    messages = [\n+        {\"role\": \"system\", \"content\": current_system_prompt},\n+        {\"role\": \"user\", \"content\": review_perspective}\n+    ]\n+\n+    start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+    # OpenAI APIを呼び出してレビュー結果を取得\n+    response_str = _call_openai_chat_api(\n+        messages=messages,\n+        model=MODEL_NAME,\n+        temperature=TEMPERATURE,\n+    )\n+    end_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+\n+    log_data = {\n+        \"file_name\": file_name,\n+        \"diff_content\": diff_info,\n+        \"review_perspective\": review_perspective,\n+        \"request_tokens\": num_tokens_from_messages(messages),\n+        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n+        \"response\": response_str,\n+        \"response_tokens\": num_tokens_from_messages(response_str),\n+        \"start_time\": start_time,\n+        \"end_time\": end_time,\n+    }\n+\n+    # 既存のログデータをロード（なければ空リストを使う）\n+    if os.path.exists(log_file):\n+        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n+            try:\n+                existing_logs = json.load(f)\n+            except json.JSONDecodeError:\n+                existing_logs = []\n+    else:\n+        existing_logs = []\n+\n+    # 新しいログデータを追加\n+    existing_logs.append(log_data)\n+    \n+    # ログをJSON形式で保存\n+    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n+        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n+    return response_str\n+\n+def get_mr_info(mr_iid) -> list:\n+    \"\"\"\n+    GitLabのマージリクエスト情報を取得する。\n+\n+    Args:\n+        mr_iid (int): マージリクエストのIID。\n+\n+    Returns:\n+        list: マージリクエストの差分情報リスト。\n+    \"\"\"\n+    base = f\"{GITLAB_URL}/api/v4\"\n+    # 1. MR基本情報\n+    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n+    # 2. 差分情報（Diff）\n+    #    MRの差分情報を取得するエンドポイント\n+    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n+\n+    all_diffs = []\n+    current_page = 1\n+\n+    while True:\n+        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n+        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n+        try:\n+            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n+        except requests.exceptions.HTTPError as e:\n+            print(f\"APIリクエストに失敗しました: {e}\")\n+            break \n+        try:\n+            page_data = response.json()\n+        except requests.exceptions.JSONDecodeError:\n+            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n+            break\n+        if not isinstance(page_data, list):\n+            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n+            break\n+        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n+            break\n+\n+        all_diffs.extend(page_data)\n+\n+        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n+        total_pages_header = response.headers.get(\"x-total-pages\")\n+        if total_pages_header and current_page >= int(total_pages_header):\n+            break # 最後のページまで到達\n+        current_page += 1\n+    return all_diffs\n+\n+def main():\n+    \"\"\"\n+    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n+    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n+    \"\"\"\n+    # コマンドライン引数を解析\n+    parser = argparse.ArgumentParser(description=\"GitLab MR情報を取得してコードレビューを実行します。\")\n+    parser.add_argument(\"--mr_iid\", type=int, required=True, help=\"マージリクエストのIIDを指定してください。\")\n+    args = parser.parse_args()\n+\n+    # GitLab MR情報を取得\n+    diffs = get_mr_info(args.mr_iid)\n+    # レビュー観点をロード\n+    review_perspectives = load_review_perspectives()\n+    \n+    # 出力ファイルの末尾に付与する現在時刻を取得してフォーマット\n+    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n+    # レビュー結果出力ファイル名とログファイル名を生成\n+    output_file = f\"review_results_{timestamp}.md\"\n+    log_file = f\"api_call_logs_{timestamp}.json\"\n+    \n+    # 各DIFFに対してレビューを実行\n+    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n+        for diff in diffs:\n+            file_name = diff['new_path']\n+            diff_info = diff['diff']\n+            if not diff_info:\n+                continue\n+            f.write(f\"# レビュー対象：{file_name}\\n\")\n+            for perspective in review_perspectives:\n+                f.write(f\"## レビュー観点：{perspective}\\n\")\n+                review_result = review_code(perspective, file_name, diff_info, log_file)\n+                f.write(f\"{review_result}\\n\")\n+\n+if __name__ == \"__main__\":\n+    main()\n",
        "review_perspective": "ドキュメンテーション文字列（docstring）があるか",
        "request_tokens": 3186,
        "api_params": {
            "model": "gpt-4.1-nano",
            "temperature": 0.2
        },
        "response": "### 指摘対象コード:\n```python\ndef load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n    \"\"\"\n    レビュー観点をJSONファイルから読み込む。\n    ...\n    \"\"\"\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        return json.load(f)\n\ndef load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n    \"\"\"\n    システムプロンプトをテキストファイルから読み込む。\n    ...\n    \"\"\"\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        return f.read()\n\ndef num_tokens_from_messages(messages) -> int:\n    \"\"\"\n    メッセージからトークン数を計算する。\n    ...\n    \"\"\"\n    num_tokens = 0\n    for message in messages:\n        num_tokens += TOKENS_PER_MESSAGE\n        for key, value in message.items():\n            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n            if key == \"name\":\n                num_tokens += TOKENS_PER_NAME\n    num_tokens += 3  # assistantの返信の開始トークン\n    return num_tokens\n\ndef _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n    \"\"\"\n    OpenAIのChat APIを呼び出す関数。\n    ...\n    \"\"\"\n    response = CLIENT.chat.completions.create(\n        model=model,\n        messages=messages,\n        temperature=temperature,\n        max_tokens=max_tokens\n    )\n    return response.choices[0].message.content\n\ndef review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n    \"\"\"\n    指定された観点でコードレビューを実行し、結果をログに記録する。\n    ...\n    \"\"\"\n    # 関数内の処理\n    ...\n    \ndef get_mr_info(mr_iid) -> list:\n    \"\"\"\n    GitLabのマージリクエスト情報を取得する。\n    ...\n    \"\"\"\n    # 関数内の処理\n    ...\n    \ndef main():\n    \"\"\"\n    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n    \"\"\"\n    # 関数内の処理\n    ...\n```\n\n### 指摘内容:\n全ての関数に対して、適切なドキュメンテーション文字列（docstring）が存在しています。\n\n### 修正提案:\n特に修正は不要です。",
        "response_tokens": 654,
        "start_time": "2025/05/26 04:48:31.487813",
        "end_time": "2025/05/26 04:48:36.506538"
    },
    {
        "file_name": "code_review.py",
        "diff_content": "@@ -0,0 +1,229 @@\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n+\n+PRIVATE_TOKEN = os.getenv(\"GITLAB_TOKEN\")\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+HEADERS = {\n+    \"PRIVATE-TOKEN\": PRIVATE_TOKEN\n+}\n+\n+CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n+TIKTOKEN_ENCODING = tiktoken.encoding_for_model(TIKTOKEN_MODEL)\n+\n+def load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n+    \"\"\"\n+    レビュー観点をJSONファイルから読み込む。\n+\n+    Args:\n+        file_path (str): レビュー観点が記載されたJSONファイルのパス。\n+\n+    Returns:\n+        list: レビュー観点のリスト。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+def load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n+    \"\"\"\n+    システムプロンプトをテキストファイルから読み込む。\n+\n+    Args:\n+        file_path (str): システムプロンプトが記載されたテキストファイルのパス。\n+\n+    Returns:\n+        str: システムプロンプトの内容。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return f.read()\n+\n+def num_tokens_from_messages(messages) -> int:\n+    \"\"\"\n+    メッセージからトークン数を計算する。\n+\n+    Args:\n+        messages (list): メッセージのリスト。\n+\n+    Returns:\n+        int: メッセージに含まれるトークン数。\n+    \"\"\"\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += TOKENS_PER_MESSAGE\n+        for key, value in message.items():\n+            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n+            if key == \"name\":\n+                num_tokens += TOKENS_PER_NAME\n+    num_tokens += 3  # assistantの返信の開始トークン\n+    return num_tokens\n+\n+def _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n+    \"\"\"\n+    OpenAIのChat APIを呼び出す関数。\n+\n+    Args:\n+        messages (list): APIに送信するメッセージリスト。\n+        model (str): 使用するモデル名。\n+        temperature (float): 出力の多様性を制御する温度パラメータ。\n+        max_tokens (int): 最大トークン数。\n+\n+    Returns:\n+        str: OpenAI APIからのレスポンス。\n+    \"\"\"\n+    response = CLIENT.chat.completions.create(\n+        model=model,\n+        messages=messages,\n+        temperature=temperature,\n+        max_tokens=max_tokens\n+    )\n+    return response.choices[0].message.content\n+\n+def review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n+    \"\"\"\n+    指定された観点でコードレビューを実行し、結果をログに記録する。\n+\n+    Args:\n+        review_perspective (str): レビュー観点。\n+        file_name (str): 対象ファイル名。\n+        diff_info (str): 差分情報。\n+        log_file (str): ログを保存するファイル名。\n+\n+    Returns:\n+        str: レビュー結果。\n+    \"\"\"\n+    # ファイル名と差分情報でシステムプロンプトをフォーマット\n+    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n+    messages = [\n+        {\"role\": \"system\", \"content\": current_system_prompt},\n+        {\"role\": \"user\", \"content\": review_perspective}\n+    ]\n+\n+    start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+    # OpenAI APIを呼び出してレビュー結果を取得\n+    response_str = _call_openai_chat_api(\n+        messages=messages,\n+        model=MODEL_NAME,\n+        temperature=TEMPERATURE,\n+    )\n+    end_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+\n+    log_data = {\n+        \"file_name\": file_name,\n+        \"diff_content\": diff_info,\n+        \"review_perspective\": review_perspective,\n+        \"request_tokens\": num_tokens_from_messages(messages),\n+        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n+        \"response\": response_str,\n+        \"response_tokens\": num_tokens_from_messages(response_str),\n+        \"start_time\": start_time,\n+        \"end_time\": end_time,\n+    }\n+\n+    # 既存のログデータをロード（なければ空リストを使う）\n+    if os.path.exists(log_file):\n+        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n+            try:\n+                existing_logs = json.load(f)\n+            except json.JSONDecodeError:\n+                existing_logs = []\n+    else:\n+        existing_logs = []\n+\n+    # 新しいログデータを追加\n+    existing_logs.append(log_data)\n+    \n+    # ログをJSON形式で保存\n+    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n+        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n+    return response_str\n+\n+def get_mr_info(mr_iid) -> list:\n+    \"\"\"\n+    GitLabのマージリクエスト情報を取得する。\n+\n+    Args:\n+        mr_iid (int): マージリクエストのIID。\n+\n+    Returns:\n+        list: マージリクエストの差分情報リスト。\n+    \"\"\"\n+    base = f\"{GITLAB_URL}/api/v4\"\n+    # 1. MR基本情報\n+    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n+    # 2. 差分情報（Diff）\n+    #    MRの差分情報を取得するエンドポイント\n+    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n+\n+    all_diffs = []\n+    current_page = 1\n+\n+    while True:\n+        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n+        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n+        try:\n+            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n+        except requests.exceptions.HTTPError as e:\n+            print(f\"APIリクエストに失敗しました: {e}\")\n+            break \n+        try:\n+            page_data = response.json()\n+        except requests.exceptions.JSONDecodeError:\n+            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n+            break\n+        if not isinstance(page_data, list):\n+            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n+            break\n+        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n+            break\n+\n+        all_diffs.extend(page_data)\n+\n+        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n+        total_pages_header = response.headers.get(\"x-total-pages\")\n+        if total_pages_header and current_page >= int(total_pages_header):\n+            break # 最後のページまで到達\n+        current_page += 1\n+    return all_diffs\n+\n+def main():\n+    \"\"\"\n+    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n+    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n+    \"\"\"\n+    # コマンドライン引数を解析\n+    parser = argparse.ArgumentParser(description=\"GitLab MR情報を取得してコードレビューを実行します。\")\n+    parser.add_argument(\"--mr_iid\", type=int, required=True, help=\"マージリクエストのIIDを指定してください。\")\n+    args = parser.parse_args()\n+\n+    # GitLab MR情報を取得\n+    diffs = get_mr_info(args.mr_iid)\n+    # レビュー観点をロード\n+    review_perspectives = load_review_perspectives()\n+    \n+    # 出力ファイルの末尾に付与する現在時刻を取得してフォーマット\n+    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n+    # レビュー結果出力ファイル名とログファイル名を生成\n+    output_file = f\"review_results_{timestamp}.md\"\n+    log_file = f\"api_call_logs_{timestamp}.json\"\n+    \n+    # 各DIFFに対してレビューを実行\n+    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n+        for diff in diffs:\n+            file_name = diff['new_path']\n+            diff_info = diff['diff']\n+            if not diff_info:\n+                continue\n+            f.write(f\"# レビュー対象：{file_name}\\n\")\n+            for perspective in review_perspectives:\n+                f.write(f\"## レビュー観点：{perspective}\\n\")\n+                review_result = review_code(perspective, file_name, diff_info, log_file)\n+                f.write(f\"{review_result}\\n\")\n+\n+if __name__ == \"__main__\":\n+    main()\n",
        "review_perspective": "関心の分離（SoC）がされているか",
        "request_tokens": 3182,
        "api_params": {
            "model": "gpt-4.1-nano",
            "temperature": 0.2
        },
        "response": "### 指摘対象コード:\n```python\ndef review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n    \"\"\"\n    指定された観点でコードレビューを実行し、結果をログに記録する。\n    \"\"\"\n    # ファイル名と差分情報でシステムプロンプトをフォーマット\n    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n    messages = [\n        {\"role\": \"system\", \"content\": current_system_prompt},\n        {\"role\": \"user\", \"content\": review_perspective}\n    ]\n    # OpenAI API呼び出しとログ記録\n    ...\n```\n\n### 指摘内容:\nこの関数`review_code`は、コードレビューのためのシステムプロンプトの生成とAPI呼び出し、ログ記録を一手に担っています。  \nこの構造は、**レビューの実行とログ記録の責務が密に結びついている**ため、関心の分離（SoC）が十分にされているとは言えません。  \n特に、レビューの実行（API呼び出し）とログの保存は異なる関心事であり、これらを分離することで、テストやメンテナンス性が向上します。\n\n### 修正提案:\n`review_code`関数からログ記録部分を切り出し、別の関数（例：`log_review_result`）に委譲する設計に改善してください。  \nこれにより、レビュー処理とログ記録の責務が明確に分離され、関心の分離が促進されます。",
        "response_tokens": 483,
        "start_time": "2025/05/26 04:48:36.514740",
        "end_time": "2025/05/26 04:48:40.370657"
    },
    {
        "file_name": "code_review.py",
        "diff_content": "@@ -0,0 +1,229 @@\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n+\n+PRIVATE_TOKEN = os.getenv(\"GITLAB_TOKEN\")\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+HEADERS = {\n+    \"PRIVATE-TOKEN\": PRIVATE_TOKEN\n+}\n+\n+CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n+TIKTOKEN_ENCODING = tiktoken.encoding_for_model(TIKTOKEN_MODEL)\n+\n+def load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n+    \"\"\"\n+    レビュー観点をJSONファイルから読み込む。\n+\n+    Args:\n+        file_path (str): レビュー観点が記載されたJSONファイルのパス。\n+\n+    Returns:\n+        list: レビュー観点のリスト。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+def load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n+    \"\"\"\n+    システムプロンプトをテキストファイルから読み込む。\n+\n+    Args:\n+        file_path (str): システムプロンプトが記載されたテキストファイルのパス。\n+\n+    Returns:\n+        str: システムプロンプトの内容。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return f.read()\n+\n+def num_tokens_from_messages(messages) -> int:\n+    \"\"\"\n+    メッセージからトークン数を計算する。\n+\n+    Args:\n+        messages (list): メッセージのリスト。\n+\n+    Returns:\n+        int: メッセージに含まれるトークン数。\n+    \"\"\"\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += TOKENS_PER_MESSAGE\n+        for key, value in message.items():\n+            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n+            if key == \"name\":\n+                num_tokens += TOKENS_PER_NAME\n+    num_tokens += 3  # assistantの返信の開始トークン\n+    return num_tokens\n+\n+def _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n+    \"\"\"\n+    OpenAIのChat APIを呼び出す関数。\n+\n+    Args:\n+        messages (list): APIに送信するメッセージリスト。\n+        model (str): 使用するモデル名。\n+        temperature (float): 出力の多様性を制御する温度パラメータ。\n+        max_tokens (int): 最大トークン数。\n+\n+    Returns:\n+        str: OpenAI APIからのレスポンス。\n+    \"\"\"\n+    response = CLIENT.chat.completions.create(\n+        model=model,\n+        messages=messages,\n+        temperature=temperature,\n+        max_tokens=max_tokens\n+    )\n+    return response.choices[0].message.content\n+\n+def review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n+    \"\"\"\n+    指定された観点でコードレビューを実行し、結果をログに記録する。\n+\n+    Args:\n+        review_perspective (str): レビュー観点。\n+        file_name (str): 対象ファイル名。\n+        diff_info (str): 差分情報。\n+        log_file (str): ログを保存するファイル名。\n+\n+    Returns:\n+        str: レビュー結果。\n+    \"\"\"\n+    # ファイル名と差分情報でシステムプロンプトをフォーマット\n+    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n+    messages = [\n+        {\"role\": \"system\", \"content\": current_system_prompt},\n+        {\"role\": \"user\", \"content\": review_perspective}\n+    ]\n+\n+    start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+    # OpenAI APIを呼び出してレビュー結果を取得\n+    response_str = _call_openai_chat_api(\n+        messages=messages,\n+        model=MODEL_NAME,\n+        temperature=TEMPERATURE,\n+    )\n+    end_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+\n+    log_data = {\n+        \"file_name\": file_name,\n+        \"diff_content\": diff_info,\n+        \"review_perspective\": review_perspective,\n+        \"request_tokens\": num_tokens_from_messages(messages),\n+        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n+        \"response\": response_str,\n+        \"response_tokens\": num_tokens_from_messages(response_str),\n+        \"start_time\": start_time,\n+        \"end_time\": end_time,\n+    }\n+\n+    # 既存のログデータをロード（なければ空リストを使う）\n+    if os.path.exists(log_file):\n+        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n+            try:\n+                existing_logs = json.load(f)\n+            except json.JSONDecodeError:\n+                existing_logs = []\n+    else:\n+        existing_logs = []\n+\n+    # 新しいログデータを追加\n+    existing_logs.append(log_data)\n+    \n+    # ログをJSON形式で保存\n+    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n+        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n+    return response_str\n+\n+def get_mr_info(mr_iid) -> list:\n+    \"\"\"\n+    GitLabのマージリクエスト情報を取得する。\n+\n+    Args:\n+        mr_iid (int): マージリクエストのIID。\n+\n+    Returns:\n+        list: マージリクエストの差分情報リスト。\n+    \"\"\"\n+    base = f\"{GITLAB_URL}/api/v4\"\n+    # 1. MR基本情報\n+    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n+    # 2. 差分情報（Diff）\n+    #    MRの差分情報を取得するエンドポイント\n+    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n+\n+    all_diffs = []\n+    current_page = 1\n+\n+    while True:\n+        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n+        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n+        try:\n+            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n+        except requests.exceptions.HTTPError as e:\n+            print(f\"APIリクエストに失敗しました: {e}\")\n+            break \n+        try:\n+            page_data = response.json()\n+        except requests.exceptions.JSONDecodeError:\n+            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n+            break\n+        if not isinstance(page_data, list):\n+            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n+            break\n+        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n+            break\n+\n+        all_diffs.extend(page_data)\n+\n+        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n+        total_pages_header = response.headers.get(\"x-total-pages\")\n+        if total_pages_header and current_page >= int(total_pages_header):\n+            break # 最後のページまで到達\n+        current_page += 1\n+    return all_diffs\n+\n+def main():\n+    \"\"\"\n+    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n+    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n+    \"\"\"\n+    # コマンドライン引数を解析\n+    parser = argparse.ArgumentParser(description=\"GitLab MR情報を取得してコードレビューを実行します。\")\n+    parser.add_argument(\"--mr_iid\", type=int, required=True, help=\"マージリクエストのIIDを指定してください。\")\n+    args = parser.parse_args()\n+\n+    # GitLab MR情報を取得\n+    diffs = get_mr_info(args.mr_iid)\n+    # レビュー観点をロード\n+    review_perspectives = load_review_perspectives()\n+    \n+    # 出力ファイルの末尾に付与する現在時刻を取得してフォーマット\n+    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n+    # レビュー結果出力ファイル名とログファイル名を生成\n+    output_file = f\"review_results_{timestamp}.md\"\n+    log_file = f\"api_call_logs_{timestamp}.json\"\n+    \n+    # 各DIFFに対してレビューを実行\n+    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n+        for diff in diffs:\n+            file_name = diff['new_path']\n+            diff_info = diff['diff']\n+            if not diff_info:\n+                continue\n+            f.write(f\"# レビュー対象：{file_name}\\n\")\n+            for perspective in review_perspectives:\n+                f.write(f\"## レビュー観点：{perspective}\\n\")\n+                review_result = review_code(perspective, file_name, diff_info, log_file)\n+                f.write(f\"{review_result}\\n\")\n+\n+if __name__ == \"__main__\":\n+    main()\n",
        "review_perspective": "単一責任の原則（SRP）に従っているか",
        "request_tokens": 3187,
        "api_params": {
            "model": "gpt-4.1-nano",
            "temperature": 0.2
        },
        "response": "### 指摘対象コード:\n```python\ndef review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n    \"\"\"\n    指定された観点でコードレビューを実行し、結果をログに記録する。\n    \"\"\"\n    # ファイル名と差分情報でシステムプロンプトをフォーマット\n    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n    messages = [\n        {\"role\": \"system\", \"content\": current_system_prompt},\n        {\"role\": \"user\", \"content\": review_perspective}\n    ]\n    ...\n    # OpenAI APIを呼び出してレビュー結果を取得\n    response_str = _call_openai_chat_api(\n        messages=messages,\n        model=MODEL_NAME,\n        temperature=TEMPERATURE,\n    )\n    ...\n    # ログの保存処理\n    log_data = {\n        \"file_name\": file_name,\n        \"diff_content\": diff_info,\n        \"review_perspective\": review_perspective,\n        \"request_tokens\": num_tokens_from_messages(messages),\n        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n        \"response\": response_str,\n        \"response_tokens\": num_tokens_from_messages(response_str),\n        \"start_time\": start_time,\n        \"end_time\": end_time,\n    }\n    ...\n```\n\n### 指摘内容:\n`review_code`関数は、コードレビューの実行、ログ記録のためのデータ構築、そしてAPI呼び出しといった複数の責務を持っています。  \nこのため、単一責任の原則（SRP）に従っているとは言えません。\n\n### 修正提案:\n`review_code`関数を以下のように分割し、それぞれの責務を明確に分離してください。\n\n- 1つ目の関数は、レビューの実行と結果取得のみを担当（例：`perform_review`）\n- 2つ目の関数は、ログの記録と保存のみを担当（例：`log_review_result`）\n\nこれにより、関数の責務が明確になり、保守性やテスト容易性が向上します。",
        "response_tokens": 588,
        "start_time": "2025/05/26 04:48:40.378216",
        "end_time": "2025/05/26 04:48:44.310199"
    },
    {
        "file_name": "code_review.py",
        "diff_content": "@@ -0,0 +1,229 @@\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n+\n+PRIVATE_TOKEN = os.getenv(\"GITLAB_TOKEN\")\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+HEADERS = {\n+    \"PRIVATE-TOKEN\": PRIVATE_TOKEN\n+}\n+\n+CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n+TIKTOKEN_ENCODING = tiktoken.encoding_for_model(TIKTOKEN_MODEL)\n+\n+def load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n+    \"\"\"\n+    レビュー観点をJSONファイルから読み込む。\n+\n+    Args:\n+        file_path (str): レビュー観点が記載されたJSONファイルのパス。\n+\n+    Returns:\n+        list: レビュー観点のリスト。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+def load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n+    \"\"\"\n+    システムプロンプトをテキストファイルから読み込む。\n+\n+    Args:\n+        file_path (str): システムプロンプトが記載されたテキストファイルのパス。\n+\n+    Returns:\n+        str: システムプロンプトの内容。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return f.read()\n+\n+def num_tokens_from_messages(messages) -> int:\n+    \"\"\"\n+    メッセージからトークン数を計算する。\n+\n+    Args:\n+        messages (list): メッセージのリスト。\n+\n+    Returns:\n+        int: メッセージに含まれるトークン数。\n+    \"\"\"\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += TOKENS_PER_MESSAGE\n+        for key, value in message.items():\n+            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n+            if key == \"name\":\n+                num_tokens += TOKENS_PER_NAME\n+    num_tokens += 3  # assistantの返信の開始トークン\n+    return num_tokens\n+\n+def _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n+    \"\"\"\n+    OpenAIのChat APIを呼び出す関数。\n+\n+    Args:\n+        messages (list): APIに送信するメッセージリスト。\n+        model (str): 使用するモデル名。\n+        temperature (float): 出力の多様性を制御する温度パラメータ。\n+        max_tokens (int): 最大トークン数。\n+\n+    Returns:\n+        str: OpenAI APIからのレスポンス。\n+    \"\"\"\n+    response = CLIENT.chat.completions.create(\n+        model=model,\n+        messages=messages,\n+        temperature=temperature,\n+        max_tokens=max_tokens\n+    )\n+    return response.choices[0].message.content\n+\n+def review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n+    \"\"\"\n+    指定された観点でコードレビューを実行し、結果をログに記録する。\n+\n+    Args:\n+        review_perspective (str): レビュー観点。\n+        file_name (str): 対象ファイル名。\n+        diff_info (str): 差分情報。\n+        log_file (str): ログを保存するファイル名。\n+\n+    Returns:\n+        str: レビュー結果。\n+    \"\"\"\n+    # ファイル名と差分情報でシステムプロンプトをフォーマット\n+    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n+    messages = [\n+        {\"role\": \"system\", \"content\": current_system_prompt},\n+        {\"role\": \"user\", \"content\": review_perspective}\n+    ]\n+\n+    start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+    # OpenAI APIを呼び出してレビュー結果を取得\n+    response_str = _call_openai_chat_api(\n+        messages=messages,\n+        model=MODEL_NAME,\n+        temperature=TEMPERATURE,\n+    )\n+    end_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+\n+    log_data = {\n+        \"file_name\": file_name,\n+        \"diff_content\": diff_info,\n+        \"review_perspective\": review_perspective,\n+        \"request_tokens\": num_tokens_from_messages(messages),\n+        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n+        \"response\": response_str,\n+        \"response_tokens\": num_tokens_from_messages(response_str),\n+        \"start_time\": start_time,\n+        \"end_time\": end_time,\n+    }\n+\n+    # 既存のログデータをロード（なければ空リストを使う）\n+    if os.path.exists(log_file):\n+        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n+            try:\n+                existing_logs = json.load(f)\n+            except json.JSONDecodeError:\n+                existing_logs = []\n+    else:\n+        existing_logs = []\n+\n+    # 新しいログデータを追加\n+    existing_logs.append(log_data)\n+    \n+    # ログをJSON形式で保存\n+    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n+        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n+    return response_str\n+\n+def get_mr_info(mr_iid) -> list:\n+    \"\"\"\n+    GitLabのマージリクエスト情報を取得する。\n+\n+    Args:\n+        mr_iid (int): マージリクエストのIID。\n+\n+    Returns:\n+        list: マージリクエストの差分情報リスト。\n+    \"\"\"\n+    base = f\"{GITLAB_URL}/api/v4\"\n+    # 1. MR基本情報\n+    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n+    # 2. 差分情報（Diff）\n+    #    MRの差分情報を取得するエンドポイント\n+    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n+\n+    all_diffs = []\n+    current_page = 1\n+\n+    while True:\n+        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n+        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n+        try:\n+            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n+        except requests.exceptions.HTTPError as e:\n+            print(f\"APIリクエストに失敗しました: {e}\")\n+            break \n+        try:\n+            page_data = response.json()\n+        except requests.exceptions.JSONDecodeError:\n+            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n+            break\n+        if not isinstance(page_data, list):\n+            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n+            break\n+        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n+            break\n+\n+        all_diffs.extend(page_data)\n+\n+        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n+        total_pages_header = response.headers.get(\"x-total-pages\")\n+        if total_pages_header and current_page >= int(total_pages_header):\n+            break # 最後のページまで到達\n+        current_page += 1\n+    return all_diffs\n+\n+def main():\n+    \"\"\"\n+    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n+    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n+    \"\"\"\n+    # コマンドライン引数を解析\n+    parser = argparse.ArgumentParser(description=\"GitLab MR情報を取得してコードレビューを実行します。\")\n+    parser.add_argument(\"--mr_iid\", type=int, required=True, help=\"マージリクエストのIIDを指定してください。\")\n+    args = parser.parse_args()\n+\n+    # GitLab MR情報を取得\n+    diffs = get_mr_info(args.mr_iid)\n+    # レビュー観点をロード\n+    review_perspectives = load_review_perspectives()\n+    \n+    # 出力ファイルの末尾に付与する現在時刻を取得してフォーマット\n+    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n+    # レビュー結果出力ファイル名とログファイル名を生成\n+    output_file = f\"review_results_{timestamp}.md\"\n+    log_file = f\"api_call_logs_{timestamp}.json\"\n+    \n+    # 各DIFFに対してレビューを実行\n+    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n+        for diff in diffs:\n+            file_name = diff['new_path']\n+            diff_info = diff['diff']\n+            if not diff_info:\n+                continue\n+            f.write(f\"# レビュー対象：{file_name}\\n\")\n+            for perspective in review_perspectives:\n+                f.write(f\"## レビュー観点：{perspective}\\n\")\n+                review_result = review_code(perspective, file_name, diff_info, log_file)\n+                f.write(f\"{review_result}\\n\")\n+\n+if __name__ == \"__main__\":\n+    main()\n",
        "review_perspective": "関数の粒度が適切か",
        "request_tokens": 3178,
        "api_params": {
            "model": "gpt-4.1-nano",
            "temperature": 0.2
        },
        "response": "### 指摘対象コード:\n```python\ndef review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n    \"\"\"\n    指定された観点でコードレビューを実行し、結果をログに記録する。\n    ...\n    \"\"\"\n    # ファイル名と差分情報でシステムプロンプトをフォーマット\n    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n    messages = [\n        {\"role\": \"system\", \"content\": current_system_prompt},\n        {\"role\": \"user\", \"content\": review_perspective}\n    ]\n    ...\n    # OpenAI APIを呼び出してレビュー結果を取得\n    response_str = _call_openai_chat_api(\n        messages=messages,\n        model=MODEL_NAME,\n        temperature=TEMPERATURE,\n    )\n    ...\n    # ログ保存処理\n    log_data = {\n        \"file_name\": file_name,\n        \"diff_content\": diff_info,\n        \"review_perspective\": review_perspective,\n        \"request_tokens\": num_tokens_from_messages(messages),\n        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n        \"response\": response_str,\n        \"response_tokens\": num_tokens_from_messages(response_str),\n        \"start_time\": start_time,\n        \"end_time\": end_time,\n    }\n    ...\n```\n\n### 指摘内容:\n`review_code`関数は、コードレビューのための処理全体を一つの関数にまとめており、システムプロンプトの生成、API呼び出し、ログ記録までを一括して行っています。  \nこの関数は複数の責務を持ちすぎており、単一責任の原則に反しています。  \n\n### 修正提案:\n`review_code`を以下のように分割し、責務ごとに関数を分離してください。  \n- システムプロンプトの生成部分（`load_system_prompt()`とフォーマット処理）  \n- API呼び出し部分（`_call_openai_chat_api`の呼び出し）  \n- ログ記録部分（ログの保存処理）  \n\nこれにより、各関数の粒度が適切になり、テストやメンテナンスが容易になります。",
        "response_tokens": 613,
        "start_time": "2025/05/26 04:48:44.318578",
        "end_time": "2025/05/26 04:48:49.196120"
    },
    {
        "file_name": "code_review.py",
        "diff_content": "@@ -0,0 +1,229 @@\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n+\n+PRIVATE_TOKEN = os.getenv(\"GITLAB_TOKEN\")\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+HEADERS = {\n+    \"PRIVATE-TOKEN\": PRIVATE_TOKEN\n+}\n+\n+CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n+TIKTOKEN_ENCODING = tiktoken.encoding_for_model(TIKTOKEN_MODEL)\n+\n+def load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n+    \"\"\"\n+    レビュー観点をJSONファイルから読み込む。\n+\n+    Args:\n+        file_path (str): レビュー観点が記載されたJSONファイルのパス。\n+\n+    Returns:\n+        list: レビュー観点のリスト。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+def load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n+    \"\"\"\n+    システムプロンプトをテキストファイルから読み込む。\n+\n+    Args:\n+        file_path (str): システムプロンプトが記載されたテキストファイルのパス。\n+\n+    Returns:\n+        str: システムプロンプトの内容。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return f.read()\n+\n+def num_tokens_from_messages(messages) -> int:\n+    \"\"\"\n+    メッセージからトークン数を計算する。\n+\n+    Args:\n+        messages (list): メッセージのリスト。\n+\n+    Returns:\n+        int: メッセージに含まれるトークン数。\n+    \"\"\"\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += TOKENS_PER_MESSAGE\n+        for key, value in message.items():\n+            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n+            if key == \"name\":\n+                num_tokens += TOKENS_PER_NAME\n+    num_tokens += 3  # assistantの返信の開始トークン\n+    return num_tokens\n+\n+def _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n+    \"\"\"\n+    OpenAIのChat APIを呼び出す関数。\n+\n+    Args:\n+        messages (list): APIに送信するメッセージリスト。\n+        model (str): 使用するモデル名。\n+        temperature (float): 出力の多様性を制御する温度パラメータ。\n+        max_tokens (int): 最大トークン数。\n+\n+    Returns:\n+        str: OpenAI APIからのレスポンス。\n+    \"\"\"\n+    response = CLIENT.chat.completions.create(\n+        model=model,\n+        messages=messages,\n+        temperature=temperature,\n+        max_tokens=max_tokens\n+    )\n+    return response.choices[0].message.content\n+\n+def review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n+    \"\"\"\n+    指定された観点でコードレビューを実行し、結果をログに記録する。\n+\n+    Args:\n+        review_perspective (str): レビュー観点。\n+        file_name (str): 対象ファイル名。\n+        diff_info (str): 差分情報。\n+        log_file (str): ログを保存するファイル名。\n+\n+    Returns:\n+        str: レビュー結果。\n+    \"\"\"\n+    # ファイル名と差分情報でシステムプロンプトをフォーマット\n+    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n+    messages = [\n+        {\"role\": \"system\", \"content\": current_system_prompt},\n+        {\"role\": \"user\", \"content\": review_perspective}\n+    ]\n+\n+    start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+    # OpenAI APIを呼び出してレビュー結果を取得\n+    response_str = _call_openai_chat_api(\n+        messages=messages,\n+        model=MODEL_NAME,\n+        temperature=TEMPERATURE,\n+    )\n+    end_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+\n+    log_data = {\n+        \"file_name\": file_name,\n+        \"diff_content\": diff_info,\n+        \"review_perspective\": review_perspective,\n+        \"request_tokens\": num_tokens_from_messages(messages),\n+        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n+        \"response\": response_str,\n+        \"response_tokens\": num_tokens_from_messages(response_str),\n+        \"start_time\": start_time,\n+        \"end_time\": end_time,\n+    }\n+\n+    # 既存のログデータをロード（なければ空リストを使う）\n+    if os.path.exists(log_file):\n+        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n+            try:\n+                existing_logs = json.load(f)\n+            except json.JSONDecodeError:\n+                existing_logs = []\n+    else:\n+        existing_logs = []\n+\n+    # 新しいログデータを追加\n+    existing_logs.append(log_data)\n+    \n+    # ログをJSON形式で保存\n+    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n+        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n+    return response_str\n+\n+def get_mr_info(mr_iid) -> list:\n+    \"\"\"\n+    GitLabのマージリクエスト情報を取得する。\n+\n+    Args:\n+        mr_iid (int): マージリクエストのIID。\n+\n+    Returns:\n+        list: マージリクエストの差分情報リスト。\n+    \"\"\"\n+    base = f\"{GITLAB_URL}/api/v4\"\n+    # 1. MR基本情報\n+    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n+    # 2. 差分情報（Diff）\n+    #    MRの差分情報を取得するエンドポイント\n+    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n+\n+    all_diffs = []\n+    current_page = 1\n+\n+    while True:\n+        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n+        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n+        try:\n+            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n+        except requests.exceptions.HTTPError as e:\n+            print(f\"APIリクエストに失敗しました: {e}\")\n+            break \n+        try:\n+            page_data = response.json()\n+        except requests.exceptions.JSONDecodeError:\n+            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n+            break\n+        if not isinstance(page_data, list):\n+            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n+            break\n+        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n+            break\n+\n+        all_diffs.extend(page_data)\n+\n+        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n+        total_pages_header = response.headers.get(\"x-total-pages\")\n+        if total_pages_header and current_page >= int(total_pages_header):\n+            break # 最後のページまで到達\n+        current_page += 1\n+    return all_diffs\n+\n+def main():\n+    \"\"\"\n+    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n+    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n+    \"\"\"\n+    # コマンドライン引数を解析\n+    parser = argparse.ArgumentParser(description=\"GitLab MR情報を取得してコードレビューを実行します。\")\n+    parser.add_argument(\"--mr_iid\", type=int, required=True, help=\"マージリクエストのIIDを指定してください。\")\n+    args = parser.parse_args()\n+\n+    # GitLab MR情報を取得\n+    diffs = get_mr_info(args.mr_iid)\n+    # レビュー観点をロード\n+    review_perspectives = load_review_perspectives()\n+    \n+    # 出力ファイルの末尾に付与する現在時刻を取得してフォーマット\n+    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n+    # レビュー結果出力ファイル名とログファイル名を生成\n+    output_file = f\"review_results_{timestamp}.md\"\n+    log_file = f\"api_call_logs_{timestamp}.json\"\n+    \n+    # 各DIFFに対してレビューを実行\n+    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n+        for diff in diffs:\n+            file_name = diff['new_path']\n+            diff_info = diff['diff']\n+            if not diff_info:\n+                continue\n+            f.write(f\"# レビュー対象：{file_name}\\n\")\n+            for perspective in review_perspectives:\n+                f.write(f\"## レビュー観点：{perspective}\\n\")\n+                review_result = review_code(perspective, file_name, diff_info, log_file)\n+                f.write(f\"{review_result}\\n\")\n+\n+if __name__ == \"__main__\":\n+    main()\n",
        "review_perspective": "再利用性のあるコードになっているか",
        "request_tokens": 3181,
        "api_params": {
            "model": "gpt-4.1-nano",
            "temperature": 0.2
        },
        "response": "### 指摘対象コード:\n- `load_review_perspectives` 関数\n- `load_system_prompt` 関数\n- `_call_openai_chat_api` 関数\n- `review_code` 関数\n- `get_mr_info` 関数\n- `main` 関数\n\n### 指摘内容:\n全体的に、関数ごとに明確な責務を持ち、汎用的に利用できる構造になっています。  \n例えば、`load_review_perspectives`や`load_system_prompt`はファイルからの読み込みを抽象化しており、異なるファイルを指定することで再利用可能です。  \n`_call_openai_chat_api`もモデルやパラメータを引数にとるため、異なる設定で呼び出すことが容易です。  \n`review_code`は差分情報やレビュー観点を引数にとり、ログ保存も汎用的に行っているため、他の用途にも応用できる設計です。  \n`get_mr_info`もページネーション処理を汎用的に行っており、異なるAPIエンドポイントやパラメータに応用できる可能性があります。\n\nただし、`main`関数はコマンドライン引数やファイル名の生成に依存しており、再利用性を高めるには、処理の一部を関数化して外部から呼び出せる形にした方が良いです。\n\n### 修正提案:\n- `main`の処理を関数化し、引数を渡せるようにすることで、他のスクリプトやテストからも呼び出しやすくなる。\n- ファイルパスやAPIパラメータを引数化し、設定を外部から渡せるようにすると、より汎用的なコードになる。\n\n全体としては、十分に再利用性のある構造になっていますが、さらに柔軟性を持たせる余地があります。",
        "response_tokens": 589,
        "start_time": "2025/05/26 04:48:49.206774",
        "end_time": "2025/05/26 04:48:53.802275"
    },
    {
        "file_name": "code_review.py",
        "diff_content": "@@ -0,0 +1,229 @@\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n+\n+PRIVATE_TOKEN = os.getenv(\"GITLAB_TOKEN\")\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+HEADERS = {\n+    \"PRIVATE-TOKEN\": PRIVATE_TOKEN\n+}\n+\n+CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n+TIKTOKEN_ENCODING = tiktoken.encoding_for_model(TIKTOKEN_MODEL)\n+\n+def load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n+    \"\"\"\n+    レビュー観点をJSONファイルから読み込む。\n+\n+    Args:\n+        file_path (str): レビュー観点が記載されたJSONファイルのパス。\n+\n+    Returns:\n+        list: レビュー観点のリスト。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+def load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n+    \"\"\"\n+    システムプロンプトをテキストファイルから読み込む。\n+\n+    Args:\n+        file_path (str): システムプロンプトが記載されたテキストファイルのパス。\n+\n+    Returns:\n+        str: システムプロンプトの内容。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return f.read()\n+\n+def num_tokens_from_messages(messages) -> int:\n+    \"\"\"\n+    メッセージからトークン数を計算する。\n+\n+    Args:\n+        messages (list): メッセージのリスト。\n+\n+    Returns:\n+        int: メッセージに含まれるトークン数。\n+    \"\"\"\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += TOKENS_PER_MESSAGE\n+        for key, value in message.items():\n+            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n+            if key == \"name\":\n+                num_tokens += TOKENS_PER_NAME\n+    num_tokens += 3  # assistantの返信の開始トークン\n+    return num_tokens\n+\n+def _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n+    \"\"\"\n+    OpenAIのChat APIを呼び出す関数。\n+\n+    Args:\n+        messages (list): APIに送信するメッセージリスト。\n+        model (str): 使用するモデル名。\n+        temperature (float): 出力の多様性を制御する温度パラメータ。\n+        max_tokens (int): 最大トークン数。\n+\n+    Returns:\n+        str: OpenAI APIからのレスポンス。\n+    \"\"\"\n+    response = CLIENT.chat.completions.create(\n+        model=model,\n+        messages=messages,\n+        temperature=temperature,\n+        max_tokens=max_tokens\n+    )\n+    return response.choices[0].message.content\n+\n+def review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n+    \"\"\"\n+    指定された観点でコードレビューを実行し、結果をログに記録する。\n+\n+    Args:\n+        review_perspective (str): レビュー観点。\n+        file_name (str): 対象ファイル名。\n+        diff_info (str): 差分情報。\n+        log_file (str): ログを保存するファイル名。\n+\n+    Returns:\n+        str: レビュー結果。\n+    \"\"\"\n+    # ファイル名と差分情報でシステムプロンプトをフォーマット\n+    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n+    messages = [\n+        {\"role\": \"system\", \"content\": current_system_prompt},\n+        {\"role\": \"user\", \"content\": review_perspective}\n+    ]\n+\n+    start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+    # OpenAI APIを呼び出してレビュー結果を取得\n+    response_str = _call_openai_chat_api(\n+        messages=messages,\n+        model=MODEL_NAME,\n+        temperature=TEMPERATURE,\n+    )\n+    end_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+\n+    log_data = {\n+        \"file_name\": file_name,\n+        \"diff_content\": diff_info,\n+        \"review_perspective\": review_perspective,\n+        \"request_tokens\": num_tokens_from_messages(messages),\n+        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n+        \"response\": response_str,\n+        \"response_tokens\": num_tokens_from_messages(response_str),\n+        \"start_time\": start_time,\n+        \"end_time\": end_time,\n+    }\n+\n+    # 既存のログデータをロード（なければ空リストを使う）\n+    if os.path.exists(log_file):\n+        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n+            try:\n+                existing_logs = json.load(f)\n+            except json.JSONDecodeError:\n+                existing_logs = []\n+    else:\n+        existing_logs = []\n+\n+    # 新しいログデータを追加\n+    existing_logs.append(log_data)\n+    \n+    # ログをJSON形式で保存\n+    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n+        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n+    return response_str\n+\n+def get_mr_info(mr_iid) -> list:\n+    \"\"\"\n+    GitLabのマージリクエスト情報を取得する。\n+\n+    Args:\n+        mr_iid (int): マージリクエストのIID。\n+\n+    Returns:\n+        list: マージリクエストの差分情報リスト。\n+    \"\"\"\n+    base = f\"{GITLAB_URL}/api/v4\"\n+    # 1. MR基本情報\n+    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n+    # 2. 差分情報（Diff）\n+    #    MRの差分情報を取得するエンドポイント\n+    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n+\n+    all_diffs = []\n+    current_page = 1\n+\n+    while True:\n+        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n+        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n+        try:\n+            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n+        except requests.exceptions.HTTPError as e:\n+            print(f\"APIリクエストに失敗しました: {e}\")\n+            break \n+        try:\n+            page_data = response.json()\n+        except requests.exceptions.JSONDecodeError:\n+            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n+            break\n+        if not isinstance(page_data, list):\n+            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n+            break\n+        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n+            break\n+\n+        all_diffs.extend(page_data)\n+\n+        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n+        total_pages_header = response.headers.get(\"x-total-pages\")\n+        if total_pages_header and current_page >= int(total_pages_header):\n+            break # 最後のページまで到達\n+        current_page += 1\n+    return all_diffs\n+\n+def main():\n+    \"\"\"\n+    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n+    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n+    \"\"\"\n+    # コマンドライン引数を解析\n+    parser = argparse.ArgumentParser(description=\"GitLab MR情報を取得してコードレビューを実行します。\")\n+    parser.add_argument(\"--mr_iid\", type=int, required=True, help=\"マージリクエストのIIDを指定してください。\")\n+    args = parser.parse_args()\n+\n+    # GitLab MR情報を取得\n+    diffs = get_mr_info(args.mr_iid)\n+    # レビュー観点をロード\n+    review_perspectives = load_review_perspectives()\n+    \n+    # 出力ファイルの末尾に付与する現在時刻を取得してフォーマット\n+    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n+    # レビュー結果出力ファイル名とログファイル名を生成\n+    output_file = f\"review_results_{timestamp}.md\"\n+    log_file = f\"api_call_logs_{timestamp}.json\"\n+    \n+    # 各DIFFに対してレビューを実行\n+    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n+        for diff in diffs:\n+            file_name = diff['new_path']\n+            diff_info = diff['diff']\n+            if not diff_info:\n+                continue\n+            f.write(f\"# レビュー対象：{file_name}\\n\")\n+            for perspective in review_perspectives:\n+                f.write(f\"## レビュー観点：{perspective}\\n\")\n+                review_result = review_code(perspective, file_name, diff_info, log_file)\n+                f.write(f\"{review_result}\\n\")\n+\n+if __name__ == \"__main__\":\n+    main()\n",
        "review_perspective": "ハードコードされた値が定数化されているか",
        "request_tokens": 3184,
        "api_params": {
            "model": "gpt-4.1-nano",
            "temperature": 0.2
        },
        "response": "### 指摘対象コード:\n```python\n+GIT_API_PER_PAGE = 100\n+MODEL_NAME = \"gpt-3.5-turbo\"\n+TEMPERATURE = 0.3\n+TIKTOKEN_MODEL = \"gpt-3.5-turbo\"\n+TOKENS_PER_MESSAGE = 4\n+TOKENS_PER_NAME = 1\n```\n\n### 指摘内容:\n`GIT_API_PER_PAGE`, `MODEL_NAME`, `TEMPERATURE`, `TIKTOKEN_MODEL`, `TOKENS_PER_MESSAGE`, `TOKENS_PER_NAME` などの値は、コード内で定数として定義されており、ハードコードされた値が定数化されています。\n\n### 修正提案:\n特に修正は不要です。これらの値は定数として定義されており、必要に応じて変更しやすくなっています。定数化されている点は良好です。",
        "response_tokens": 234,
        "start_time": "2025/05/26 04:48:53.812920",
        "end_time": "2025/05/26 04:48:56.315384"
    },
    {
        "file_name": "code_review.py",
        "diff_content": "@@ -0,0 +1,229 @@\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n+\n+PRIVATE_TOKEN = os.getenv(\"GITLAB_TOKEN\")\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+HEADERS = {\n+    \"PRIVATE-TOKEN\": PRIVATE_TOKEN\n+}\n+\n+CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n+TIKTOKEN_ENCODING = tiktoken.encoding_for_model(TIKTOKEN_MODEL)\n+\n+def load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n+    \"\"\"\n+    レビュー観点をJSONファイルから読み込む。\n+\n+    Args:\n+        file_path (str): レビュー観点が記載されたJSONファイルのパス。\n+\n+    Returns:\n+        list: レビュー観点のリスト。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+def load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n+    \"\"\"\n+    システムプロンプトをテキストファイルから読み込む。\n+\n+    Args:\n+        file_path (str): システムプロンプトが記載されたテキストファイルのパス。\n+\n+    Returns:\n+        str: システムプロンプトの内容。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return f.read()\n+\n+def num_tokens_from_messages(messages) -> int:\n+    \"\"\"\n+    メッセージからトークン数を計算する。\n+\n+    Args:\n+        messages (list): メッセージのリスト。\n+\n+    Returns:\n+        int: メッセージに含まれるトークン数。\n+    \"\"\"\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += TOKENS_PER_MESSAGE\n+        for key, value in message.items():\n+            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n+            if key == \"name\":\n+                num_tokens += TOKENS_PER_NAME\n+    num_tokens += 3  # assistantの返信の開始トークン\n+    return num_tokens\n+\n+def _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n+    \"\"\"\n+    OpenAIのChat APIを呼び出す関数。\n+\n+    Args:\n+        messages (list): APIに送信するメッセージリスト。\n+        model (str): 使用するモデル名。\n+        temperature (float): 出力の多様性を制御する温度パラメータ。\n+        max_tokens (int): 最大トークン数。\n+\n+    Returns:\n+        str: OpenAI APIからのレスポンス。\n+    \"\"\"\n+    response = CLIENT.chat.completions.create(\n+        model=model,\n+        messages=messages,\n+        temperature=temperature,\n+        max_tokens=max_tokens\n+    )\n+    return response.choices[0].message.content\n+\n+def review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n+    \"\"\"\n+    指定された観点でコードレビューを実行し、結果をログに記録する。\n+\n+    Args:\n+        review_perspective (str): レビュー観点。\n+        file_name (str): 対象ファイル名。\n+        diff_info (str): 差分情報。\n+        log_file (str): ログを保存するファイル名。\n+\n+    Returns:\n+        str: レビュー結果。\n+    \"\"\"\n+    # ファイル名と差分情報でシステムプロンプトをフォーマット\n+    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n+    messages = [\n+        {\"role\": \"system\", \"content\": current_system_prompt},\n+        {\"role\": \"user\", \"content\": review_perspective}\n+    ]\n+\n+    start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+    # OpenAI APIを呼び出してレビュー結果を取得\n+    response_str = _call_openai_chat_api(\n+        messages=messages,\n+        model=MODEL_NAME,\n+        temperature=TEMPERATURE,\n+    )\n+    end_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+\n+    log_data = {\n+        \"file_name\": file_name,\n+        \"diff_content\": diff_info,\n+        \"review_perspective\": review_perspective,\n+        \"request_tokens\": num_tokens_from_messages(messages),\n+        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n+        \"response\": response_str,\n+        \"response_tokens\": num_tokens_from_messages(response_str),\n+        \"start_time\": start_time,\n+        \"end_time\": end_time,\n+    }\n+\n+    # 既存のログデータをロード（なければ空リストを使う）\n+    if os.path.exists(log_file):\n+        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n+            try:\n+                existing_logs = json.load(f)\n+            except json.JSONDecodeError:\n+                existing_logs = []\n+    else:\n+        existing_logs = []\n+\n+    # 新しいログデータを追加\n+    existing_logs.append(log_data)\n+    \n+    # ログをJSON形式で保存\n+    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n+        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n+    return response_str\n+\n+def get_mr_info(mr_iid) -> list:\n+    \"\"\"\n+    GitLabのマージリクエスト情報を取得する。\n+\n+    Args:\n+        mr_iid (int): マージリクエストのIID。\n+\n+    Returns:\n+        list: マージリクエストの差分情報リスト。\n+    \"\"\"\n+    base = f\"{GITLAB_URL}/api/v4\"\n+    # 1. MR基本情報\n+    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n+    # 2. 差分情報（Diff）\n+    #    MRの差分情報を取得するエンドポイント\n+    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n+\n+    all_diffs = []\n+    current_page = 1\n+\n+    while True:\n+        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n+        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n+        try:\n+            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n+        except requests.exceptions.HTTPError as e:\n+            print(f\"APIリクエストに失敗しました: {e}\")\n+            break \n+        try:\n+            page_data = response.json()\n+        except requests.exceptions.JSONDecodeError:\n+            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n+            break\n+        if not isinstance(page_data, list):\n+            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n+            break\n+        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n+            break\n+\n+        all_diffs.extend(page_data)\n+\n+        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n+        total_pages_header = response.headers.get(\"x-total-pages\")\n+        if total_pages_header and current_page >= int(total_pages_header):\n+            break # 最後のページまで到達\n+        current_page += 1\n+    return all_diffs\n+\n+def main():\n+    \"\"\"\n+    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n+    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n+    \"\"\"\n+    # コマンドライン引数を解析\n+    parser = argparse.ArgumentParser(description=\"GitLab MR情報を取得してコードレビューを実行します。\")\n+    parser.add_argument(\"--mr_iid\", type=int, required=True, help=\"マージリクエストのIIDを指定してください。\")\n+    args = parser.parse_args()\n+\n+    # GitLab MR情報を取得\n+    diffs = get_mr_info(args.mr_iid)\n+    # レビュー観点をロード\n+    review_perspectives = load_review_perspectives()\n+    \n+    # 出力ファイルの末尾に付与する現在時刻を取得してフォーマット\n+    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n+    # レビュー結果出力ファイル名とログファイル名を生成\n+    output_file = f\"review_results_{timestamp}.md\"\n+    log_file = f\"api_call_logs_{timestamp}.json\"\n+    \n+    # 各DIFFに対してレビューを実行\n+    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n+        for diff in diffs:\n+            file_name = diff['new_path']\n+            diff_info = diff['diff']\n+            if not diff_info:\n+                continue\n+            f.write(f\"# レビュー対象：{file_name}\\n\")\n+            for perspective in review_perspectives:\n+                f.write(f\"## レビュー観点：{perspective}\\n\")\n+                review_result = review_code(perspective, file_name, diff_info, log_file)\n+                f.write(f\"{review_result}\\n\")\n+\n+if __name__ == \"__main__\":\n+    main()\n",
        "review_perspective": "設定やパラメータが外部化されているか",
        "request_tokens": 3184,
        "api_params": {
            "model": "gpt-4.1-nano",
            "temperature": 0.2
        },
        "response": "この観点では問題ありません。  \nコード内の設定やパラメータは、`constants`モジュールからインポートされており、`GITLAB_URL`や`PROJECT_ID`、`MODEL_NAME`などの値は外部ファイルや環境変数に依存していることが示唆されています。  \nまた、APIキーやトークンも環境変数から取得しており、コード内にハードコーディングされていません。  \nこれにより、設定やパラメータの外部化が適切に行われていると評価できます。",
        "response_tokens": 184,
        "start_time": "2025/05/26 04:48:56.323076",
        "end_time": "2025/05/26 04:48:58.068725"
    },
    {
        "file_name": "code_review.py",
        "diff_content": "@@ -0,0 +1,229 @@\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n+\n+PRIVATE_TOKEN = os.getenv(\"GITLAB_TOKEN\")\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+HEADERS = {\n+    \"PRIVATE-TOKEN\": PRIVATE_TOKEN\n+}\n+\n+CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n+TIKTOKEN_ENCODING = tiktoken.encoding_for_model(TIKTOKEN_MODEL)\n+\n+def load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n+    \"\"\"\n+    レビュー観点をJSONファイルから読み込む。\n+\n+    Args:\n+        file_path (str): レビュー観点が記載されたJSONファイルのパス。\n+\n+    Returns:\n+        list: レビュー観点のリスト。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+def load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n+    \"\"\"\n+    システムプロンプトをテキストファイルから読み込む。\n+\n+    Args:\n+        file_path (str): システムプロンプトが記載されたテキストファイルのパス。\n+\n+    Returns:\n+        str: システムプロンプトの内容。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return f.read()\n+\n+def num_tokens_from_messages(messages) -> int:\n+    \"\"\"\n+    メッセージからトークン数を計算する。\n+\n+    Args:\n+        messages (list): メッセージのリスト。\n+\n+    Returns:\n+        int: メッセージに含まれるトークン数。\n+    \"\"\"\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += TOKENS_PER_MESSAGE\n+        for key, value in message.items():\n+            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n+            if key == \"name\":\n+                num_tokens += TOKENS_PER_NAME\n+    num_tokens += 3  # assistantの返信の開始トークン\n+    return num_tokens\n+\n+def _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n+    \"\"\"\n+    OpenAIのChat APIを呼び出す関数。\n+\n+    Args:\n+        messages (list): APIに送信するメッセージリスト。\n+        model (str): 使用するモデル名。\n+        temperature (float): 出力の多様性を制御する温度パラメータ。\n+        max_tokens (int): 最大トークン数。\n+\n+    Returns:\n+        str: OpenAI APIからのレスポンス。\n+    \"\"\"\n+    response = CLIENT.chat.completions.create(\n+        model=model,\n+        messages=messages,\n+        temperature=temperature,\n+        max_tokens=max_tokens\n+    )\n+    return response.choices[0].message.content\n+\n+def review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n+    \"\"\"\n+    指定された観点でコードレビューを実行し、結果をログに記録する。\n+\n+    Args:\n+        review_perspective (str): レビュー観点。\n+        file_name (str): 対象ファイル名。\n+        diff_info (str): 差分情報。\n+        log_file (str): ログを保存するファイル名。\n+\n+    Returns:\n+        str: レビュー結果。\n+    \"\"\"\n+    # ファイル名と差分情報でシステムプロンプトをフォーマット\n+    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n+    messages = [\n+        {\"role\": \"system\", \"content\": current_system_prompt},\n+        {\"role\": \"user\", \"content\": review_perspective}\n+    ]\n+\n+    start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+    # OpenAI APIを呼び出してレビュー結果を取得\n+    response_str = _call_openai_chat_api(\n+        messages=messages,\n+        model=MODEL_NAME,\n+        temperature=TEMPERATURE,\n+    )\n+    end_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+\n+    log_data = {\n+        \"file_name\": file_name,\n+        \"diff_content\": diff_info,\n+        \"review_perspective\": review_perspective,\n+        \"request_tokens\": num_tokens_from_messages(messages),\n+        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n+        \"response\": response_str,\n+        \"response_tokens\": num_tokens_from_messages(response_str),\n+        \"start_time\": start_time,\n+        \"end_time\": end_time,\n+    }\n+\n+    # 既存のログデータをロード（なければ空リストを使う）\n+    if os.path.exists(log_file):\n+        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n+            try:\n+                existing_logs = json.load(f)\n+            except json.JSONDecodeError:\n+                existing_logs = []\n+    else:\n+        existing_logs = []\n+\n+    # 新しいログデータを追加\n+    existing_logs.append(log_data)\n+    \n+    # ログをJSON形式で保存\n+    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n+        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n+    return response_str\n+\n+def get_mr_info(mr_iid) -> list:\n+    \"\"\"\n+    GitLabのマージリクエスト情報を取得する。\n+\n+    Args:\n+        mr_iid (int): マージリクエストのIID。\n+\n+    Returns:\n+        list: マージリクエストの差分情報リスト。\n+    \"\"\"\n+    base = f\"{GITLAB_URL}/api/v4\"\n+    # 1. MR基本情報\n+    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n+    # 2. 差分情報（Diff）\n+    #    MRの差分情報を取得するエンドポイント\n+    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n+\n+    all_diffs = []\n+    current_page = 1\n+\n+    while True:\n+        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n+        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n+        try:\n+            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n+        except requests.exceptions.HTTPError as e:\n+            print(f\"APIリクエストに失敗しました: {e}\")\n+            break \n+        try:\n+            page_data = response.json()\n+        except requests.exceptions.JSONDecodeError:\n+            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n+            break\n+        if not isinstance(page_data, list):\n+            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n+            break\n+        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n+            break\n+\n+        all_diffs.extend(page_data)\n+\n+        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n+        total_pages_header = response.headers.get(\"x-total-pages\")\n+        if total_pages_header and current_page >= int(total_pages_header):\n+            break # 最後のページまで到達\n+        current_page += 1\n+    return all_diffs\n+\n+def main():\n+    \"\"\"\n+    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n+    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n+    \"\"\"\n+    # コマンドライン引数を解析\n+    parser = argparse.ArgumentParser(description=\"GitLab MR情報を取得してコードレビューを実行します。\")\n+    parser.add_argument(\"--mr_iid\", type=int, required=True, help=\"マージリクエストのIIDを指定してください。\")\n+    args = parser.parse_args()\n+\n+    # GitLab MR情報を取得\n+    diffs = get_mr_info(args.mr_iid)\n+    # レビュー観点をロード\n+    review_perspectives = load_review_perspectives()\n+    \n+    # 出力ファイルの末尾に付与する現在時刻を取得してフォーマット\n+    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n+    # レビュー結果出力ファイル名とログファイル名を生成\n+    output_file = f\"review_results_{timestamp}.md\"\n+    log_file = f\"api_call_logs_{timestamp}.json\"\n+    \n+    # 各DIFFに対してレビューを実行\n+    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n+        for diff in diffs:\n+            file_name = diff['new_path']\n+            diff_info = diff['diff']\n+            if not diff_info:\n+                continue\n+            f.write(f\"# レビュー対象：{file_name}\\n\")\n+            for perspective in review_perspectives:\n+                f.write(f\"## レビュー観点：{perspective}\\n\")\n+                review_result = review_code(perspective, file_name, diff_info, log_file)\n+                f.write(f\"{review_result}\\n\")\n+\n+if __name__ == \"__main__\":\n+    main()\n",
        "review_perspective": "例外処理が適切に行われているか",
        "request_tokens": 3184,
        "api_params": {
            "model": "gpt-4.1-nano",
            "temperature": 0.2
        },
        "response": "### 指摘対象コード:\n```python\nresponse = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\ntry:\n    response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\nexcept requests.exceptions.HTTPError as e:\n    print(f\"APIリクエストに失敗しました: {e}\")\n    break \ntry:\n    page_data = response.json()\nexcept requests.exceptions.JSONDecodeError:\n    print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n    break\nif not isinstance(page_data, list):\n    print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n    break\n```\n\n### 指摘内容:\nAPIリクエストやレスポンスの処理において例外処理は行われていますが、例外発生時に単にエラーメッセージを出力してループを抜けるだけで、呼び出し元や上位処理に例外を伝播させていません。そのため、エラーが発生した場合に処理の継続や適切なエラーハンドリングができず、問題の根本的な原因を把握しづらくなる可能性があります。\n\n### 修正提案:\n例外発生時にエラーをログに記録した上で、必要に応じて例外を再スローするか、エラーに応じた適切な処理を行うべきです。例えば、`raise`を用いて例外を再スローし、呼び出し側で適切にハンドリングできるようにするか、またはエラーに応じてループを継続させるロジックを追加してください。\n\n```python\ntry:\n    response.raise_for_status()\nexcept requests.exceptions.HTTPError as e:\n    print(f\"APIリクエストに失敗しました: {e}\")\n    raise  # 例外を再スローして呼び出し側に通知\n```\n\nまたは、エラー時にループを続行させる場合は、`continue`を用いるなどの対応も検討してください。",
        "response_tokens": 629,
        "start_time": "2025/05/26 04:48:58.076175",
        "end_time": "2025/05/26 04:49:02.716369"
    },
    {
        "file_name": "code_review.py",
        "diff_content": "@@ -0,0 +1,229 @@\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n+\n+PRIVATE_TOKEN = os.getenv(\"GITLAB_TOKEN\")\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+HEADERS = {\n+    \"PRIVATE-TOKEN\": PRIVATE_TOKEN\n+}\n+\n+CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n+TIKTOKEN_ENCODING = tiktoken.encoding_for_model(TIKTOKEN_MODEL)\n+\n+def load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n+    \"\"\"\n+    レビュー観点をJSONファイルから読み込む。\n+\n+    Args:\n+        file_path (str): レビュー観点が記載されたJSONファイルのパス。\n+\n+    Returns:\n+        list: レビュー観点のリスト。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+def load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n+    \"\"\"\n+    システムプロンプトをテキストファイルから読み込む。\n+\n+    Args:\n+        file_path (str): システムプロンプトが記載されたテキストファイルのパス。\n+\n+    Returns:\n+        str: システムプロンプトの内容。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return f.read()\n+\n+def num_tokens_from_messages(messages) -> int:\n+    \"\"\"\n+    メッセージからトークン数を計算する。\n+\n+    Args:\n+        messages (list): メッセージのリスト。\n+\n+    Returns:\n+        int: メッセージに含まれるトークン数。\n+    \"\"\"\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += TOKENS_PER_MESSAGE\n+        for key, value in message.items():\n+            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n+            if key == \"name\":\n+                num_tokens += TOKENS_PER_NAME\n+    num_tokens += 3  # assistantの返信の開始トークン\n+    return num_tokens\n+\n+def _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n+    \"\"\"\n+    OpenAIのChat APIを呼び出す関数。\n+\n+    Args:\n+        messages (list): APIに送信するメッセージリスト。\n+        model (str): 使用するモデル名。\n+        temperature (float): 出力の多様性を制御する温度パラメータ。\n+        max_tokens (int): 最大トークン数。\n+\n+    Returns:\n+        str: OpenAI APIからのレスポンス。\n+    \"\"\"\n+    response = CLIENT.chat.completions.create(\n+        model=model,\n+        messages=messages,\n+        temperature=temperature,\n+        max_tokens=max_tokens\n+    )\n+    return response.choices[0].message.content\n+\n+def review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n+    \"\"\"\n+    指定された観点でコードレビューを実行し、結果をログに記録する。\n+\n+    Args:\n+        review_perspective (str): レビュー観点。\n+        file_name (str): 対象ファイル名。\n+        diff_info (str): 差分情報。\n+        log_file (str): ログを保存するファイル名。\n+\n+    Returns:\n+        str: レビュー結果。\n+    \"\"\"\n+    # ファイル名と差分情報でシステムプロンプトをフォーマット\n+    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n+    messages = [\n+        {\"role\": \"system\", \"content\": current_system_prompt},\n+        {\"role\": \"user\", \"content\": review_perspective}\n+    ]\n+\n+    start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+    # OpenAI APIを呼び出してレビュー結果を取得\n+    response_str = _call_openai_chat_api(\n+        messages=messages,\n+        model=MODEL_NAME,\n+        temperature=TEMPERATURE,\n+    )\n+    end_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+\n+    log_data = {\n+        \"file_name\": file_name,\n+        \"diff_content\": diff_info,\n+        \"review_perspective\": review_perspective,\n+        \"request_tokens\": num_tokens_from_messages(messages),\n+        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n+        \"response\": response_str,\n+        \"response_tokens\": num_tokens_from_messages(response_str),\n+        \"start_time\": start_time,\n+        \"end_time\": end_time,\n+    }\n+\n+    # 既存のログデータをロード（なければ空リストを使う）\n+    if os.path.exists(log_file):\n+        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n+            try:\n+                existing_logs = json.load(f)\n+            except json.JSONDecodeError:\n+                existing_logs = []\n+    else:\n+        existing_logs = []\n+\n+    # 新しいログデータを追加\n+    existing_logs.append(log_data)\n+    \n+    # ログをJSON形式で保存\n+    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n+        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n+    return response_str\n+\n+def get_mr_info(mr_iid) -> list:\n+    \"\"\"\n+    GitLabのマージリクエスト情報を取得する。\n+\n+    Args:\n+        mr_iid (int): マージリクエストのIID。\n+\n+    Returns:\n+        list: マージリクエストの差分情報リスト。\n+    \"\"\"\n+    base = f\"{GITLAB_URL}/api/v4\"\n+    # 1. MR基本情報\n+    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n+    # 2. 差分情報（Diff）\n+    #    MRの差分情報を取得するエンドポイント\n+    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n+\n+    all_diffs = []\n+    current_page = 1\n+\n+    while True:\n+        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n+        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n+        try:\n+            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n+        except requests.exceptions.HTTPError as e:\n+            print(f\"APIリクエストに失敗しました: {e}\")\n+            break \n+        try:\n+            page_data = response.json()\n+        except requests.exceptions.JSONDecodeError:\n+            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n+            break\n+        if not isinstance(page_data, list):\n+            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n+            break\n+        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n+            break\n+\n+        all_diffs.extend(page_data)\n+\n+        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n+        total_pages_header = response.headers.get(\"x-total-pages\")\n+        if total_pages_header and current_page >= int(total_pages_header):\n+            break # 最後のページまで到達\n+        current_page += 1\n+    return all_diffs\n+\n+def main():\n+    \"\"\"\n+    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n+    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n+    \"\"\"\n+    # コマンドライン引数を解析\n+    parser = argparse.ArgumentParser(description=\"GitLab MR情報を取得してコードレビューを実行します。\")\n+    parser.add_argument(\"--mr_iid\", type=int, required=True, help=\"マージリクエストのIIDを指定してください。\")\n+    args = parser.parse_args()\n+\n+    # GitLab MR情報を取得\n+    diffs = get_mr_info(args.mr_iid)\n+    # レビュー観点をロード\n+    review_perspectives = load_review_perspectives()\n+    \n+    # 出力ファイルの末尾に付与する現在時刻を取得してフォーマット\n+    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n+    # レビュー結果出力ファイル名とログファイル名を生成\n+    output_file = f\"review_results_{timestamp}.md\"\n+    log_file = f\"api_call_logs_{timestamp}.json\"\n+    \n+    # 各DIFFに対してレビューを実行\n+    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n+        for diff in diffs:\n+            file_name = diff['new_path']\n+            diff_info = diff['diff']\n+            if not diff_info:\n+                continue\n+            f.write(f\"# レビュー対象：{file_name}\\n\")\n+            for perspective in review_perspectives:\n+                f.write(f\"## レビュー観点：{perspective}\\n\")\n+                review_result = review_code(perspective, file_name, diff_info, log_file)\n+                f.write(f\"{review_result}\\n\")\n+\n+if __name__ == \"__main__\":\n+    main()\n",
        "review_perspective": "エラーメッセージが明確か",
        "request_tokens": 3178,
        "api_params": {
            "model": "gpt-4.1-nano",
            "temperature": 0.2
        },
        "response": "この観点では問題ありません。",
        "response_tokens": 13,
        "start_time": "2025/05/26 04:49:02.726884",
        "end_time": "2025/05/26 04:49:03.325505"
    },
    {
        "file_name": "code_review.py",
        "diff_content": "@@ -0,0 +1,229 @@\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n+\n+PRIVATE_TOKEN = os.getenv(\"GITLAB_TOKEN\")\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+HEADERS = {\n+    \"PRIVATE-TOKEN\": PRIVATE_TOKEN\n+}\n+\n+CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n+TIKTOKEN_ENCODING = tiktoken.encoding_for_model(TIKTOKEN_MODEL)\n+\n+def load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n+    \"\"\"\n+    レビュー観点をJSONファイルから読み込む。\n+\n+    Args:\n+        file_path (str): レビュー観点が記載されたJSONファイルのパス。\n+\n+    Returns:\n+        list: レビュー観点のリスト。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+def load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n+    \"\"\"\n+    システムプロンプトをテキストファイルから読み込む。\n+\n+    Args:\n+        file_path (str): システムプロンプトが記載されたテキストファイルのパス。\n+\n+    Returns:\n+        str: システムプロンプトの内容。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return f.read()\n+\n+def num_tokens_from_messages(messages) -> int:\n+    \"\"\"\n+    メッセージからトークン数を計算する。\n+\n+    Args:\n+        messages (list): メッセージのリスト。\n+\n+    Returns:\n+        int: メッセージに含まれるトークン数。\n+    \"\"\"\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += TOKENS_PER_MESSAGE\n+        for key, value in message.items():\n+            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n+            if key == \"name\":\n+                num_tokens += TOKENS_PER_NAME\n+    num_tokens += 3  # assistantの返信の開始トークン\n+    return num_tokens\n+\n+def _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n+    \"\"\"\n+    OpenAIのChat APIを呼び出す関数。\n+\n+    Args:\n+        messages (list): APIに送信するメッセージリスト。\n+        model (str): 使用するモデル名。\n+        temperature (float): 出力の多様性を制御する温度パラメータ。\n+        max_tokens (int): 最大トークン数。\n+\n+    Returns:\n+        str: OpenAI APIからのレスポンス。\n+    \"\"\"\n+    response = CLIENT.chat.completions.create(\n+        model=model,\n+        messages=messages,\n+        temperature=temperature,\n+        max_tokens=max_tokens\n+    )\n+    return response.choices[0].message.content\n+\n+def review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n+    \"\"\"\n+    指定された観点でコードレビューを実行し、結果をログに記録する。\n+\n+    Args:\n+        review_perspective (str): レビュー観点。\n+        file_name (str): 対象ファイル名。\n+        diff_info (str): 差分情報。\n+        log_file (str): ログを保存するファイル名。\n+\n+    Returns:\n+        str: レビュー結果。\n+    \"\"\"\n+    # ファイル名と差分情報でシステムプロンプトをフォーマット\n+    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n+    messages = [\n+        {\"role\": \"system\", \"content\": current_system_prompt},\n+        {\"role\": \"user\", \"content\": review_perspective}\n+    ]\n+\n+    start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+    # OpenAI APIを呼び出してレビュー結果を取得\n+    response_str = _call_openai_chat_api(\n+        messages=messages,\n+        model=MODEL_NAME,\n+        temperature=TEMPERATURE,\n+    )\n+    end_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+\n+    log_data = {\n+        \"file_name\": file_name,\n+        \"diff_content\": diff_info,\n+        \"review_perspective\": review_perspective,\n+        \"request_tokens\": num_tokens_from_messages(messages),\n+        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n+        \"response\": response_str,\n+        \"response_tokens\": num_tokens_from_messages(response_str),\n+        \"start_time\": start_time,\n+        \"end_time\": end_time,\n+    }\n+\n+    # 既存のログデータをロード（なければ空リストを使う）\n+    if os.path.exists(log_file):\n+        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n+            try:\n+                existing_logs = json.load(f)\n+            except json.JSONDecodeError:\n+                existing_logs = []\n+    else:\n+        existing_logs = []\n+\n+    # 新しいログデータを追加\n+    existing_logs.append(log_data)\n+    \n+    # ログをJSON形式で保存\n+    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n+        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n+    return response_str\n+\n+def get_mr_info(mr_iid) -> list:\n+    \"\"\"\n+    GitLabのマージリクエスト情報を取得する。\n+\n+    Args:\n+        mr_iid (int): マージリクエストのIID。\n+\n+    Returns:\n+        list: マージリクエストの差分情報リスト。\n+    \"\"\"\n+    base = f\"{GITLAB_URL}/api/v4\"\n+    # 1. MR基本情報\n+    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n+    # 2. 差分情報（Diff）\n+    #    MRの差分情報を取得するエンドポイント\n+    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n+\n+    all_diffs = []\n+    current_page = 1\n+\n+    while True:\n+        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n+        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n+        try:\n+            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n+        except requests.exceptions.HTTPError as e:\n+            print(f\"APIリクエストに失敗しました: {e}\")\n+            break \n+        try:\n+            page_data = response.json()\n+        except requests.exceptions.JSONDecodeError:\n+            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n+            break\n+        if not isinstance(page_data, list):\n+            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n+            break\n+        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n+            break\n+\n+        all_diffs.extend(page_data)\n+\n+        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n+        total_pages_header = response.headers.get(\"x-total-pages\")\n+        if total_pages_header and current_page >= int(total_pages_header):\n+            break # 最後のページまで到達\n+        current_page += 1\n+    return all_diffs\n+\n+def main():\n+    \"\"\"\n+    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n+    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n+    \"\"\"\n+    # コマンドライン引数を解析\n+    parser = argparse.ArgumentParser(description=\"GitLab MR情報を取得してコードレビューを実行します。\")\n+    parser.add_argument(\"--mr_iid\", type=int, required=True, help=\"マージリクエストのIIDを指定してください。\")\n+    args = parser.parse_args()\n+\n+    # GitLab MR情報を取得\n+    diffs = get_mr_info(args.mr_iid)\n+    # レビュー観点をロード\n+    review_perspectives = load_review_perspectives()\n+    \n+    # 出力ファイルの末尾に付与する現在時刻を取得してフォーマット\n+    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n+    # レビュー結果出力ファイル名とログファイル名を生成\n+    output_file = f\"review_results_{timestamp}.md\"\n+    log_file = f\"api_call_logs_{timestamp}.json\"\n+    \n+    # 各DIFFに対してレビューを実行\n+    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n+        for diff in diffs:\n+            file_name = diff['new_path']\n+            diff_info = diff['diff']\n+            if not diff_info:\n+                continue\n+            f.write(f\"# レビュー対象：{file_name}\\n\")\n+            for perspective in review_perspectives:\n+                f.write(f\"## レビュー観点：{perspective}\\n\")\n+                review_result = review_code(perspective, file_name, diff_info, log_file)\n+                f.write(f\"{review_result}\\n\")\n+\n+if __name__ == \"__main__\":\n+    main()\n",
        "review_perspective": "入力値の検証・バリデーションがされているか",
        "request_tokens": 3190,
        "api_params": {
            "model": "gpt-4.1-nano",
            "temperature": 0.2
        },
        "response": "### 指摘対象コード:\n```python\ndef review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n    # ファイル名と差分情報でシステムプロンプトをフォーマット\n    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n    messages = [\n        {\"role\": \"system\", \"content\": current_system_prompt},\n        {\"role\": \"user\", \"content\": review_perspective}\n    ]\n    ...\n```\n\n### 指摘内容:\n`review_code`関数内で、`file_name`と`diff_info`の入力値に対して検証やバリデーションが行われていません。特に`diff_info`は差分情報として期待されるフォーマットや内容が不明な場合もあり、空文字や不正なデータが渡された場合にシステムプロンプトのフォーマット崩れやエラーの原因となる可能性があります。\n\n### 修正提案:\n`file_name`と`diff_info`に対して、以下のような検証を追加すると良いでしょう。\n\n```python\ndef review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n    # 入力値の検証\n    if not isinstance(file_name, str) or not file_name.strip():\n        raise ValueError(\"file_nameが無効です。空文字または文字列である必要があります。\")\n    if not isinstance(diff_info, str) or not diff_info.strip():\n        raise ValueError(\"diff_infoが無効です。空文字または文字列である必要があります。\")\n    # さらに必要に応じて、diff_infoの内容の妥当性も検証可能\n\n    # ファイル名と差分情報でシステムプロンプトをフォーマット\n    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n    ...\n```\n\nこれにより、不正な入力によるエラーや予期しない動作を未然に防ぐことができます。",
        "response_tokens": 582,
        "start_time": "2025/05/26 04:49:03.332920",
        "end_time": "2025/05/26 04:49:07.832836"
    },
    {
        "file_name": "code_review.py",
        "diff_content": "@@ -0,0 +1,229 @@\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n+\n+PRIVATE_TOKEN = os.getenv(\"GITLAB_TOKEN\")\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+HEADERS = {\n+    \"PRIVATE-TOKEN\": PRIVATE_TOKEN\n+}\n+\n+CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n+TIKTOKEN_ENCODING = tiktoken.encoding_for_model(TIKTOKEN_MODEL)\n+\n+def load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n+    \"\"\"\n+    レビュー観点をJSONファイルから読み込む。\n+\n+    Args:\n+        file_path (str): レビュー観点が記載されたJSONファイルのパス。\n+\n+    Returns:\n+        list: レビュー観点のリスト。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+def load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n+    \"\"\"\n+    システムプロンプトをテキストファイルから読み込む。\n+\n+    Args:\n+        file_path (str): システムプロンプトが記載されたテキストファイルのパス。\n+\n+    Returns:\n+        str: システムプロンプトの内容。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return f.read()\n+\n+def num_tokens_from_messages(messages) -> int:\n+    \"\"\"\n+    メッセージからトークン数を計算する。\n+\n+    Args:\n+        messages (list): メッセージのリスト。\n+\n+    Returns:\n+        int: メッセージに含まれるトークン数。\n+    \"\"\"\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += TOKENS_PER_MESSAGE\n+        for key, value in message.items():\n+            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n+            if key == \"name\":\n+                num_tokens += TOKENS_PER_NAME\n+    num_tokens += 3  # assistantの返信の開始トークン\n+    return num_tokens\n+\n+def _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n+    \"\"\"\n+    OpenAIのChat APIを呼び出す関数。\n+\n+    Args:\n+        messages (list): APIに送信するメッセージリスト。\n+        model (str): 使用するモデル名。\n+        temperature (float): 出力の多様性を制御する温度パラメータ。\n+        max_tokens (int): 最大トークン数。\n+\n+    Returns:\n+        str: OpenAI APIからのレスポンス。\n+    \"\"\"\n+    response = CLIENT.chat.completions.create(\n+        model=model,\n+        messages=messages,\n+        temperature=temperature,\n+        max_tokens=max_tokens\n+    )\n+    return response.choices[0].message.content\n+\n+def review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n+    \"\"\"\n+    指定された観点でコードレビューを実行し、結果をログに記録する。\n+\n+    Args:\n+        review_perspective (str): レビュー観点。\n+        file_name (str): 対象ファイル名。\n+        diff_info (str): 差分情報。\n+        log_file (str): ログを保存するファイル名。\n+\n+    Returns:\n+        str: レビュー結果。\n+    \"\"\"\n+    # ファイル名と差分情報でシステムプロンプトをフォーマット\n+    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n+    messages = [\n+        {\"role\": \"system\", \"content\": current_system_prompt},\n+        {\"role\": \"user\", \"content\": review_perspective}\n+    ]\n+\n+    start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+    # OpenAI APIを呼び出してレビュー結果を取得\n+    response_str = _call_openai_chat_api(\n+        messages=messages,\n+        model=MODEL_NAME,\n+        temperature=TEMPERATURE,\n+    )\n+    end_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+\n+    log_data = {\n+        \"file_name\": file_name,\n+        \"diff_content\": diff_info,\n+        \"review_perspective\": review_perspective,\n+        \"request_tokens\": num_tokens_from_messages(messages),\n+        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n+        \"response\": response_str,\n+        \"response_tokens\": num_tokens_from_messages(response_str),\n+        \"start_time\": start_time,\n+        \"end_time\": end_time,\n+    }\n+\n+    # 既存のログデータをロード（なければ空リストを使う）\n+    if os.path.exists(log_file):\n+        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n+            try:\n+                existing_logs = json.load(f)\n+            except json.JSONDecodeError:\n+                existing_logs = []\n+    else:\n+        existing_logs = []\n+\n+    # 新しいログデータを追加\n+    existing_logs.append(log_data)\n+    \n+    # ログをJSON形式で保存\n+    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n+        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n+    return response_str\n+\n+def get_mr_info(mr_iid) -> list:\n+    \"\"\"\n+    GitLabのマージリクエスト情報を取得する。\n+\n+    Args:\n+        mr_iid (int): マージリクエストのIID。\n+\n+    Returns:\n+        list: マージリクエストの差分情報リスト。\n+    \"\"\"\n+    base = f\"{GITLAB_URL}/api/v4\"\n+    # 1. MR基本情報\n+    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n+    # 2. 差分情報（Diff）\n+    #    MRの差分情報を取得するエンドポイント\n+    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n+\n+    all_diffs = []\n+    current_page = 1\n+\n+    while True:\n+        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n+        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n+        try:\n+            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n+        except requests.exceptions.HTTPError as e:\n+            print(f\"APIリクエストに失敗しました: {e}\")\n+            break \n+        try:\n+            page_data = response.json()\n+        except requests.exceptions.JSONDecodeError:\n+            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n+            break\n+        if not isinstance(page_data, list):\n+            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n+            break\n+        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n+            break\n+\n+        all_diffs.extend(page_data)\n+\n+        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n+        total_pages_header = response.headers.get(\"x-total-pages\")\n+        if total_pages_header and current_page >= int(total_pages_header):\n+            break # 最後のページまで到達\n+        current_page += 1\n+    return all_diffs\n+\n+def main():\n+    \"\"\"\n+    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n+    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n+    \"\"\"\n+    # コマンドライン引数を解析\n+    parser = argparse.ArgumentParser(description=\"GitLab MR情報を取得してコードレビューを実行します。\")\n+    parser.add_argument(\"--mr_iid\", type=int, required=True, help=\"マージリクエストのIIDを指定してください。\")\n+    args = parser.parse_args()\n+\n+    # GitLab MR情報を取得\n+    diffs = get_mr_info(args.mr_iid)\n+    # レビュー観点をロード\n+    review_perspectives = load_review_perspectives()\n+    \n+    # 出力ファイルの末尾に付与する現在時刻を取得してフォーマット\n+    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n+    # レビュー結果出力ファイル名とログファイル名を生成\n+    output_file = f\"review_results_{timestamp}.md\"\n+    log_file = f\"api_call_logs_{timestamp}.json\"\n+    \n+    # 各DIFFに対してレビューを実行\n+    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n+        for diff in diffs:\n+            file_name = diff['new_path']\n+            diff_info = diff['diff']\n+            if not diff_info:\n+                continue\n+            f.write(f\"# レビュー対象：{file_name}\\n\")\n+            for perspective in review_perspectives:\n+                f.write(f\"## レビュー観点：{perspective}\\n\")\n+                review_result = review_code(perspective, file_name, diff_info, log_file)\n+                f.write(f\"{review_result}\\n\")\n+\n+if __name__ == \"__main__\":\n+    main()\n",
        "review_perspective": "リソース解放（ファイル、DB接続など）が確実に行われているか",
        "request_tokens": 3198,
        "api_params": {
            "model": "gpt-4.1-nano",
            "temperature": 0.2
        },
        "response": "### 指摘対象コード:\n```python\n# 既存のログデータをロード（なければ空リストを使う）\nif os.path.exists(log_file):\n    with open(log_file, \"r\", encoding=\"utf-8\") as f:\n        try:\n            existing_logs = json.load(f)\n        except json.JSONDecodeError:\n            existing_logs = []\nelse:\n    existing_logs = []\n\n# 新しいログデータを追加\nexisting_logs.append(log_data)\n\n# ログをJSON形式で保存\nwith open(log_file, \"w\", encoding=\"utf-8\") as f:\n    json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n```\n\n### 指摘内容:\nこのコードでは、ファイルのオープンと書き込みに`with`文を使用しており、ファイルリソースは確実に閉じられるため、リソース解放の観点では問題ありません。ただし、例外処理の範囲が限定的であり、`json.load()`や`json.dump()`の途中で例外が発生した場合に備えたエラーハンドリングや、ファイル操作の失敗に対する対策が不足しています。\n\n### 修正提案:\n特に修正は必要ありませんが、より堅牢にするために例外処理を拡充し、ファイル操作に失敗した場合の対応を追加すると良いでしょう。例えば、`try-except`ブロックを用いて例外発生時に適切なログ出力や再試行を行うことが考えられます。\n\n```python\ntry:\n    # 既存のログデータをロード（なければ空リストを使う）\n    if os.path.exists(log_file):\n        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n            try:\n                existing_logs = json.load(f)\n            except json.JSONDecodeError:\n                existing_logs = []\n    else:\n        existing_logs = []\n\n    # 新しいログデータを追加\n    existing_logs.append(log_data)\n\n    # ログをJSON形式で保存\n    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\nexcept IOError as e:\n    print(f\"ファイル操作中にエラーが発生しました: {e}\")\n```\n\nしかし、現状の`with`文の使用はリソース解放の観点では適切です。",
        "response_tokens": 665,
        "start_time": "2025/05/26 04:49:07.844853",
        "end_time": "2025/05/26 04:49:12.578618"
    },
    {
        "file_name": "code_review.py",
        "diff_content": "@@ -0,0 +1,229 @@\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n+\n+PRIVATE_TOKEN = os.getenv(\"GITLAB_TOKEN\")\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+HEADERS = {\n+    \"PRIVATE-TOKEN\": PRIVATE_TOKEN\n+}\n+\n+CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n+TIKTOKEN_ENCODING = tiktoken.encoding_for_model(TIKTOKEN_MODEL)\n+\n+def load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n+    \"\"\"\n+    レビュー観点をJSONファイルから読み込む。\n+\n+    Args:\n+        file_path (str): レビュー観点が記載されたJSONファイルのパス。\n+\n+    Returns:\n+        list: レビュー観点のリスト。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+def load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n+    \"\"\"\n+    システムプロンプトをテキストファイルから読み込む。\n+\n+    Args:\n+        file_path (str): システムプロンプトが記載されたテキストファイルのパス。\n+\n+    Returns:\n+        str: システムプロンプトの内容。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return f.read()\n+\n+def num_tokens_from_messages(messages) -> int:\n+    \"\"\"\n+    メッセージからトークン数を計算する。\n+\n+    Args:\n+        messages (list): メッセージのリスト。\n+\n+    Returns:\n+        int: メッセージに含まれるトークン数。\n+    \"\"\"\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += TOKENS_PER_MESSAGE\n+        for key, value in message.items():\n+            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n+            if key == \"name\":\n+                num_tokens += TOKENS_PER_NAME\n+    num_tokens += 3  # assistantの返信の開始トークン\n+    return num_tokens\n+\n+def _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n+    \"\"\"\n+    OpenAIのChat APIを呼び出す関数。\n+\n+    Args:\n+        messages (list): APIに送信するメッセージリスト。\n+        model (str): 使用するモデル名。\n+        temperature (float): 出力の多様性を制御する温度パラメータ。\n+        max_tokens (int): 最大トークン数。\n+\n+    Returns:\n+        str: OpenAI APIからのレスポンス。\n+    \"\"\"\n+    response = CLIENT.chat.completions.create(\n+        model=model,\n+        messages=messages,\n+        temperature=temperature,\n+        max_tokens=max_tokens\n+    )\n+    return response.choices[0].message.content\n+\n+def review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n+    \"\"\"\n+    指定された観点でコードレビューを実行し、結果をログに記録する。\n+\n+    Args:\n+        review_perspective (str): レビュー観点。\n+        file_name (str): 対象ファイル名。\n+        diff_info (str): 差分情報。\n+        log_file (str): ログを保存するファイル名。\n+\n+    Returns:\n+        str: レビュー結果。\n+    \"\"\"\n+    # ファイル名と差分情報でシステムプロンプトをフォーマット\n+    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n+    messages = [\n+        {\"role\": \"system\", \"content\": current_system_prompt},\n+        {\"role\": \"user\", \"content\": review_perspective}\n+    ]\n+\n+    start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+    # OpenAI APIを呼び出してレビュー結果を取得\n+    response_str = _call_openai_chat_api(\n+        messages=messages,\n+        model=MODEL_NAME,\n+        temperature=TEMPERATURE,\n+    )\n+    end_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+\n+    log_data = {\n+        \"file_name\": file_name,\n+        \"diff_content\": diff_info,\n+        \"review_perspective\": review_perspective,\n+        \"request_tokens\": num_tokens_from_messages(messages),\n+        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n+        \"response\": response_str,\n+        \"response_tokens\": num_tokens_from_messages(response_str),\n+        \"start_time\": start_time,\n+        \"end_time\": end_time,\n+    }\n+\n+    # 既存のログデータをロード（なければ空リストを使う）\n+    if os.path.exists(log_file):\n+        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n+            try:\n+                existing_logs = json.load(f)\n+            except json.JSONDecodeError:\n+                existing_logs = []\n+    else:\n+        existing_logs = []\n+\n+    # 新しいログデータを追加\n+    existing_logs.append(log_data)\n+    \n+    # ログをJSON形式で保存\n+    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n+        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n+    return response_str\n+\n+def get_mr_info(mr_iid) -> list:\n+    \"\"\"\n+    GitLabのマージリクエスト情報を取得する。\n+\n+    Args:\n+        mr_iid (int): マージリクエストのIID。\n+\n+    Returns:\n+        list: マージリクエストの差分情報リスト。\n+    \"\"\"\n+    base = f\"{GITLAB_URL}/api/v4\"\n+    # 1. MR基本情報\n+    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n+    # 2. 差分情報（Diff）\n+    #    MRの差分情報を取得するエンドポイント\n+    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n+\n+    all_diffs = []\n+    current_page = 1\n+\n+    while True:\n+        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n+        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n+        try:\n+            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n+        except requests.exceptions.HTTPError as e:\n+            print(f\"APIリクエストに失敗しました: {e}\")\n+            break \n+        try:\n+            page_data = response.json()\n+        except requests.exceptions.JSONDecodeError:\n+            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n+            break\n+        if not isinstance(page_data, list):\n+            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n+            break\n+        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n+            break\n+\n+        all_diffs.extend(page_data)\n+\n+        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n+        total_pages_header = response.headers.get(\"x-total-pages\")\n+        if total_pages_header and current_page >= int(total_pages_header):\n+            break # 最後のページまで到達\n+        current_page += 1\n+    return all_diffs\n+\n+def main():\n+    \"\"\"\n+    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n+    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n+    \"\"\"\n+    # コマンドライン引数を解析\n+    parser = argparse.ArgumentParser(description=\"GitLab MR情報を取得してコードレビューを実行します。\")\n+    parser.add_argument(\"--mr_iid\", type=int, required=True, help=\"マージリクエストのIIDを指定してください。\")\n+    args = parser.parse_args()\n+\n+    # GitLab MR情報を取得\n+    diffs = get_mr_info(args.mr_iid)\n+    # レビュー観点をロード\n+    review_perspectives = load_review_perspectives()\n+    \n+    # 出力ファイルの末尾に付与する現在時刻を取得してフォーマット\n+    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n+    # レビュー結果出力ファイル名とログファイル名を生成\n+    output_file = f\"review_results_{timestamp}.md\"\n+    log_file = f\"api_call_logs_{timestamp}.json\"\n+    \n+    # 各DIFFに対してレビューを実行\n+    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n+        for diff in diffs:\n+            file_name = diff['new_path']\n+            diff_info = diff['diff']\n+            if not diff_info:\n+                continue\n+            f.write(f\"# レビュー対象：{file_name}\\n\")\n+            for perspective in review_perspectives:\n+                f.write(f\"## レビュー観点：{perspective}\\n\")\n+                review_result = review_code(perspective, file_name, diff_info, log_file)\n+                f.write(f\"{review_result}\\n\")\n+\n+if __name__ == \"__main__\":\n+    main()\n",
        "review_perspective": "スレッドセーフ・非同期処理が適切か",
        "request_tokens": 3186,
        "api_params": {
            "model": "gpt-4.1-nano",
            "temperature": 0.2
        },
        "response": "### 指摘対象コード:\n```python\ndef review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n    # ...省略...\n    # ログデータをロード（なければ空リストを使う）\n    if os.path.exists(log_file):\n        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n            try:\n                existing_logs = json.load(f)\n            except json.JSONDecodeError:\n                existing_logs = []\n    else:\n        existing_logs = []\n\n    # 新しいログデータを追加\n    existing_logs.append(log_data)\n    \n    # ログをJSON形式で保存\n    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n    return response_str\n```\n\n### 指摘内容:\nこのコードは、`review_code`関数内で`log_file`に対して読み込みと書き込みを行っていますが、これを複数のスレッドや非同期処理から同時に呼び出した場合、ファイルアクセスの競合やデータ破損のリスクがあります。特に、複数の処理が同時に`json.load`や`json.dump`を実行すると、ファイルの内容が不整合になる可能性があります。\n\n### 修正提案:\n- ファイルアクセスに排他制御を導入し、スレッドセーフ・非同期処理に対応できるようにする。\n- 例えば、`threading.Lock`や`asyncio.Lock`を用いて排他制御を行う。\n- もしくは、ログの書き込みをキューに入れて逐次処理する仕組みを検討する。\n\n例（同期処理の場合）：\n```python\nimport threading\n\nlog_lock = threading.Lock()\n\ndef review_code(...):\n    # ...省略...\n    with log_lock:\n        if os.path.exists(log_file):\n            with open(log_file, \"r\", encoding=\"utf-8\") as f:\n                try:\n                    existing_logs = json.load(f)\n                except json.JSONDecodeError:\n                    existing_logs = []\n        else:\n            existing_logs = []\n\n        existing_logs.append(log_data)\n\n        with open(log_file, \"w\", encoding=\"utf-8\") as f:\n            json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n    # ...省略...\n```\n\n非同期処理の場合は`asyncio.Lock`を使用してください。\n\nこの修正により、複数のスレッドや非同期タスクからの同時アクセスによる問題を防止できます。",
        "response_tokens": 705,
        "start_time": "2025/05/26 04:49:12.586411",
        "end_time": "2025/05/26 04:49:17.863147"
    },
    {
        "file_name": "code_review.py",
        "diff_content": "@@ -0,0 +1,229 @@\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n+\n+PRIVATE_TOKEN = os.getenv(\"GITLAB_TOKEN\")\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+HEADERS = {\n+    \"PRIVATE-TOKEN\": PRIVATE_TOKEN\n+}\n+\n+CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n+TIKTOKEN_ENCODING = tiktoken.encoding_for_model(TIKTOKEN_MODEL)\n+\n+def load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n+    \"\"\"\n+    レビュー観点をJSONファイルから読み込む。\n+\n+    Args:\n+        file_path (str): レビュー観点が記載されたJSONファイルのパス。\n+\n+    Returns:\n+        list: レビュー観点のリスト。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+def load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n+    \"\"\"\n+    システムプロンプトをテキストファイルから読み込む。\n+\n+    Args:\n+        file_path (str): システムプロンプトが記載されたテキストファイルのパス。\n+\n+    Returns:\n+        str: システムプロンプトの内容。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return f.read()\n+\n+def num_tokens_from_messages(messages) -> int:\n+    \"\"\"\n+    メッセージからトークン数を計算する。\n+\n+    Args:\n+        messages (list): メッセージのリスト。\n+\n+    Returns:\n+        int: メッセージに含まれるトークン数。\n+    \"\"\"\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += TOKENS_PER_MESSAGE\n+        for key, value in message.items():\n+            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n+            if key == \"name\":\n+                num_tokens += TOKENS_PER_NAME\n+    num_tokens += 3  # assistantの返信の開始トークン\n+    return num_tokens\n+\n+def _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n+    \"\"\"\n+    OpenAIのChat APIを呼び出す関数。\n+\n+    Args:\n+        messages (list): APIに送信するメッセージリスト。\n+        model (str): 使用するモデル名。\n+        temperature (float): 出力の多様性を制御する温度パラメータ。\n+        max_tokens (int): 最大トークン数。\n+\n+    Returns:\n+        str: OpenAI APIからのレスポンス。\n+    \"\"\"\n+    response = CLIENT.chat.completions.create(\n+        model=model,\n+        messages=messages,\n+        temperature=temperature,\n+        max_tokens=max_tokens\n+    )\n+    return response.choices[0].message.content\n+\n+def review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n+    \"\"\"\n+    指定された観点でコードレビューを実行し、結果をログに記録する。\n+\n+    Args:\n+        review_perspective (str): レビュー観点。\n+        file_name (str): 対象ファイル名。\n+        diff_info (str): 差分情報。\n+        log_file (str): ログを保存するファイル名。\n+\n+    Returns:\n+        str: レビュー結果。\n+    \"\"\"\n+    # ファイル名と差分情報でシステムプロンプトをフォーマット\n+    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n+    messages = [\n+        {\"role\": \"system\", \"content\": current_system_prompt},\n+        {\"role\": \"user\", \"content\": review_perspective}\n+    ]\n+\n+    start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+    # OpenAI APIを呼び出してレビュー結果を取得\n+    response_str = _call_openai_chat_api(\n+        messages=messages,\n+        model=MODEL_NAME,\n+        temperature=TEMPERATURE,\n+    )\n+    end_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+\n+    log_data = {\n+        \"file_name\": file_name,\n+        \"diff_content\": diff_info,\n+        \"review_perspective\": review_perspective,\n+        \"request_tokens\": num_tokens_from_messages(messages),\n+        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n+        \"response\": response_str,\n+        \"response_tokens\": num_tokens_from_messages(response_str),\n+        \"start_time\": start_time,\n+        \"end_time\": end_time,\n+    }\n+\n+    # 既存のログデータをロード（なければ空リストを使う）\n+    if os.path.exists(log_file):\n+        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n+            try:\n+                existing_logs = json.load(f)\n+            except json.JSONDecodeError:\n+                existing_logs = []\n+    else:\n+        existing_logs = []\n+\n+    # 新しいログデータを追加\n+    existing_logs.append(log_data)\n+    \n+    # ログをJSON形式で保存\n+    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n+        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n+    return response_str\n+\n+def get_mr_info(mr_iid) -> list:\n+    \"\"\"\n+    GitLabのマージリクエスト情報を取得する。\n+\n+    Args:\n+        mr_iid (int): マージリクエストのIID。\n+\n+    Returns:\n+        list: マージリクエストの差分情報リスト。\n+    \"\"\"\n+    base = f\"{GITLAB_URL}/api/v4\"\n+    # 1. MR基本情報\n+    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n+    # 2. 差分情報（Diff）\n+    #    MRの差分情報を取得するエンドポイント\n+    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n+\n+    all_diffs = []\n+    current_page = 1\n+\n+    while True:\n+        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n+        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n+        try:\n+            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n+        except requests.exceptions.HTTPError as e:\n+            print(f\"APIリクエストに失敗しました: {e}\")\n+            break \n+        try:\n+            page_data = response.json()\n+        except requests.exceptions.JSONDecodeError:\n+            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n+            break\n+        if not isinstance(page_data, list):\n+            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n+            break\n+        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n+            break\n+\n+        all_diffs.extend(page_data)\n+\n+        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n+        total_pages_header = response.headers.get(\"x-total-pages\")\n+        if total_pages_header and current_page >= int(total_pages_header):\n+            break # 最後のページまで到達\n+        current_page += 1\n+    return all_diffs\n+\n+def main():\n+    \"\"\"\n+    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n+    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n+    \"\"\"\n+    # コマンドライン引数を解析\n+    parser = argparse.ArgumentParser(description=\"GitLab MR情報を取得してコードレビューを実行します。\")\n+    parser.add_argument(\"--mr_iid\", type=int, required=True, help=\"マージリクエストのIIDを指定してください。\")\n+    args = parser.parse_args()\n+\n+    # GitLab MR情報を取得\n+    diffs = get_mr_info(args.mr_iid)\n+    # レビュー観点をロード\n+    review_perspectives = load_review_perspectives()\n+    \n+    # 出力ファイルの末尾に付与する現在時刻を取得してフォーマット\n+    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n+    # レビュー結果出力ファイル名とログファイル名を生成\n+    output_file = f\"review_results_{timestamp}.md\"\n+    log_file = f\"api_call_logs_{timestamp}.json\"\n+    \n+    # 各DIFFに対してレビューを実行\n+    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n+        for diff in diffs:\n+            file_name = diff['new_path']\n+            diff_info = diff['diff']\n+            if not diff_info:\n+                continue\n+            f.write(f\"# レビュー対象：{file_name}\\n\")\n+            for perspective in review_perspectives:\n+                f.write(f\"## レビュー観点：{perspective}\\n\")\n+                review_result = review_code(perspective, file_name, diff_info, log_file)\n+                f.write(f\"{review_result}\\n\")\n+\n+if __name__ == \"__main__\":\n+    main()\n",
        "review_perspective": "不要な計算やループがないか",
        "request_tokens": 3179,
        "api_params": {
            "model": "gpt-4.1-nano",
            "temperature": 0.2
        },
        "response": "### 指摘対象コード:\n```python\ndef num_tokens_from_messages(messages) -> int:\n    num_tokens = 0\n    for message in messages:\n        num_tokens += TOKENS_PER_MESSAGE\n        for key, value in message.items():\n            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n            if key == \"name\":\n                num_tokens += TOKENS_PER_NAME\n    num_tokens += 3  # assistantの返信の開始トークン\n    return num_tokens\n```\n\n### 指摘内容:\n`num_tokens_from_messages`関数は、メッセージリストの各メッセージに対して必要なトークン数を計算していますが、`messages`が文字列（例：`response_str`）の場合に対応していません。この関数は、リスト内の各要素が辞書形式であることを前提としていますが、`review_code`関数内で`num_tokens_from_messages`に`response_str`を渡している箇所があります。\n\nまた、`response_str`は文字列なので、`num_tokens_from_messages`の処理に適さず、不要なループや計算が行われてしまいます。これにより、無駄な計算や誤ったトークン数の計算が発生する可能性があります。\n\n### 修正提案:\n- `num_tokens_from_messages`関数は、引数がリスト（メッセージのリスト）であることを前提としているため、文字列が渡された場合は別途トークン数を計算する処理に分岐させるか、呼び出し側で文字列の場合は直接文字列の長さに基づいてトークン数を推定する処理を追加してください。\n- もしくは、`review_code`関数内で`response_str`のトークン数を計算する際に、`len(TIKTOKEN_ENCODING.encode(response_str))`のように直接計算するのが良いです。\n\n例：\n```python\n# response_strのトークン数を計算\nresponse_tokens = len(TIKTOKEN_ENCODING.encode(response_str))\n```\n\nこれにより、不要なループや誤った計算を避けることができます。",
        "response_tokens": 591,
        "start_time": "2025/05/26 04:49:17.874876",
        "end_time": "2025/05/26 04:49:22.692557"
    },
    {
        "file_name": "code_review.py",
        "diff_content": "@@ -0,0 +1,229 @@\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n+\n+PRIVATE_TOKEN = os.getenv(\"GITLAB_TOKEN\")\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+HEADERS = {\n+    \"PRIVATE-TOKEN\": PRIVATE_TOKEN\n+}\n+\n+CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n+TIKTOKEN_ENCODING = tiktoken.encoding_for_model(TIKTOKEN_MODEL)\n+\n+def load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n+    \"\"\"\n+    レビュー観点をJSONファイルから読み込む。\n+\n+    Args:\n+        file_path (str): レビュー観点が記載されたJSONファイルのパス。\n+\n+    Returns:\n+        list: レビュー観点のリスト。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+def load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n+    \"\"\"\n+    システムプロンプトをテキストファイルから読み込む。\n+\n+    Args:\n+        file_path (str): システムプロンプトが記載されたテキストファイルのパス。\n+\n+    Returns:\n+        str: システムプロンプトの内容。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return f.read()\n+\n+def num_tokens_from_messages(messages) -> int:\n+    \"\"\"\n+    メッセージからトークン数を計算する。\n+\n+    Args:\n+        messages (list): メッセージのリスト。\n+\n+    Returns:\n+        int: メッセージに含まれるトークン数。\n+    \"\"\"\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += TOKENS_PER_MESSAGE\n+        for key, value in message.items():\n+            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n+            if key == \"name\":\n+                num_tokens += TOKENS_PER_NAME\n+    num_tokens += 3  # assistantの返信の開始トークン\n+    return num_tokens\n+\n+def _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n+    \"\"\"\n+    OpenAIのChat APIを呼び出す関数。\n+\n+    Args:\n+        messages (list): APIに送信するメッセージリスト。\n+        model (str): 使用するモデル名。\n+        temperature (float): 出力の多様性を制御する温度パラメータ。\n+        max_tokens (int): 最大トークン数。\n+\n+    Returns:\n+        str: OpenAI APIからのレスポンス。\n+    \"\"\"\n+    response = CLIENT.chat.completions.create(\n+        model=model,\n+        messages=messages,\n+        temperature=temperature,\n+        max_tokens=max_tokens\n+    )\n+    return response.choices[0].message.content\n+\n+def review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n+    \"\"\"\n+    指定された観点でコードレビューを実行し、結果をログに記録する。\n+\n+    Args:\n+        review_perspective (str): レビュー観点。\n+        file_name (str): 対象ファイル名。\n+        diff_info (str): 差分情報。\n+        log_file (str): ログを保存するファイル名。\n+\n+    Returns:\n+        str: レビュー結果。\n+    \"\"\"\n+    # ファイル名と差分情報でシステムプロンプトをフォーマット\n+    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n+    messages = [\n+        {\"role\": \"system\", \"content\": current_system_prompt},\n+        {\"role\": \"user\", \"content\": review_perspective}\n+    ]\n+\n+    start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+    # OpenAI APIを呼び出してレビュー結果を取得\n+    response_str = _call_openai_chat_api(\n+        messages=messages,\n+        model=MODEL_NAME,\n+        temperature=TEMPERATURE,\n+    )\n+    end_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+\n+    log_data = {\n+        \"file_name\": file_name,\n+        \"diff_content\": diff_info,\n+        \"review_perspective\": review_perspective,\n+        \"request_tokens\": num_tokens_from_messages(messages),\n+        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n+        \"response\": response_str,\n+        \"response_tokens\": num_tokens_from_messages(response_str),\n+        \"start_time\": start_time,\n+        \"end_time\": end_time,\n+    }\n+\n+    # 既存のログデータをロード（なければ空リストを使う）\n+    if os.path.exists(log_file):\n+        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n+            try:\n+                existing_logs = json.load(f)\n+            except json.JSONDecodeError:\n+                existing_logs = []\n+    else:\n+        existing_logs = []\n+\n+    # 新しいログデータを追加\n+    existing_logs.append(log_data)\n+    \n+    # ログをJSON形式で保存\n+    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n+        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n+    return response_str\n+\n+def get_mr_info(mr_iid) -> list:\n+    \"\"\"\n+    GitLabのマージリクエスト情報を取得する。\n+\n+    Args:\n+        mr_iid (int): マージリクエストのIID。\n+\n+    Returns:\n+        list: マージリクエストの差分情報リスト。\n+    \"\"\"\n+    base = f\"{GITLAB_URL}/api/v4\"\n+    # 1. MR基本情報\n+    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n+    # 2. 差分情報（Diff）\n+    #    MRの差分情報を取得するエンドポイント\n+    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n+\n+    all_diffs = []\n+    current_page = 1\n+\n+    while True:\n+        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n+        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n+        try:\n+            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n+        except requests.exceptions.HTTPError as e:\n+            print(f\"APIリクエストに失敗しました: {e}\")\n+            break \n+        try:\n+            page_data = response.json()\n+        except requests.exceptions.JSONDecodeError:\n+            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n+            break\n+        if not isinstance(page_data, list):\n+            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n+            break\n+        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n+            break\n+\n+        all_diffs.extend(page_data)\n+\n+        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n+        total_pages_header = response.headers.get(\"x-total-pages\")\n+        if total_pages_header and current_page >= int(total_pages_header):\n+            break # 最後のページまで到達\n+        current_page += 1\n+    return all_diffs\n+\n+def main():\n+    \"\"\"\n+    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n+    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n+    \"\"\"\n+    # コマンドライン引数を解析\n+    parser = argparse.ArgumentParser(description=\"GitLab MR情報を取得してコードレビューを実行します。\")\n+    parser.add_argument(\"--mr_iid\", type=int, required=True, help=\"マージリクエストのIIDを指定してください。\")\n+    args = parser.parse_args()\n+\n+    # GitLab MR情報を取得\n+    diffs = get_mr_info(args.mr_iid)\n+    # レビュー観点をロード\n+    review_perspectives = load_review_perspectives()\n+    \n+    # 出力ファイルの末尾に付与する現在時刻を取得してフォーマット\n+    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n+    # レビュー結果出力ファイル名とログファイル名を生成\n+    output_file = f\"review_results_{timestamp}.md\"\n+    log_file = f\"api_call_logs_{timestamp}.json\"\n+    \n+    # 各DIFFに対してレビューを実行\n+    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n+        for diff in diffs:\n+            file_name = diff['new_path']\n+            diff_info = diff['diff']\n+            if not diff_info:\n+                continue\n+            f.write(f\"# レビュー対象：{file_name}\\n\")\n+            for perspective in review_perspectives:\n+                f.write(f\"## レビュー観点：{perspective}\\n\")\n+                review_result = review_code(perspective, file_name, diff_info, log_file)\n+                f.write(f\"{review_result}\\n\")\n+\n+if __name__ == \"__main__\":\n+    main()\n",
        "review_perspective": "データ構造の選定が適切か（list vs set vs dictなど）",
        "request_tokens": 3192,
        "api_params": {
            "model": "gpt-4.1-nano",
            "temperature": 0.2
        },
        "response": "### 指摘対象コード:\n```python\ndef get_mr_info(mr_iid) -> list:\n    ...\n    all_diffs.extend(page_data)\n    ...\n    return all_diffs\n```\n\n### 指摘内容:\n`get_mr_info`関数は、GitLabの差分情報を複数ページから取得し、`all_diffs`リストに格納して返しています。この差分情報は、各差分の詳細を含む辞書のリストとして扱われており、特に重複を排除する必要がなければリストのままで問題ありません。\n\nしかし、差分情報の中で`diff`や`new_path`といった特定のキーを基準に重複排除や高速な検索を行いたい場合は、`set`や`dict`を用いる方が適切です。特に、`file_name`をキーにした重複排除や検索を行う場合、`dict`や`set`の方が効率的です。\n\n現状のコードでは、差分情報をそのままリストに格納しているため、重複や検索効率の観点から最適とは言えません。もし差分情報に重複が存在し、かつそれを排除したい場合は、`dict`や`set`を用いることを検討すべきです。\n\n### 修正提案:\n差分情報の重複排除や特定のキーに基づく高速アクセスを行いたい場合は、`dict`を用いて以下のように実装できます。\n\n```python\ndef get_mr_info(mr_iid) -> dict:\n    ...\n    all_diffs_dict = {}\n    for page_data in response.json():\n        for diff in page_data:\n            # 例：ファイル名をキーにして重複排除\n            all_diffs_dict[diff['new_path']] = diff\n    return all_diffs_dict\n```\n\nただし、差分情報をそのままリストとして扱うことに問題がなければ、現状のままで十分です。  \n必要に応じて、差分情報の重複や検索効率の改善を検討してください。",
        "response_tokens": 632,
        "start_time": "2025/05/26 04:49:22.702555",
        "end_time": "2025/05/26 04:49:27.698768"
    },
    {
        "file_name": "code_review.py",
        "diff_content": "@@ -0,0 +1,229 @@\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n+\n+PRIVATE_TOKEN = os.getenv(\"GITLAB_TOKEN\")\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+HEADERS = {\n+    \"PRIVATE-TOKEN\": PRIVATE_TOKEN\n+}\n+\n+CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n+TIKTOKEN_ENCODING = tiktoken.encoding_for_model(TIKTOKEN_MODEL)\n+\n+def load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n+    \"\"\"\n+    レビュー観点をJSONファイルから読み込む。\n+\n+    Args:\n+        file_path (str): レビュー観点が記載されたJSONファイルのパス。\n+\n+    Returns:\n+        list: レビュー観点のリスト。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+def load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n+    \"\"\"\n+    システムプロンプトをテキストファイルから読み込む。\n+\n+    Args:\n+        file_path (str): システムプロンプトが記載されたテキストファイルのパス。\n+\n+    Returns:\n+        str: システムプロンプトの内容。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return f.read()\n+\n+def num_tokens_from_messages(messages) -> int:\n+    \"\"\"\n+    メッセージからトークン数を計算する。\n+\n+    Args:\n+        messages (list): メッセージのリスト。\n+\n+    Returns:\n+        int: メッセージに含まれるトークン数。\n+    \"\"\"\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += TOKENS_PER_MESSAGE\n+        for key, value in message.items():\n+            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n+            if key == \"name\":\n+                num_tokens += TOKENS_PER_NAME\n+    num_tokens += 3  # assistantの返信の開始トークン\n+    return num_tokens\n+\n+def _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n+    \"\"\"\n+    OpenAIのChat APIを呼び出す関数。\n+\n+    Args:\n+        messages (list): APIに送信するメッセージリスト。\n+        model (str): 使用するモデル名。\n+        temperature (float): 出力の多様性を制御する温度パラメータ。\n+        max_tokens (int): 最大トークン数。\n+\n+    Returns:\n+        str: OpenAI APIからのレスポンス。\n+    \"\"\"\n+    response = CLIENT.chat.completions.create(\n+        model=model,\n+        messages=messages,\n+        temperature=temperature,\n+        max_tokens=max_tokens\n+    )\n+    return response.choices[0].message.content\n+\n+def review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n+    \"\"\"\n+    指定された観点でコードレビューを実行し、結果をログに記録する。\n+\n+    Args:\n+        review_perspective (str): レビュー観点。\n+        file_name (str): 対象ファイル名。\n+        diff_info (str): 差分情報。\n+        log_file (str): ログを保存するファイル名。\n+\n+    Returns:\n+        str: レビュー結果。\n+    \"\"\"\n+    # ファイル名と差分情報でシステムプロンプトをフォーマット\n+    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n+    messages = [\n+        {\"role\": \"system\", \"content\": current_system_prompt},\n+        {\"role\": \"user\", \"content\": review_perspective}\n+    ]\n+\n+    start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+    # OpenAI APIを呼び出してレビュー結果を取得\n+    response_str = _call_openai_chat_api(\n+        messages=messages,\n+        model=MODEL_NAME,\n+        temperature=TEMPERATURE,\n+    )\n+    end_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+\n+    log_data = {\n+        \"file_name\": file_name,\n+        \"diff_content\": diff_info,\n+        \"review_perspective\": review_perspective,\n+        \"request_tokens\": num_tokens_from_messages(messages),\n+        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n+        \"response\": response_str,\n+        \"response_tokens\": num_tokens_from_messages(response_str),\n+        \"start_time\": start_time,\n+        \"end_time\": end_time,\n+    }\n+\n+    # 既存のログデータをロード（なければ空リストを使う）\n+    if os.path.exists(log_file):\n+        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n+            try:\n+                existing_logs = json.load(f)\n+            except json.JSONDecodeError:\n+                existing_logs = []\n+    else:\n+        existing_logs = []\n+\n+    # 新しいログデータを追加\n+    existing_logs.append(log_data)\n+    \n+    # ログをJSON形式で保存\n+    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n+        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n+    return response_str\n+\n+def get_mr_info(mr_iid) -> list:\n+    \"\"\"\n+    GitLabのマージリクエスト情報を取得する。\n+\n+    Args:\n+        mr_iid (int): マージリクエストのIID。\n+\n+    Returns:\n+        list: マージリクエストの差分情報リスト。\n+    \"\"\"\n+    base = f\"{GITLAB_URL}/api/v4\"\n+    # 1. MR基本情報\n+    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n+    # 2. 差分情報（Diff）\n+    #    MRの差分情報を取得するエンドポイント\n+    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n+\n+    all_diffs = []\n+    current_page = 1\n+\n+    while True:\n+        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n+        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n+        try:\n+            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n+        except requests.exceptions.HTTPError as e:\n+            print(f\"APIリクエストに失敗しました: {e}\")\n+            break \n+        try:\n+            page_data = response.json()\n+        except requests.exceptions.JSONDecodeError:\n+            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n+            break\n+        if not isinstance(page_data, list):\n+            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n+            break\n+        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n+            break\n+\n+        all_diffs.extend(page_data)\n+\n+        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n+        total_pages_header = response.headers.get(\"x-total-pages\")\n+        if total_pages_header and current_page >= int(total_pages_header):\n+            break # 最後のページまで到達\n+        current_page += 1\n+    return all_diffs\n+\n+def main():\n+    \"\"\"\n+    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n+    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n+    \"\"\"\n+    # コマンドライン引数を解析\n+    parser = argparse.ArgumentParser(description=\"GitLab MR情報を取得してコードレビューを実行します。\")\n+    parser.add_argument(\"--mr_iid\", type=int, required=True, help=\"マージリクエストのIIDを指定してください。\")\n+    args = parser.parse_args()\n+\n+    # GitLab MR情報を取得\n+    diffs = get_mr_info(args.mr_iid)\n+    # レビュー観点をロード\n+    review_perspectives = load_review_perspectives()\n+    \n+    # 出力ファイルの末尾に付与する現在時刻を取得してフォーマット\n+    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n+    # レビュー結果出力ファイル名とログファイル名を生成\n+    output_file = f\"review_results_{timestamp}.md\"\n+    log_file = f\"api_call_logs_{timestamp}.json\"\n+    \n+    # 各DIFFに対してレビューを実行\n+    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n+        for diff in diffs:\n+            file_name = diff['new_path']\n+            diff_info = diff['diff']\n+            if not diff_info:\n+                continue\n+            f.write(f\"# レビュー対象：{file_name}\\n\")\n+            for perspective in review_perspectives:\n+                f.write(f\"## レビュー観点：{perspective}\\n\")\n+                review_result = review_code(perspective, file_name, diff_info, log_file)\n+                f.write(f\"{review_result}\\n\")\n+\n+if __name__ == \"__main__\":\n+    main()\n",
        "review_perspective": "大きなデータ処理に効率的なアルゴリズムを使っているか",
        "request_tokens": 3194,
        "api_params": {
            "model": "gpt-4.1-nano",
            "temperature": 0.2
        },
        "response": "この観点では問題ありません。",
        "response_tokens": 13,
        "start_time": "2025/05/26 04:49:27.709979",
        "end_time": "2025/05/26 04:49:28.232869"
    },
    {
        "file_name": "code_review.py",
        "diff_content": "@@ -0,0 +1,229 @@\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n+\n+PRIVATE_TOKEN = os.getenv(\"GITLAB_TOKEN\")\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+HEADERS = {\n+    \"PRIVATE-TOKEN\": PRIVATE_TOKEN\n+}\n+\n+CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n+TIKTOKEN_ENCODING = tiktoken.encoding_for_model(TIKTOKEN_MODEL)\n+\n+def load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n+    \"\"\"\n+    レビュー観点をJSONファイルから読み込む。\n+\n+    Args:\n+        file_path (str): レビュー観点が記載されたJSONファイルのパス。\n+\n+    Returns:\n+        list: レビュー観点のリスト。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+def load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n+    \"\"\"\n+    システムプロンプトをテキストファイルから読み込む。\n+\n+    Args:\n+        file_path (str): システムプロンプトが記載されたテキストファイルのパス。\n+\n+    Returns:\n+        str: システムプロンプトの内容。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return f.read()\n+\n+def num_tokens_from_messages(messages) -> int:\n+    \"\"\"\n+    メッセージからトークン数を計算する。\n+\n+    Args:\n+        messages (list): メッセージのリスト。\n+\n+    Returns:\n+        int: メッセージに含まれるトークン数。\n+    \"\"\"\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += TOKENS_PER_MESSAGE\n+        for key, value in message.items():\n+            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n+            if key == \"name\":\n+                num_tokens += TOKENS_PER_NAME\n+    num_tokens += 3  # assistantの返信の開始トークン\n+    return num_tokens\n+\n+def _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n+    \"\"\"\n+    OpenAIのChat APIを呼び出す関数。\n+\n+    Args:\n+        messages (list): APIに送信するメッセージリスト。\n+        model (str): 使用するモデル名。\n+        temperature (float): 出力の多様性を制御する温度パラメータ。\n+        max_tokens (int): 最大トークン数。\n+\n+    Returns:\n+        str: OpenAI APIからのレスポンス。\n+    \"\"\"\n+    response = CLIENT.chat.completions.create(\n+        model=model,\n+        messages=messages,\n+        temperature=temperature,\n+        max_tokens=max_tokens\n+    )\n+    return response.choices[0].message.content\n+\n+def review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n+    \"\"\"\n+    指定された観点でコードレビューを実行し、結果をログに記録する。\n+\n+    Args:\n+        review_perspective (str): レビュー観点。\n+        file_name (str): 対象ファイル名。\n+        diff_info (str): 差分情報。\n+        log_file (str): ログを保存するファイル名。\n+\n+    Returns:\n+        str: レビュー結果。\n+    \"\"\"\n+    # ファイル名と差分情報でシステムプロンプトをフォーマット\n+    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n+    messages = [\n+        {\"role\": \"system\", \"content\": current_system_prompt},\n+        {\"role\": \"user\", \"content\": review_perspective}\n+    ]\n+\n+    start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+    # OpenAI APIを呼び出してレビュー結果を取得\n+    response_str = _call_openai_chat_api(\n+        messages=messages,\n+        model=MODEL_NAME,\n+        temperature=TEMPERATURE,\n+    )\n+    end_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+\n+    log_data = {\n+        \"file_name\": file_name,\n+        \"diff_content\": diff_info,\n+        \"review_perspective\": review_perspective,\n+        \"request_tokens\": num_tokens_from_messages(messages),\n+        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n+        \"response\": response_str,\n+        \"response_tokens\": num_tokens_from_messages(response_str),\n+        \"start_time\": start_time,\n+        \"end_time\": end_time,\n+    }\n+\n+    # 既存のログデータをロード（なければ空リストを使う）\n+    if os.path.exists(log_file):\n+        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n+            try:\n+                existing_logs = json.load(f)\n+            except json.JSONDecodeError:\n+                existing_logs = []\n+    else:\n+        existing_logs = []\n+\n+    # 新しいログデータを追加\n+    existing_logs.append(log_data)\n+    \n+    # ログをJSON形式で保存\n+    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n+        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n+    return response_str\n+\n+def get_mr_info(mr_iid) -> list:\n+    \"\"\"\n+    GitLabのマージリクエスト情報を取得する。\n+\n+    Args:\n+        mr_iid (int): マージリクエストのIID。\n+\n+    Returns:\n+        list: マージリクエストの差分情報リスト。\n+    \"\"\"\n+    base = f\"{GITLAB_URL}/api/v4\"\n+    # 1. MR基本情報\n+    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n+    # 2. 差分情報（Diff）\n+    #    MRの差分情報を取得するエンドポイント\n+    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n+\n+    all_diffs = []\n+    current_page = 1\n+\n+    while True:\n+        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n+        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n+        try:\n+            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n+        except requests.exceptions.HTTPError as e:\n+            print(f\"APIリクエストに失敗しました: {e}\")\n+            break \n+        try:\n+            page_data = response.json()\n+        except requests.exceptions.JSONDecodeError:\n+            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n+            break\n+        if not isinstance(page_data, list):\n+            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n+            break\n+        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n+            break\n+\n+        all_diffs.extend(page_data)\n+\n+        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n+        total_pages_header = response.headers.get(\"x-total-pages\")\n+        if total_pages_header and current_page >= int(total_pages_header):\n+            break # 最後のページまで到達\n+        current_page += 1\n+    return all_diffs\n+\n+def main():\n+    \"\"\"\n+    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n+    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n+    \"\"\"\n+    # コマンドライン引数を解析\n+    parser = argparse.ArgumentParser(description=\"GitLab MR情報を取得してコードレビューを実行します。\")\n+    parser.add_argument(\"--mr_iid\", type=int, required=True, help=\"マージリクエストのIIDを指定してください。\")\n+    args = parser.parse_args()\n+\n+    # GitLab MR情報を取得\n+    diffs = get_mr_info(args.mr_iid)\n+    # レビュー観点をロード\n+    review_perspectives = load_review_perspectives()\n+    \n+    # 出力ファイルの末尾に付与する現在時刻を取得してフォーマット\n+    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n+    # レビュー結果出力ファイル名とログファイル名を生成\n+    output_file = f\"review_results_{timestamp}.md\"\n+    log_file = f\"api_call_logs_{timestamp}.json\"\n+    \n+    # 各DIFFに対してレビューを実行\n+    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n+        for diff in diffs:\n+            file_name = diff['new_path']\n+            diff_info = diff['diff']\n+            if not diff_info:\n+                continue\n+            f.write(f\"# レビュー対象：{file_name}\\n\")\n+            for perspective in review_perspectives:\n+                f.write(f\"## レビュー観点：{perspective}\\n\")\n+                review_result = review_code(perspective, file_name, diff_info, log_file)\n+                f.write(f\"{review_result}\\n\")\n+\n+if __name__ == \"__main__\":\n+    main()\n",
        "review_perspective": "ユニットテストが存在するか",
        "request_tokens": 3176,
        "api_params": {
            "model": "gpt-4.1-nano",
            "temperature": 0.2
        },
        "response": "この観点では問題ありません。",
        "response_tokens": 13,
        "start_time": "2025/05/26 04:49:28.240885",
        "end_time": "2025/05/26 04:49:28.745554"
    },
    {
        "file_name": "code_review.py",
        "diff_content": "@@ -0,0 +1,229 @@\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n+\n+PRIVATE_TOKEN = os.getenv(\"GITLAB_TOKEN\")\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+HEADERS = {\n+    \"PRIVATE-TOKEN\": PRIVATE_TOKEN\n+}\n+\n+CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n+TIKTOKEN_ENCODING = tiktoken.encoding_for_model(TIKTOKEN_MODEL)\n+\n+def load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n+    \"\"\"\n+    レビュー観点をJSONファイルから読み込む。\n+\n+    Args:\n+        file_path (str): レビュー観点が記載されたJSONファイルのパス。\n+\n+    Returns:\n+        list: レビュー観点のリスト。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+def load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n+    \"\"\"\n+    システムプロンプトをテキストファイルから読み込む。\n+\n+    Args:\n+        file_path (str): システムプロンプトが記載されたテキストファイルのパス。\n+\n+    Returns:\n+        str: システムプロンプトの内容。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return f.read()\n+\n+def num_tokens_from_messages(messages) -> int:\n+    \"\"\"\n+    メッセージからトークン数を計算する。\n+\n+    Args:\n+        messages (list): メッセージのリスト。\n+\n+    Returns:\n+        int: メッセージに含まれるトークン数。\n+    \"\"\"\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += TOKENS_PER_MESSAGE\n+        for key, value in message.items():\n+            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n+            if key == \"name\":\n+                num_tokens += TOKENS_PER_NAME\n+    num_tokens += 3  # assistantの返信の開始トークン\n+    return num_tokens\n+\n+def _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n+    \"\"\"\n+    OpenAIのChat APIを呼び出す関数。\n+\n+    Args:\n+        messages (list): APIに送信するメッセージリスト。\n+        model (str): 使用するモデル名。\n+        temperature (float): 出力の多様性を制御する温度パラメータ。\n+        max_tokens (int): 最大トークン数。\n+\n+    Returns:\n+        str: OpenAI APIからのレスポンス。\n+    \"\"\"\n+    response = CLIENT.chat.completions.create(\n+        model=model,\n+        messages=messages,\n+        temperature=temperature,\n+        max_tokens=max_tokens\n+    )\n+    return response.choices[0].message.content\n+\n+def review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n+    \"\"\"\n+    指定された観点でコードレビューを実行し、結果をログに記録する。\n+\n+    Args:\n+        review_perspective (str): レビュー観点。\n+        file_name (str): 対象ファイル名。\n+        diff_info (str): 差分情報。\n+        log_file (str): ログを保存するファイル名。\n+\n+    Returns:\n+        str: レビュー結果。\n+    \"\"\"\n+    # ファイル名と差分情報でシステムプロンプトをフォーマット\n+    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n+    messages = [\n+        {\"role\": \"system\", \"content\": current_system_prompt},\n+        {\"role\": \"user\", \"content\": review_perspective}\n+    ]\n+\n+    start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+    # OpenAI APIを呼び出してレビュー結果を取得\n+    response_str = _call_openai_chat_api(\n+        messages=messages,\n+        model=MODEL_NAME,\n+        temperature=TEMPERATURE,\n+    )\n+    end_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+\n+    log_data = {\n+        \"file_name\": file_name,\n+        \"diff_content\": diff_info,\n+        \"review_perspective\": review_perspective,\n+        \"request_tokens\": num_tokens_from_messages(messages),\n+        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n+        \"response\": response_str,\n+        \"response_tokens\": num_tokens_from_messages(response_str),\n+        \"start_time\": start_time,\n+        \"end_time\": end_time,\n+    }\n+\n+    # 既存のログデータをロード（なければ空リストを使う）\n+    if os.path.exists(log_file):\n+        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n+            try:\n+                existing_logs = json.load(f)\n+            except json.JSONDecodeError:\n+                existing_logs = []\n+    else:\n+        existing_logs = []\n+\n+    # 新しいログデータを追加\n+    existing_logs.append(log_data)\n+    \n+    # ログをJSON形式で保存\n+    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n+        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n+    return response_str\n+\n+def get_mr_info(mr_iid) -> list:\n+    \"\"\"\n+    GitLabのマージリクエスト情報を取得する。\n+\n+    Args:\n+        mr_iid (int): マージリクエストのIID。\n+\n+    Returns:\n+        list: マージリクエストの差分情報リスト。\n+    \"\"\"\n+    base = f\"{GITLAB_URL}/api/v4\"\n+    # 1. MR基本情報\n+    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n+    # 2. 差分情報（Diff）\n+    #    MRの差分情報を取得するエンドポイント\n+    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n+\n+    all_diffs = []\n+    current_page = 1\n+\n+    while True:\n+        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n+        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n+        try:\n+            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n+        except requests.exceptions.HTTPError as e:\n+            print(f\"APIリクエストに失敗しました: {e}\")\n+            break \n+        try:\n+            page_data = response.json()\n+        except requests.exceptions.JSONDecodeError:\n+            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n+            break\n+        if not isinstance(page_data, list):\n+            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n+            break\n+        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n+            break\n+\n+        all_diffs.extend(page_data)\n+\n+        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n+        total_pages_header = response.headers.get(\"x-total-pages\")\n+        if total_pages_header and current_page >= int(total_pages_header):\n+            break # 最後のページまで到達\n+        current_page += 1\n+    return all_diffs\n+\n+def main():\n+    \"\"\"\n+    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n+    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n+    \"\"\"\n+    # コマンドライン引数を解析\n+    parser = argparse.ArgumentParser(description=\"GitLab MR情報を取得してコードレビューを実行します。\")\n+    parser.add_argument(\"--mr_iid\", type=int, required=True, help=\"マージリクエストのIIDを指定してください。\")\n+    args = parser.parse_args()\n+\n+    # GitLab MR情報を取得\n+    diffs = get_mr_info(args.mr_iid)\n+    # レビュー観点をロード\n+    review_perspectives = load_review_perspectives()\n+    \n+    # 出力ファイルの末尾に付与する現在時刻を取得してフォーマット\n+    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n+    # レビュー結果出力ファイル名とログファイル名を生成\n+    output_file = f\"review_results_{timestamp}.md\"\n+    log_file = f\"api_call_logs_{timestamp}.json\"\n+    \n+    # 各DIFFに対してレビューを実行\n+    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n+        for diff in diffs:\n+            file_name = diff['new_path']\n+            diff_info = diff['diff']\n+            if not diff_info:\n+                continue\n+            f.write(f\"# レビュー対象：{file_name}\\n\")\n+            for perspective in review_perspectives:\n+                f.write(f\"## レビュー観点：{perspective}\\n\")\n+                review_result = review_code(perspective, file_name, diff_info, log_file)\n+                f.write(f\"{review_result}\\n\")\n+\n+if __name__ == \"__main__\":\n+    main()\n",
        "review_perspective": "関数やクラスがテストしやすい設計か",
        "request_tokens": 3183,
        "api_params": {
            "model": "gpt-4.1-nano",
            "temperature": 0.2
        },
        "response": "### 指摘対象コード:\n```python\ndef review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n    # ファイル名と差分情報でシステムプロンプトをフォーマット\n    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n    messages = [\n        {\"role\": \"system\", \"content\": current_system_prompt},\n        {\"role\": \"user\", \"content\": review_perspective}\n    ]\n    # OpenAI APIを呼び出してレビュー結果を取得\n    response_str = _call_openai_chat_api(\n        messages=messages,\n        model=MODEL_NAME,\n        temperature=TEMPERATURE,\n    )\n    # ログ保存処理\n    ...\n    return response_str\n```\n\n### 指摘内容:\nこの関数は、外部API呼び出しやファイル操作を含む複合的な処理を行っており、単体テストが難しい設計になっています。特に、`_call_openai_chat_api`や`load_system_prompt`といった外部依存部分が内部に直接埋め込まれているため、モックやスタブを使ったテストが困難です。\n\n### 修正提案:\n関数をよりテストしやすくするために、外部依存部分を引数として渡す形にリファクタリングしてください。例えば、API呼び出しやプロンプト読み込みを引数にして、依存性注入を行う設計にすると良いです。\n\n例:\n```python\ndef review_code(\n    review_perspective: str,\n    file_name: str,\n    diff_info: str,\n    log_file: str,\n    system_prompt_loader=load_system_prompt,\n    api_caller=_call_openai_chat_api\n) -> str:\n    current_system_prompt = system_prompt_loader().format(file_name=file_name, diff_info=diff_info)\n    messages = [\n        {\"role\": \"system\", \"content\": current_system_prompt},\n        {\"role\": \"user\", \"content\": review_perspective}\n    ]\n    response_str = api_caller(messages=messages, model=MODEL_NAME, temperature=TEMPERATURE)\n    # ログ保存処理も分離可能\n    ...\n    return response_str\n```\n\nこれにより、テスト時にはモック関数を渡すことで、外部APIやファイルアクセスを排除した単体テストが容易になります。",
        "response_tokens": 643,
        "start_time": "2025/05/26 04:49:28.755651",
        "end_time": "2025/05/26 04:49:34.764435"
    },
    {
        "file_name": "code_review.py",
        "diff_content": "@@ -0,0 +1,229 @@\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n+\n+PRIVATE_TOKEN = os.getenv(\"GITLAB_TOKEN\")\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+HEADERS = {\n+    \"PRIVATE-TOKEN\": PRIVATE_TOKEN\n+}\n+\n+CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n+TIKTOKEN_ENCODING = tiktoken.encoding_for_model(TIKTOKEN_MODEL)\n+\n+def load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n+    \"\"\"\n+    レビュー観点をJSONファイルから読み込む。\n+\n+    Args:\n+        file_path (str): レビュー観点が記載されたJSONファイルのパス。\n+\n+    Returns:\n+        list: レビュー観点のリスト。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+def load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n+    \"\"\"\n+    システムプロンプトをテキストファイルから読み込む。\n+\n+    Args:\n+        file_path (str): システムプロンプトが記載されたテキストファイルのパス。\n+\n+    Returns:\n+        str: システムプロンプトの内容。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return f.read()\n+\n+def num_tokens_from_messages(messages) -> int:\n+    \"\"\"\n+    メッセージからトークン数を計算する。\n+\n+    Args:\n+        messages (list): メッセージのリスト。\n+\n+    Returns:\n+        int: メッセージに含まれるトークン数。\n+    \"\"\"\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += TOKENS_PER_MESSAGE\n+        for key, value in message.items():\n+            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n+            if key == \"name\":\n+                num_tokens += TOKENS_PER_NAME\n+    num_tokens += 3  # assistantの返信の開始トークン\n+    return num_tokens\n+\n+def _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n+    \"\"\"\n+    OpenAIのChat APIを呼び出す関数。\n+\n+    Args:\n+        messages (list): APIに送信するメッセージリスト。\n+        model (str): 使用するモデル名。\n+        temperature (float): 出力の多様性を制御する温度パラメータ。\n+        max_tokens (int): 最大トークン数。\n+\n+    Returns:\n+        str: OpenAI APIからのレスポンス。\n+    \"\"\"\n+    response = CLIENT.chat.completions.create(\n+        model=model,\n+        messages=messages,\n+        temperature=temperature,\n+        max_tokens=max_tokens\n+    )\n+    return response.choices[0].message.content\n+\n+def review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n+    \"\"\"\n+    指定された観点でコードレビューを実行し、結果をログに記録する。\n+\n+    Args:\n+        review_perspective (str): レビュー観点。\n+        file_name (str): 対象ファイル名。\n+        diff_info (str): 差分情報。\n+        log_file (str): ログを保存するファイル名。\n+\n+    Returns:\n+        str: レビュー結果。\n+    \"\"\"\n+    # ファイル名と差分情報でシステムプロンプトをフォーマット\n+    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n+    messages = [\n+        {\"role\": \"system\", \"content\": current_system_prompt},\n+        {\"role\": \"user\", \"content\": review_perspective}\n+    ]\n+\n+    start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+    # OpenAI APIを呼び出してレビュー結果を取得\n+    response_str = _call_openai_chat_api(\n+        messages=messages,\n+        model=MODEL_NAME,\n+        temperature=TEMPERATURE,\n+    )\n+    end_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+\n+    log_data = {\n+        \"file_name\": file_name,\n+        \"diff_content\": diff_info,\n+        \"review_perspective\": review_perspective,\n+        \"request_tokens\": num_tokens_from_messages(messages),\n+        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n+        \"response\": response_str,\n+        \"response_tokens\": num_tokens_from_messages(response_str),\n+        \"start_time\": start_time,\n+        \"end_time\": end_time,\n+    }\n+\n+    # 既存のログデータをロード（なければ空リストを使う）\n+    if os.path.exists(log_file):\n+        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n+            try:\n+                existing_logs = json.load(f)\n+            except json.JSONDecodeError:\n+                existing_logs = []\n+    else:\n+        existing_logs = []\n+\n+    # 新しいログデータを追加\n+    existing_logs.append(log_data)\n+    \n+    # ログをJSON形式で保存\n+    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n+        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n+    return response_str\n+\n+def get_mr_info(mr_iid) -> list:\n+    \"\"\"\n+    GitLabのマージリクエスト情報を取得する。\n+\n+    Args:\n+        mr_iid (int): マージリクエストのIID。\n+\n+    Returns:\n+        list: マージリクエストの差分情報リスト。\n+    \"\"\"\n+    base = f\"{GITLAB_URL}/api/v4\"\n+    # 1. MR基本情報\n+    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n+    # 2. 差分情報（Diff）\n+    #    MRの差分情報を取得するエンドポイント\n+    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n+\n+    all_diffs = []\n+    current_page = 1\n+\n+    while True:\n+        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n+        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n+        try:\n+            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n+        except requests.exceptions.HTTPError as e:\n+            print(f\"APIリクエストに失敗しました: {e}\")\n+            break \n+        try:\n+            page_data = response.json()\n+        except requests.exceptions.JSONDecodeError:\n+            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n+            break\n+        if not isinstance(page_data, list):\n+            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n+            break\n+        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n+            break\n+\n+        all_diffs.extend(page_data)\n+\n+        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n+        total_pages_header = response.headers.get(\"x-total-pages\")\n+        if total_pages_header and current_page >= int(total_pages_header):\n+            break # 最後のページまで到達\n+        current_page += 1\n+    return all_diffs\n+\n+def main():\n+    \"\"\"\n+    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n+    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n+    \"\"\"\n+    # コマンドライン引数を解析\n+    parser = argparse.ArgumentParser(description=\"GitLab MR情報を取得してコードレビューを実行します。\")\n+    parser.add_argument(\"--mr_iid\", type=int, required=True, help=\"マージリクエストのIIDを指定してください。\")\n+    args = parser.parse_args()\n+\n+    # GitLab MR情報を取得\n+    diffs = get_mr_info(args.mr_iid)\n+    # レビュー観点をロード\n+    review_perspectives = load_review_perspectives()\n+    \n+    # 出力ファイルの末尾に付与する現在時刻を取得してフォーマット\n+    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n+    # レビュー結果出力ファイル名とログファイル名を生成\n+    output_file = f\"review_results_{timestamp}.md\"\n+    log_file = f\"api_call_logs_{timestamp}.json\"\n+    \n+    # 各DIFFに対してレビューを実行\n+    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n+        for diff in diffs:\n+            file_name = diff['new_path']\n+            diff_info = diff['diff']\n+            if not diff_info:\n+                continue\n+            f.write(f\"# レビュー対象：{file_name}\\n\")\n+            for perspective in review_perspectives:\n+                f.write(f\"## レビュー観点：{perspective}\\n\")\n+                review_result = review_code(perspective, file_name, diff_info, log_file)\n+                f.write(f\"{review_result}\\n\")\n+\n+if __name__ == \"__main__\":\n+    main()\n",
        "review_perspective": "モック・スタブなどが適切に使われているか",
        "request_tokens": 3188,
        "api_params": {
            "model": "gpt-4.1-nano",
            "temperature": 0.2
        },
        "response": "この観点では問題ありません。",
        "response_tokens": 13,
        "start_time": "2025/05/26 04:49:34.776834",
        "end_time": "2025/05/26 04:49:35.377383"
    },
    {
        "file_name": "code_review.py",
        "diff_content": "@@ -0,0 +1,229 @@\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n+\n+PRIVATE_TOKEN = os.getenv(\"GITLAB_TOKEN\")\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+HEADERS = {\n+    \"PRIVATE-TOKEN\": PRIVATE_TOKEN\n+}\n+\n+CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n+TIKTOKEN_ENCODING = tiktoken.encoding_for_model(TIKTOKEN_MODEL)\n+\n+def load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n+    \"\"\"\n+    レビュー観点をJSONファイルから読み込む。\n+\n+    Args:\n+        file_path (str): レビュー観点が記載されたJSONファイルのパス。\n+\n+    Returns:\n+        list: レビュー観点のリスト。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+def load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n+    \"\"\"\n+    システムプロンプトをテキストファイルから読み込む。\n+\n+    Args:\n+        file_path (str): システムプロンプトが記載されたテキストファイルのパス。\n+\n+    Returns:\n+        str: システムプロンプトの内容。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return f.read()\n+\n+def num_tokens_from_messages(messages) -> int:\n+    \"\"\"\n+    メッセージからトークン数を計算する。\n+\n+    Args:\n+        messages (list): メッセージのリスト。\n+\n+    Returns:\n+        int: メッセージに含まれるトークン数。\n+    \"\"\"\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += TOKENS_PER_MESSAGE\n+        for key, value in message.items():\n+            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n+            if key == \"name\":\n+                num_tokens += TOKENS_PER_NAME\n+    num_tokens += 3  # assistantの返信の開始トークン\n+    return num_tokens\n+\n+def _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n+    \"\"\"\n+    OpenAIのChat APIを呼び出す関数。\n+\n+    Args:\n+        messages (list): APIに送信するメッセージリスト。\n+        model (str): 使用するモデル名。\n+        temperature (float): 出力の多様性を制御する温度パラメータ。\n+        max_tokens (int): 最大トークン数。\n+\n+    Returns:\n+        str: OpenAI APIからのレスポンス。\n+    \"\"\"\n+    response = CLIENT.chat.completions.create(\n+        model=model,\n+        messages=messages,\n+        temperature=temperature,\n+        max_tokens=max_tokens\n+    )\n+    return response.choices[0].message.content\n+\n+def review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n+    \"\"\"\n+    指定された観点でコードレビューを実行し、結果をログに記録する。\n+\n+    Args:\n+        review_perspective (str): レビュー観点。\n+        file_name (str): 対象ファイル名。\n+        diff_info (str): 差分情報。\n+        log_file (str): ログを保存するファイル名。\n+\n+    Returns:\n+        str: レビュー結果。\n+    \"\"\"\n+    # ファイル名と差分情報でシステムプロンプトをフォーマット\n+    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n+    messages = [\n+        {\"role\": \"system\", \"content\": current_system_prompt},\n+        {\"role\": \"user\", \"content\": review_perspective}\n+    ]\n+\n+    start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+    # OpenAI APIを呼び出してレビュー結果を取得\n+    response_str = _call_openai_chat_api(\n+        messages=messages,\n+        model=MODEL_NAME,\n+        temperature=TEMPERATURE,\n+    )\n+    end_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+\n+    log_data = {\n+        \"file_name\": file_name,\n+        \"diff_content\": diff_info,\n+        \"review_perspective\": review_perspective,\n+        \"request_tokens\": num_tokens_from_messages(messages),\n+        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n+        \"response\": response_str,\n+        \"response_tokens\": num_tokens_from_messages(response_str),\n+        \"start_time\": start_time,\n+        \"end_time\": end_time,\n+    }\n+\n+    # 既存のログデータをロード（なければ空リストを使う）\n+    if os.path.exists(log_file):\n+        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n+            try:\n+                existing_logs = json.load(f)\n+            except json.JSONDecodeError:\n+                existing_logs = []\n+    else:\n+        existing_logs = []\n+\n+    # 新しいログデータを追加\n+    existing_logs.append(log_data)\n+    \n+    # ログをJSON形式で保存\n+    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n+        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n+    return response_str\n+\n+def get_mr_info(mr_iid) -> list:\n+    \"\"\"\n+    GitLabのマージリクエスト情報を取得する。\n+\n+    Args:\n+        mr_iid (int): マージリクエストのIID。\n+\n+    Returns:\n+        list: マージリクエストの差分情報リスト。\n+    \"\"\"\n+    base = f\"{GITLAB_URL}/api/v4\"\n+    # 1. MR基本情報\n+    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n+    # 2. 差分情報（Diff）\n+    #    MRの差分情報を取得するエンドポイント\n+    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n+\n+    all_diffs = []\n+    current_page = 1\n+\n+    while True:\n+        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n+        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n+        try:\n+            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n+        except requests.exceptions.HTTPError as e:\n+            print(f\"APIリクエストに失敗しました: {e}\")\n+            break \n+        try:\n+            page_data = response.json()\n+        except requests.exceptions.JSONDecodeError:\n+            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n+            break\n+        if not isinstance(page_data, list):\n+            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n+            break\n+        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n+            break\n+\n+        all_diffs.extend(page_data)\n+\n+        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n+        total_pages_header = response.headers.get(\"x-total-pages\")\n+        if total_pages_header and current_page >= int(total_pages_header):\n+            break # 最後のページまで到達\n+        current_page += 1\n+    return all_diffs\n+\n+def main():\n+    \"\"\"\n+    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n+    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n+    \"\"\"\n+    # コマンドライン引数を解析\n+    parser = argparse.ArgumentParser(description=\"GitLab MR情報を取得してコードレビューを実行します。\")\n+    parser.add_argument(\"--mr_iid\", type=int, required=True, help=\"マージリクエストのIIDを指定してください。\")\n+    args = parser.parse_args()\n+\n+    # GitLab MR情報を取得\n+    diffs = get_mr_info(args.mr_iid)\n+    # レビュー観点をロード\n+    review_perspectives = load_review_perspectives()\n+    \n+    # 出力ファイルの末尾に付与する現在時刻を取得してフォーマット\n+    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n+    # レビュー結果出力ファイル名とログファイル名を生成\n+    output_file = f\"review_results_{timestamp}.md\"\n+    log_file = f\"api_call_logs_{timestamp}.json\"\n+    \n+    # 各DIFFに対してレビューを実行\n+    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n+        for diff in diffs:\n+            file_name = diff['new_path']\n+            diff_info = diff['diff']\n+            if not diff_info:\n+                continue\n+            f.write(f\"# レビュー対象：{file_name}\\n\")\n+            for perspective in review_perspectives:\n+                f.write(f\"## レビュー観点：{perspective}\\n\")\n+                review_result = review_code(perspective, file_name, diff_info, log_file)\n+                f.write(f\"{review_result}\\n\")\n+\n+if __name__ == \"__main__\":\n+    main()\n",
        "review_perspective": "副作用が少ないコードか（純粋関数）",
        "request_tokens": 3185,
        "api_params": {
            "model": "gpt-4.1-nano",
            "temperature": 0.2
        },
        "response": "この観点では問題ありません。",
        "response_tokens": 13,
        "start_time": "2025/05/26 04:49:35.386402",
        "end_time": "2025/05/26 04:49:35.993398"
    },
    {
        "file_name": "code_review.py",
        "diff_content": "@@ -0,0 +1,229 @@\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n+\n+PRIVATE_TOKEN = os.getenv(\"GITLAB_TOKEN\")\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+HEADERS = {\n+    \"PRIVATE-TOKEN\": PRIVATE_TOKEN\n+}\n+\n+CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n+TIKTOKEN_ENCODING = tiktoken.encoding_for_model(TIKTOKEN_MODEL)\n+\n+def load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n+    \"\"\"\n+    レビュー観点をJSONファイルから読み込む。\n+\n+    Args:\n+        file_path (str): レビュー観点が記載されたJSONファイルのパス。\n+\n+    Returns:\n+        list: レビュー観点のリスト。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+def load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n+    \"\"\"\n+    システムプロンプトをテキストファイルから読み込む。\n+\n+    Args:\n+        file_path (str): システムプロンプトが記載されたテキストファイルのパス。\n+\n+    Returns:\n+        str: システムプロンプトの内容。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return f.read()\n+\n+def num_tokens_from_messages(messages) -> int:\n+    \"\"\"\n+    メッセージからトークン数を計算する。\n+\n+    Args:\n+        messages (list): メッセージのリスト。\n+\n+    Returns:\n+        int: メッセージに含まれるトークン数。\n+    \"\"\"\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += TOKENS_PER_MESSAGE\n+        for key, value in message.items():\n+            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n+            if key == \"name\":\n+                num_tokens += TOKENS_PER_NAME\n+    num_tokens += 3  # assistantの返信の開始トークン\n+    return num_tokens\n+\n+def _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n+    \"\"\"\n+    OpenAIのChat APIを呼び出す関数。\n+\n+    Args:\n+        messages (list): APIに送信するメッセージリスト。\n+        model (str): 使用するモデル名。\n+        temperature (float): 出力の多様性を制御する温度パラメータ。\n+        max_tokens (int): 最大トークン数。\n+\n+    Returns:\n+        str: OpenAI APIからのレスポンス。\n+    \"\"\"\n+    response = CLIENT.chat.completions.create(\n+        model=model,\n+        messages=messages,\n+        temperature=temperature,\n+        max_tokens=max_tokens\n+    )\n+    return response.choices[0].message.content\n+\n+def review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n+    \"\"\"\n+    指定された観点でコードレビューを実行し、結果をログに記録する。\n+\n+    Args:\n+        review_perspective (str): レビュー観点。\n+        file_name (str): 対象ファイル名。\n+        diff_info (str): 差分情報。\n+        log_file (str): ログを保存するファイル名。\n+\n+    Returns:\n+        str: レビュー結果。\n+    \"\"\"\n+    # ファイル名と差分情報でシステムプロンプトをフォーマット\n+    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n+    messages = [\n+        {\"role\": \"system\", \"content\": current_system_prompt},\n+        {\"role\": \"user\", \"content\": review_perspective}\n+    ]\n+\n+    start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+    # OpenAI APIを呼び出してレビュー結果を取得\n+    response_str = _call_openai_chat_api(\n+        messages=messages,\n+        model=MODEL_NAME,\n+        temperature=TEMPERATURE,\n+    )\n+    end_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+\n+    log_data = {\n+        \"file_name\": file_name,\n+        \"diff_content\": diff_info,\n+        \"review_perspective\": review_perspective,\n+        \"request_tokens\": num_tokens_from_messages(messages),\n+        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n+        \"response\": response_str,\n+        \"response_tokens\": num_tokens_from_messages(response_str),\n+        \"start_time\": start_time,\n+        \"end_time\": end_time,\n+    }\n+\n+    # 既存のログデータをロード（なければ空リストを使う）\n+    if os.path.exists(log_file):\n+        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n+            try:\n+                existing_logs = json.load(f)\n+            except json.JSONDecodeError:\n+                existing_logs = []\n+    else:\n+        existing_logs = []\n+\n+    # 新しいログデータを追加\n+    existing_logs.append(log_data)\n+    \n+    # ログをJSON形式で保存\n+    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n+        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n+    return response_str\n+\n+def get_mr_info(mr_iid) -> list:\n+    \"\"\"\n+    GitLabのマージリクエスト情報を取得する。\n+\n+    Args:\n+        mr_iid (int): マージリクエストのIID。\n+\n+    Returns:\n+        list: マージリクエストの差分情報リスト。\n+    \"\"\"\n+    base = f\"{GITLAB_URL}/api/v4\"\n+    # 1. MR基本情報\n+    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n+    # 2. 差分情報（Diff）\n+    #    MRの差分情報を取得するエンドポイント\n+    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n+\n+    all_diffs = []\n+    current_page = 1\n+\n+    while True:\n+        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n+        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n+        try:\n+            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n+        except requests.exceptions.HTTPError as e:\n+            print(f\"APIリクエストに失敗しました: {e}\")\n+            break \n+        try:\n+            page_data = response.json()\n+        except requests.exceptions.JSONDecodeError:\n+            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n+            break\n+        if not isinstance(page_data, list):\n+            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n+            break\n+        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n+            break\n+\n+        all_diffs.extend(page_data)\n+\n+        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n+        total_pages_header = response.headers.get(\"x-total-pages\")\n+        if total_pages_header and current_page >= int(total_pages_header):\n+            break # 最後のページまで到達\n+        current_page += 1\n+    return all_diffs\n+\n+def main():\n+    \"\"\"\n+    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n+    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n+    \"\"\"\n+    # コマンドライン引数を解析\n+    parser = argparse.ArgumentParser(description=\"GitLab MR情報を取得してコードレビューを実行します。\")\n+    parser.add_argument(\"--mr_iid\", type=int, required=True, help=\"マージリクエストのIIDを指定してください。\")\n+    args = parser.parse_args()\n+\n+    # GitLab MR情報を取得\n+    diffs = get_mr_info(args.mr_iid)\n+    # レビュー観点をロード\n+    review_perspectives = load_review_perspectives()\n+    \n+    # 出力ファイルの末尾に付与する現在時刻を取得してフォーマット\n+    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n+    # レビュー結果出力ファイル名とログファイル名を生成\n+    output_file = f\"review_results_{timestamp}.md\"\n+    log_file = f\"api_call_logs_{timestamp}.json\"\n+    \n+    # 各DIFFに対してレビューを実行\n+    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n+        for diff in diffs:\n+            file_name = diff['new_path']\n+            diff_info = diff['diff']\n+            if not diff_info:\n+                continue\n+            f.write(f\"# レビュー対象：{file_name}\\n\")\n+            for perspective in review_perspectives:\n+                f.write(f\"## レビュー観点：{perspective}\\n\")\n+                review_result = review_code(perspective, file_name, diff_info, log_file)\n+                f.write(f\"{review_result}\\n\")\n+\n+if __name__ == \"__main__\":\n+    main()\n",
        "review_perspective": "型ヒント（type hints / annotations）が記述されているか",
        "request_tokens": 3184,
        "api_params": {
            "model": "gpt-4.1-nano",
            "temperature": 0.2
        },
        "response": "### 指摘対象コード:\n```python\ndef load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\ndef load_system_prompt(file_path=\"system_prompt.txt\") -> str:\ndef num_tokens_from_messages(messages) -> int:\ndef _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\ndef review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\ndef get_mr_info(mr_iid) -> list:\n```\n\n### 指摘内容:\n関数の引数や返り値に対して型ヒント（type hints / annotations）が一部記述されているが、`get_mr_info`関数の引数`mr_iid`には型ヒントが付いていません。\n\n### 修正提案:\n`get_mr_info`関数の引数`mr_iid`に`int`型の型ヒントを追加してください。\n\n修正例:\n```python\ndef get_mr_info(mr_iid: int) -> list:\n```",
        "response_tokens": 260,
        "start_time": "2025/05/26 04:49:36.004708",
        "end_time": "2025/05/26 04:49:38.449181"
    },
    {
        "file_name": "code_review.py",
        "diff_content": "@@ -0,0 +1,229 @@\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n+\n+PRIVATE_TOKEN = os.getenv(\"GITLAB_TOKEN\")\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+HEADERS = {\n+    \"PRIVATE-TOKEN\": PRIVATE_TOKEN\n+}\n+\n+CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n+TIKTOKEN_ENCODING = tiktoken.encoding_for_model(TIKTOKEN_MODEL)\n+\n+def load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n+    \"\"\"\n+    レビュー観点をJSONファイルから読み込む。\n+\n+    Args:\n+        file_path (str): レビュー観点が記載されたJSONファイルのパス。\n+\n+    Returns:\n+        list: レビュー観点のリスト。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+def load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n+    \"\"\"\n+    システムプロンプトをテキストファイルから読み込む。\n+\n+    Args:\n+        file_path (str): システムプロンプトが記載されたテキストファイルのパス。\n+\n+    Returns:\n+        str: システムプロンプトの内容。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return f.read()\n+\n+def num_tokens_from_messages(messages) -> int:\n+    \"\"\"\n+    メッセージからトークン数を計算する。\n+\n+    Args:\n+        messages (list): メッセージのリスト。\n+\n+    Returns:\n+        int: メッセージに含まれるトークン数。\n+    \"\"\"\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += TOKENS_PER_MESSAGE\n+        for key, value in message.items():\n+            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n+            if key == \"name\":\n+                num_tokens += TOKENS_PER_NAME\n+    num_tokens += 3  # assistantの返信の開始トークン\n+    return num_tokens\n+\n+def _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n+    \"\"\"\n+    OpenAIのChat APIを呼び出す関数。\n+\n+    Args:\n+        messages (list): APIに送信するメッセージリスト。\n+        model (str): 使用するモデル名。\n+        temperature (float): 出力の多様性を制御する温度パラメータ。\n+        max_tokens (int): 最大トークン数。\n+\n+    Returns:\n+        str: OpenAI APIからのレスポンス。\n+    \"\"\"\n+    response = CLIENT.chat.completions.create(\n+        model=model,\n+        messages=messages,\n+        temperature=temperature,\n+        max_tokens=max_tokens\n+    )\n+    return response.choices[0].message.content\n+\n+def review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n+    \"\"\"\n+    指定された観点でコードレビューを実行し、結果をログに記録する。\n+\n+    Args:\n+        review_perspective (str): レビュー観点。\n+        file_name (str): 対象ファイル名。\n+        diff_info (str): 差分情報。\n+        log_file (str): ログを保存するファイル名。\n+\n+    Returns:\n+        str: レビュー結果。\n+    \"\"\"\n+    # ファイル名と差分情報でシステムプロンプトをフォーマット\n+    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n+    messages = [\n+        {\"role\": \"system\", \"content\": current_system_prompt},\n+        {\"role\": \"user\", \"content\": review_perspective}\n+    ]\n+\n+    start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+    # OpenAI APIを呼び出してレビュー結果を取得\n+    response_str = _call_openai_chat_api(\n+        messages=messages,\n+        model=MODEL_NAME,\n+        temperature=TEMPERATURE,\n+    )\n+    end_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+\n+    log_data = {\n+        \"file_name\": file_name,\n+        \"diff_content\": diff_info,\n+        \"review_perspective\": review_perspective,\n+        \"request_tokens\": num_tokens_from_messages(messages),\n+        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n+        \"response\": response_str,\n+        \"response_tokens\": num_tokens_from_messages(response_str),\n+        \"start_time\": start_time,\n+        \"end_time\": end_time,\n+    }\n+\n+    # 既存のログデータをロード（なければ空リストを使う）\n+    if os.path.exists(log_file):\n+        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n+            try:\n+                existing_logs = json.load(f)\n+            except json.JSONDecodeError:\n+                existing_logs = []\n+    else:\n+        existing_logs = []\n+\n+    # 新しいログデータを追加\n+    existing_logs.append(log_data)\n+    \n+    # ログをJSON形式で保存\n+    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n+        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n+    return response_str\n+\n+def get_mr_info(mr_iid) -> list:\n+    \"\"\"\n+    GitLabのマージリクエスト情報を取得する。\n+\n+    Args:\n+        mr_iid (int): マージリクエストのIID。\n+\n+    Returns:\n+        list: マージリクエストの差分情報リスト。\n+    \"\"\"\n+    base = f\"{GITLAB_URL}/api/v4\"\n+    # 1. MR基本情報\n+    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n+    # 2. 差分情報（Diff）\n+    #    MRの差分情報を取得するエンドポイント\n+    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n+\n+    all_diffs = []\n+    current_page = 1\n+\n+    while True:\n+        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n+        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n+        try:\n+            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n+        except requests.exceptions.HTTPError as e:\n+            print(f\"APIリクエストに失敗しました: {e}\")\n+            break \n+        try:\n+            page_data = response.json()\n+        except requests.exceptions.JSONDecodeError:\n+            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n+            break\n+        if not isinstance(page_data, list):\n+            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n+            break\n+        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n+            break\n+\n+        all_diffs.extend(page_data)\n+\n+        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n+        total_pages_header = response.headers.get(\"x-total-pages\")\n+        if total_pages_header and current_page >= int(total_pages_header):\n+            break # 最後のページまで到達\n+        current_page += 1\n+    return all_diffs\n+\n+def main():\n+    \"\"\"\n+    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n+    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n+    \"\"\"\n+    # コマンドライン引数を解析\n+    parser = argparse.ArgumentParser(description=\"GitLab MR情報を取得してコードレビューを実行します。\")\n+    parser.add_argument(\"--mr_iid\", type=int, required=True, help=\"マージリクエストのIIDを指定してください。\")\n+    args = parser.parse_args()\n+\n+    # GitLab MR情報を取得\n+    diffs = get_mr_info(args.mr_iid)\n+    # レビュー観点をロード\n+    review_perspectives = load_review_perspectives()\n+    \n+    # 出力ファイルの末尾に付与する現在時刻を取得してフォーマット\n+    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n+    # レビュー結果出力ファイル名とログファイル名を生成\n+    output_file = f\"review_results_{timestamp}.md\"\n+    log_file = f\"api_call_logs_{timestamp}.json\"\n+    \n+    # 各DIFFに対してレビューを実行\n+    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n+        for diff in diffs:\n+            file_name = diff['new_path']\n+            diff_info = diff['diff']\n+            if not diff_info:\n+                continue\n+            f.write(f\"# レビュー対象：{file_name}\\n\")\n+            for perspective in review_perspectives:\n+                f.write(f\"## レビュー観点：{perspective}\\n\")\n+                review_result = review_code(perspective, file_name, diff_info, log_file)\n+                f.write(f\"{review_result}\\n\")\n+\n+if __name__ == \"__main__\":\n+    main()\n",
        "review_perspective": "不要なimportがないか",
        "request_tokens": 3174,
        "api_params": {
            "model": "gpt-4.1-nano",
            "temperature": 0.2
        },
        "response": "### 指摘対象コード:\n```python\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n```\n\n### 指摘内容:\n`OpenAI`クラスのインポートは`from openai import OpenAI`としていますが、実際のコード内では`OpenAI`をインスタンス化しているだけで、`openai`モジュール全体を使用していません。そのため、`from openai import OpenAI`は必要ですが、`import openai`は不要です。\n\nまた、`requests`、`argparse`、`os`、`json`、`datetime`はそれぞれ使用されているため問題ありません。\n\n### 修正提案:\n`from openai import OpenAI`はそのままで良いですが、`import openai`は削除してください。  \nまた、不要なimportがあれば削除してください。\n\n```python\n- import openai\n+ # import openai  # 不要な場合はコメントアウトまたは削除\n```",
        "response_tokens": 325,
        "start_time": "2025/05/26 04:49:38.459752",
        "end_time": "2025/05/26 04:49:41.194974"
    },
    {
        "file_name": "code_review.py",
        "diff_content": "@@ -0,0 +1,229 @@\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n+\n+PRIVATE_TOKEN = os.getenv(\"GITLAB_TOKEN\")\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+HEADERS = {\n+    \"PRIVATE-TOKEN\": PRIVATE_TOKEN\n+}\n+\n+CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n+TIKTOKEN_ENCODING = tiktoken.encoding_for_model(TIKTOKEN_MODEL)\n+\n+def load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n+    \"\"\"\n+    レビュー観点をJSONファイルから読み込む。\n+\n+    Args:\n+        file_path (str): レビュー観点が記載されたJSONファイルのパス。\n+\n+    Returns:\n+        list: レビュー観点のリスト。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+def load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n+    \"\"\"\n+    システムプロンプトをテキストファイルから読み込む。\n+\n+    Args:\n+        file_path (str): システムプロンプトが記載されたテキストファイルのパス。\n+\n+    Returns:\n+        str: システムプロンプトの内容。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return f.read()\n+\n+def num_tokens_from_messages(messages) -> int:\n+    \"\"\"\n+    メッセージからトークン数を計算する。\n+\n+    Args:\n+        messages (list): メッセージのリスト。\n+\n+    Returns:\n+        int: メッセージに含まれるトークン数。\n+    \"\"\"\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += TOKENS_PER_MESSAGE\n+        for key, value in message.items():\n+            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n+            if key == \"name\":\n+                num_tokens += TOKENS_PER_NAME\n+    num_tokens += 3  # assistantの返信の開始トークン\n+    return num_tokens\n+\n+def _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n+    \"\"\"\n+    OpenAIのChat APIを呼び出す関数。\n+\n+    Args:\n+        messages (list): APIに送信するメッセージリスト。\n+        model (str): 使用するモデル名。\n+        temperature (float): 出力の多様性を制御する温度パラメータ。\n+        max_tokens (int): 最大トークン数。\n+\n+    Returns:\n+        str: OpenAI APIからのレスポンス。\n+    \"\"\"\n+    response = CLIENT.chat.completions.create(\n+        model=model,\n+        messages=messages,\n+        temperature=temperature,\n+        max_tokens=max_tokens\n+    )\n+    return response.choices[0].message.content\n+\n+def review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n+    \"\"\"\n+    指定された観点でコードレビューを実行し、結果をログに記録する。\n+\n+    Args:\n+        review_perspective (str): レビュー観点。\n+        file_name (str): 対象ファイル名。\n+        diff_info (str): 差分情報。\n+        log_file (str): ログを保存するファイル名。\n+\n+    Returns:\n+        str: レビュー結果。\n+    \"\"\"\n+    # ファイル名と差分情報でシステムプロンプトをフォーマット\n+    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n+    messages = [\n+        {\"role\": \"system\", \"content\": current_system_prompt},\n+        {\"role\": \"user\", \"content\": review_perspective}\n+    ]\n+\n+    start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+    # OpenAI APIを呼び出してレビュー結果を取得\n+    response_str = _call_openai_chat_api(\n+        messages=messages,\n+        model=MODEL_NAME,\n+        temperature=TEMPERATURE,\n+    )\n+    end_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+\n+    log_data = {\n+        \"file_name\": file_name,\n+        \"diff_content\": diff_info,\n+        \"review_perspective\": review_perspective,\n+        \"request_tokens\": num_tokens_from_messages(messages),\n+        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n+        \"response\": response_str,\n+        \"response_tokens\": num_tokens_from_messages(response_str),\n+        \"start_time\": start_time,\n+        \"end_time\": end_time,\n+    }\n+\n+    # 既存のログデータをロード（なければ空リストを使う）\n+    if os.path.exists(log_file):\n+        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n+            try:\n+                existing_logs = json.load(f)\n+            except json.JSONDecodeError:\n+                existing_logs = []\n+    else:\n+        existing_logs = []\n+\n+    # 新しいログデータを追加\n+    existing_logs.append(log_data)\n+    \n+    # ログをJSON形式で保存\n+    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n+        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n+    return response_str\n+\n+def get_mr_info(mr_iid) -> list:\n+    \"\"\"\n+    GitLabのマージリクエスト情報を取得する。\n+\n+    Args:\n+        mr_iid (int): マージリクエストのIID。\n+\n+    Returns:\n+        list: マージリクエストの差分情報リスト。\n+    \"\"\"\n+    base = f\"{GITLAB_URL}/api/v4\"\n+    # 1. MR基本情報\n+    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n+    # 2. 差分情報（Diff）\n+    #    MRの差分情報を取得するエンドポイント\n+    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n+\n+    all_diffs = []\n+    current_page = 1\n+\n+    while True:\n+        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n+        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n+        try:\n+            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n+        except requests.exceptions.HTTPError as e:\n+            print(f\"APIリクエストに失敗しました: {e}\")\n+            break \n+        try:\n+            page_data = response.json()\n+        except requests.exceptions.JSONDecodeError:\n+            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n+            break\n+        if not isinstance(page_data, list):\n+            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n+            break\n+        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n+            break\n+\n+        all_diffs.extend(page_data)\n+\n+        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n+        total_pages_header = response.headers.get(\"x-total-pages\")\n+        if total_pages_header and current_page >= int(total_pages_header):\n+            break # 最後のページまで到達\n+        current_page += 1\n+    return all_diffs\n+\n+def main():\n+    \"\"\"\n+    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n+    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n+    \"\"\"\n+    # コマンドライン引数を解析\n+    parser = argparse.ArgumentParser(description=\"GitLab MR情報を取得してコードレビューを実行します。\")\n+    parser.add_argument(\"--mr_iid\", type=int, required=True, help=\"マージリクエストのIIDを指定してください。\")\n+    args = parser.parse_args()\n+\n+    # GitLab MR情報を取得\n+    diffs = get_mr_info(args.mr_iid)\n+    # レビュー観点をロード\n+    review_perspectives = load_review_perspectives()\n+    \n+    # 出力ファイルの末尾に付与する現在時刻を取得してフォーマット\n+    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n+    # レビュー結果出力ファイル名とログファイル名を生成\n+    output_file = f\"review_results_{timestamp}.md\"\n+    log_file = f\"api_call_logs_{timestamp}.json\"\n+    \n+    # 各DIFFに対してレビューを実行\n+    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n+        for diff in diffs:\n+            file_name = diff['new_path']\n+            diff_info = diff['diff']\n+            if not diff_info:\n+                continue\n+            f.write(f\"# レビュー対象：{file_name}\\n\")\n+            for perspective in review_perspectives:\n+                f.write(f\"## レビュー観点：{perspective}\\n\")\n+                review_result = review_code(perspective, file_name, diff_info, log_file)\n+                f.write(f\"{review_result}\\n\")\n+\n+if __name__ == \"__main__\":\n+    main()\n",
        "review_perspective": "相対importと絶対importが一貫しているか",
        "request_tokens": 3184,
        "api_params": {
            "model": "gpt-4.1-nano",
            "temperature": 0.2
        },
        "response": "### 指摘対象コード:\n```python\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n```\n\n### 指摘内容:\nこのコードでは、`constants`モジュールを`from constants import ...`の形式で絶対インポートしています。一方、標準ライブラリやサードパーティライブラリは`import`文で直接インポートしています。\n\nただし、`constants`が同じパッケージ内のモジュールである場合は相対インポートを用いるのが一般的です。現状のコードでは、`constants`がどの位置にあるか明示されていませんが、ファイルがトップレベルのスクリプトである場合は絶対インポートが適切です。\n\n### 修正提案:\n- `constants`が同じパッケージ内にある場合は、相対インポート（例：`from .constants import ...`）を検討してください。\n- もし`constants`がプロジェクトのルートにあるモジュールであれば、現状の絶対インポートは一貫しています。\n\n**結論**：このコードでは、標準ライブラリやサードパーティライブラリと`constants`のインポート方法が一貫していません。  \n`constants`が同じパッケージ内であれば、相対インポートに統一すべきです。",
        "response_tokens": 473,
        "start_time": "2025/05/26 04:49:41.209719",
        "end_time": "2025/05/26 04:49:44.903818"
    },
    {
        "file_name": "code_review.py",
        "diff_content": "@@ -0,0 +1,229 @@\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n+\n+PRIVATE_TOKEN = os.getenv(\"GITLAB_TOKEN\")\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+HEADERS = {\n+    \"PRIVATE-TOKEN\": PRIVATE_TOKEN\n+}\n+\n+CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n+TIKTOKEN_ENCODING = tiktoken.encoding_for_model(TIKTOKEN_MODEL)\n+\n+def load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n+    \"\"\"\n+    レビュー観点をJSONファイルから読み込む。\n+\n+    Args:\n+        file_path (str): レビュー観点が記載されたJSONファイルのパス。\n+\n+    Returns:\n+        list: レビュー観点のリスト。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+def load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n+    \"\"\"\n+    システムプロンプトをテキストファイルから読み込む。\n+\n+    Args:\n+        file_path (str): システムプロンプトが記載されたテキストファイルのパス。\n+\n+    Returns:\n+        str: システムプロンプトの内容。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return f.read()\n+\n+def num_tokens_from_messages(messages) -> int:\n+    \"\"\"\n+    メッセージからトークン数を計算する。\n+\n+    Args:\n+        messages (list): メッセージのリスト。\n+\n+    Returns:\n+        int: メッセージに含まれるトークン数。\n+    \"\"\"\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += TOKENS_PER_MESSAGE\n+        for key, value in message.items():\n+            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n+            if key == \"name\":\n+                num_tokens += TOKENS_PER_NAME\n+    num_tokens += 3  # assistantの返信の開始トークン\n+    return num_tokens\n+\n+def _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n+    \"\"\"\n+    OpenAIのChat APIを呼び出す関数。\n+\n+    Args:\n+        messages (list): APIに送信するメッセージリスト。\n+        model (str): 使用するモデル名。\n+        temperature (float): 出力の多様性を制御する温度パラメータ。\n+        max_tokens (int): 最大トークン数。\n+\n+    Returns:\n+        str: OpenAI APIからのレスポンス。\n+    \"\"\"\n+    response = CLIENT.chat.completions.create(\n+        model=model,\n+        messages=messages,\n+        temperature=temperature,\n+        max_tokens=max_tokens\n+    )\n+    return response.choices[0].message.content\n+\n+def review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n+    \"\"\"\n+    指定された観点でコードレビューを実行し、結果をログに記録する。\n+\n+    Args:\n+        review_perspective (str): レビュー観点。\n+        file_name (str): 対象ファイル名。\n+        diff_info (str): 差分情報。\n+        log_file (str): ログを保存するファイル名。\n+\n+    Returns:\n+        str: レビュー結果。\n+    \"\"\"\n+    # ファイル名と差分情報でシステムプロンプトをフォーマット\n+    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n+    messages = [\n+        {\"role\": \"system\", \"content\": current_system_prompt},\n+        {\"role\": \"user\", \"content\": review_perspective}\n+    ]\n+\n+    start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+    # OpenAI APIを呼び出してレビュー結果を取得\n+    response_str = _call_openai_chat_api(\n+        messages=messages,\n+        model=MODEL_NAME,\n+        temperature=TEMPERATURE,\n+    )\n+    end_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+\n+    log_data = {\n+        \"file_name\": file_name,\n+        \"diff_content\": diff_info,\n+        \"review_perspective\": review_perspective,\n+        \"request_tokens\": num_tokens_from_messages(messages),\n+        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n+        \"response\": response_str,\n+        \"response_tokens\": num_tokens_from_messages(response_str),\n+        \"start_time\": start_time,\n+        \"end_time\": end_time,\n+    }\n+\n+    # 既存のログデータをロード（なければ空リストを使う）\n+    if os.path.exists(log_file):\n+        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n+            try:\n+                existing_logs = json.load(f)\n+            except json.JSONDecodeError:\n+                existing_logs = []\n+    else:\n+        existing_logs = []\n+\n+    # 新しいログデータを追加\n+    existing_logs.append(log_data)\n+    \n+    # ログをJSON形式で保存\n+    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n+        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n+    return response_str\n+\n+def get_mr_info(mr_iid) -> list:\n+    \"\"\"\n+    GitLabのマージリクエスト情報を取得する。\n+\n+    Args:\n+        mr_iid (int): マージリクエストのIID。\n+\n+    Returns:\n+        list: マージリクエストの差分情報リスト。\n+    \"\"\"\n+    base = f\"{GITLAB_URL}/api/v4\"\n+    # 1. MR基本情報\n+    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n+    # 2. 差分情報（Diff）\n+    #    MRの差分情報を取得するエンドポイント\n+    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n+\n+    all_diffs = []\n+    current_page = 1\n+\n+    while True:\n+        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n+        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n+        try:\n+            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n+        except requests.exceptions.HTTPError as e:\n+            print(f\"APIリクエストに失敗しました: {e}\")\n+            break \n+        try:\n+            page_data = response.json()\n+        except requests.exceptions.JSONDecodeError:\n+            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n+            break\n+        if not isinstance(page_data, list):\n+            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n+            break\n+        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n+            break\n+\n+        all_diffs.extend(page_data)\n+\n+        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n+        total_pages_header = response.headers.get(\"x-total-pages\")\n+        if total_pages_header and current_page >= int(total_pages_header):\n+            break # 最後のページまで到達\n+        current_page += 1\n+    return all_diffs\n+\n+def main():\n+    \"\"\"\n+    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n+    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n+    \"\"\"\n+    # コマンドライン引数を解析\n+    parser = argparse.ArgumentParser(description=\"GitLab MR情報を取得してコードレビューを実行します。\")\n+    parser.add_argument(\"--mr_iid\", type=int, required=True, help=\"マージリクエストのIIDを指定してください。\")\n+    args = parser.parse_args()\n+\n+    # GitLab MR情報を取得\n+    diffs = get_mr_info(args.mr_iid)\n+    # レビュー観点をロード\n+    review_perspectives = load_review_perspectives()\n+    \n+    # 出力ファイルの末尾に付与する現在時刻を取得してフォーマット\n+    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n+    # レビュー結果出力ファイル名とログファイル名を生成\n+    output_file = f\"review_results_{timestamp}.md\"\n+    log_file = f\"api_call_logs_{timestamp}.json\"\n+    \n+    # 各DIFFに対してレビューを実行\n+    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n+        for diff in diffs:\n+            file_name = diff['new_path']\n+            diff_info = diff['diff']\n+            if not diff_info:\n+                continue\n+            f.write(f\"# レビュー対象：{file_name}\\n\")\n+            for perspective in review_perspectives:\n+                f.write(f\"## レビュー観点：{perspective}\\n\")\n+                review_result = review_code(perspective, file_name, diff_info, log_file)\n+                f.write(f\"{review_result}\\n\")\n+\n+if __name__ == \"__main__\":\n+    main()\n",
        "review_perspective": "外部ライブラリの依存が最小限に抑えられているか",
        "request_tokens": 3191,
        "api_params": {
            "model": "gpt-4.1-nano",
            "temperature": 0.2
        },
        "response": "この観点では問題ありません。",
        "response_tokens": 13,
        "start_time": "2025/05/26 04:49:44.914523",
        "end_time": "2025/05/26 04:49:45.515068"
    },
    {
        "file_name": "code_review.py",
        "diff_content": "@@ -0,0 +1,229 @@\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n+\n+PRIVATE_TOKEN = os.getenv(\"GITLAB_TOKEN\")\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+HEADERS = {\n+    \"PRIVATE-TOKEN\": PRIVATE_TOKEN\n+}\n+\n+CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n+TIKTOKEN_ENCODING = tiktoken.encoding_for_model(TIKTOKEN_MODEL)\n+\n+def load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n+    \"\"\"\n+    レビュー観点をJSONファイルから読み込む。\n+\n+    Args:\n+        file_path (str): レビュー観点が記載されたJSONファイルのパス。\n+\n+    Returns:\n+        list: レビュー観点のリスト。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+def load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n+    \"\"\"\n+    システムプロンプトをテキストファイルから読み込む。\n+\n+    Args:\n+        file_path (str): システムプロンプトが記載されたテキストファイルのパス。\n+\n+    Returns:\n+        str: システムプロンプトの内容。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return f.read()\n+\n+def num_tokens_from_messages(messages) -> int:\n+    \"\"\"\n+    メッセージからトークン数を計算する。\n+\n+    Args:\n+        messages (list): メッセージのリスト。\n+\n+    Returns:\n+        int: メッセージに含まれるトークン数。\n+    \"\"\"\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += TOKENS_PER_MESSAGE\n+        for key, value in message.items():\n+            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n+            if key == \"name\":\n+                num_tokens += TOKENS_PER_NAME\n+    num_tokens += 3  # assistantの返信の開始トークン\n+    return num_tokens\n+\n+def _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n+    \"\"\"\n+    OpenAIのChat APIを呼び出す関数。\n+\n+    Args:\n+        messages (list): APIに送信するメッセージリスト。\n+        model (str): 使用するモデル名。\n+        temperature (float): 出力の多様性を制御する温度パラメータ。\n+        max_tokens (int): 最大トークン数。\n+\n+    Returns:\n+        str: OpenAI APIからのレスポンス。\n+    \"\"\"\n+    response = CLIENT.chat.completions.create(\n+        model=model,\n+        messages=messages,\n+        temperature=temperature,\n+        max_tokens=max_tokens\n+    )\n+    return response.choices[0].message.content\n+\n+def review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n+    \"\"\"\n+    指定された観点でコードレビューを実行し、結果をログに記録する。\n+\n+    Args:\n+        review_perspective (str): レビュー観点。\n+        file_name (str): 対象ファイル名。\n+        diff_info (str): 差分情報。\n+        log_file (str): ログを保存するファイル名。\n+\n+    Returns:\n+        str: レビュー結果。\n+    \"\"\"\n+    # ファイル名と差分情報でシステムプロンプトをフォーマット\n+    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n+    messages = [\n+        {\"role\": \"system\", \"content\": current_system_prompt},\n+        {\"role\": \"user\", \"content\": review_perspective}\n+    ]\n+\n+    start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+    # OpenAI APIを呼び出してレビュー結果を取得\n+    response_str = _call_openai_chat_api(\n+        messages=messages,\n+        model=MODEL_NAME,\n+        temperature=TEMPERATURE,\n+    )\n+    end_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+\n+    log_data = {\n+        \"file_name\": file_name,\n+        \"diff_content\": diff_info,\n+        \"review_perspective\": review_perspective,\n+        \"request_tokens\": num_tokens_from_messages(messages),\n+        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n+        \"response\": response_str,\n+        \"response_tokens\": num_tokens_from_messages(response_str),\n+        \"start_time\": start_time,\n+        \"end_time\": end_time,\n+    }\n+\n+    # 既存のログデータをロード（なければ空リストを使う）\n+    if os.path.exists(log_file):\n+        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n+            try:\n+                existing_logs = json.load(f)\n+            except json.JSONDecodeError:\n+                existing_logs = []\n+    else:\n+        existing_logs = []\n+\n+    # 新しいログデータを追加\n+    existing_logs.append(log_data)\n+    \n+    # ログをJSON形式で保存\n+    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n+        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n+    return response_str\n+\n+def get_mr_info(mr_iid) -> list:\n+    \"\"\"\n+    GitLabのマージリクエスト情報を取得する。\n+\n+    Args:\n+        mr_iid (int): マージリクエストのIID。\n+\n+    Returns:\n+        list: マージリクエストの差分情報リスト。\n+    \"\"\"\n+    base = f\"{GITLAB_URL}/api/v4\"\n+    # 1. MR基本情報\n+    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n+    # 2. 差分情報（Diff）\n+    #    MRの差分情報を取得するエンドポイント\n+    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n+\n+    all_diffs = []\n+    current_page = 1\n+\n+    while True:\n+        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n+        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n+        try:\n+            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n+        except requests.exceptions.HTTPError as e:\n+            print(f\"APIリクエストに失敗しました: {e}\")\n+            break \n+        try:\n+            page_data = response.json()\n+        except requests.exceptions.JSONDecodeError:\n+            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n+            break\n+        if not isinstance(page_data, list):\n+            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n+            break\n+        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n+            break\n+\n+        all_diffs.extend(page_data)\n+\n+        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n+        total_pages_header = response.headers.get(\"x-total-pages\")\n+        if total_pages_header and current_page >= int(total_pages_header):\n+            break # 最後のページまで到達\n+        current_page += 1\n+    return all_diffs\n+\n+def main():\n+    \"\"\"\n+    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n+    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n+    \"\"\"\n+    # コマンドライン引数を解析\n+    parser = argparse.ArgumentParser(description=\"GitLab MR情報を取得してコードレビューを実行します。\")\n+    parser.add_argument(\"--mr_iid\", type=int, required=True, help=\"マージリクエストのIIDを指定してください。\")\n+    args = parser.parse_args()\n+\n+    # GitLab MR情報を取得\n+    diffs = get_mr_info(args.mr_iid)\n+    # レビュー観点をロード\n+    review_perspectives = load_review_perspectives()\n+    \n+    # 出力ファイルの末尾に付与する現在時刻を取得してフォーマット\n+    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n+    # レビュー結果出力ファイル名とログファイル名を生成\n+    output_file = f\"review_results_{timestamp}.md\"\n+    log_file = f\"api_call_logs_{timestamp}.json\"\n+    \n+    # 各DIFFに対してレビューを実行\n+    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n+        for diff in diffs:\n+            file_name = diff['new_path']\n+            diff_info = diff['diff']\n+            if not diff_info:\n+                continue\n+            f.write(f\"# レビュー対象：{file_name}\\n\")\n+            for perspective in review_perspectives:\n+                f.write(f\"## レビュー観点：{perspective}\\n\")\n+                review_result = review_code(perspective, file_name, diff_info, log_file)\n+                f.write(f\"{review_result}\\n\")\n+\n+if __name__ == \"__main__\":\n+    main()\n",
        "review_perspective": "パッケージ構成が適切か（例：init.pyの活用）",
        "request_tokens": 3190,
        "api_params": {
            "model": "gpt-4.1-nano",
            "temperature": 0.2
        },
        "response": "### 指摘対象コード:\n特にパッケージ構成や`__init__.py`の活用に関する記述は見受けられません。\n\n### 指摘内容:\nこのコードは単一のスクリプトファイルとして完結しており、複数のモジュールやパッケージに分割されていません。`constants`や`openai`などの外部ライブラリをインポートしていますが、これらをパッケージとして整理し、`__init__.py`を活用してモジュールの管理を行うことで、コードの拡張性や保守性が向上します。\n\n### 修正提案:\n- `constants.py`やその他の関連モジュールをパッケージとして整理し、`__init__.py`を配置して管理を行う。\n- 例えば、`review`や`utils`といったディレクトリを作成し、その中にモジュールを配置し、`__init__.py`を追加する。\n- メインのスクリプトはパッケージから必要な関数やクラスをインポートする形に変更し、コードの構造化を図る。\n\n例:\n```\nproject/\n│\n├── review/\n│   ├── __init__.py\n│   ├── code_review.py\n│   ├── constants.py\n│   └── utils.py\n│\n└── main.py\n```\n\nこれにより、パッケージ構成が明確になり、将来的な拡張や他のモジュールとの連携も容易になります。",
        "response_tokens": 448,
        "start_time": "2025/05/26 04:49:45.526279",
        "end_time": "2025/05/26 04:49:49.307907"
    },
    {
        "file_name": "code_review.py",
        "diff_content": "@@ -0,0 +1,229 @@\n+import os\n+import json\n+import requests\n+import argparse\n+import tiktoken\n+from constants import GITLAB_URL, GIT_API_PER_PAGE, PROJECT_ID, MODEL_NAME, TEMPERATURE, TIKTOKEN_MODEL, TOKENS_PER_MESSAGE, TOKENS_PER_NAME\n+from datetime import datetime\n+from openai import OpenAI\n+\n+PRIVATE_TOKEN = os.getenv(\"GITLAB_TOKEN\")\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+HEADERS = {\n+    \"PRIVATE-TOKEN\": PRIVATE_TOKEN\n+}\n+\n+CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n+TIKTOKEN_ENCODING = tiktoken.encoding_for_model(TIKTOKEN_MODEL)\n+\n+def load_review_perspectives(file_path=\"review_perspectives.json\") -> list:\n+    \"\"\"\n+    レビュー観点をJSONファイルから読み込む。\n+\n+    Args:\n+        file_path (str): レビュー観点が記載されたJSONファイルのパス。\n+\n+    Returns:\n+        list: レビュー観点のリスト。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+def load_system_prompt(file_path=\"system_prompt.txt\") -> str:\n+    \"\"\"\n+    システムプロンプトをテキストファイルから読み込む。\n+\n+    Args:\n+        file_path (str): システムプロンプトが記載されたテキストファイルのパス。\n+\n+    Returns:\n+        str: システムプロンプトの内容。\n+    \"\"\"\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        return f.read()\n+\n+def num_tokens_from_messages(messages) -> int:\n+    \"\"\"\n+    メッセージからトークン数を計算する。\n+\n+    Args:\n+        messages (list): メッセージのリスト。\n+\n+    Returns:\n+        int: メッセージに含まれるトークン数。\n+    \"\"\"\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += TOKENS_PER_MESSAGE\n+        for key, value in message.items():\n+            num_tokens += len(TIKTOKEN_ENCODING.encode(value))\n+            if key == \"name\":\n+                num_tokens += TOKENS_PER_NAME\n+    num_tokens += 3  # assistantの返信の開始トークン\n+    return num_tokens\n+\n+def _call_openai_chat_api(messages: list, model: str, temperature: float, max_tokens: int) -> str:\n+    \"\"\"\n+    OpenAIのChat APIを呼び出す関数。\n+\n+    Args:\n+        messages (list): APIに送信するメッセージリスト。\n+        model (str): 使用するモデル名。\n+        temperature (float): 出力の多様性を制御する温度パラメータ。\n+        max_tokens (int): 最大トークン数。\n+\n+    Returns:\n+        str: OpenAI APIからのレスポンス。\n+    \"\"\"\n+    response = CLIENT.chat.completions.create(\n+        model=model,\n+        messages=messages,\n+        temperature=temperature,\n+        max_tokens=max_tokens\n+    )\n+    return response.choices[0].message.content\n+\n+def review_code(review_perspective: str, file_name: str, diff_info: str, log_file: str) -> str:\n+    \"\"\"\n+    指定された観点でコードレビューを実行し、結果をログに記録する。\n+\n+    Args:\n+        review_perspective (str): レビュー観点。\n+        file_name (str): 対象ファイル名。\n+        diff_info (str): 差分情報。\n+        log_file (str): ログを保存するファイル名。\n+\n+    Returns:\n+        str: レビュー結果。\n+    \"\"\"\n+    # ファイル名と差分情報でシステムプロンプトをフォーマット\n+    current_system_prompt = load_system_prompt().format(file_name=file_name, diff_info=diff_info)\n+    messages = [\n+        {\"role\": \"system\", \"content\": current_system_prompt},\n+        {\"role\": \"user\", \"content\": review_perspective}\n+    ]\n+\n+    start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+    # OpenAI APIを呼び出してレビュー結果を取得\n+    response_str = _call_openai_chat_api(\n+        messages=messages,\n+        model=MODEL_NAME,\n+        temperature=TEMPERATURE,\n+    )\n+    end_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S.%f\")\n+\n+    log_data = {\n+        \"file_name\": file_name,\n+        \"diff_content\": diff_info,\n+        \"review_perspective\": review_perspective,\n+        \"request_tokens\": num_tokens_from_messages(messages),\n+        \"api_params\": {\"model\": MODEL_NAME, \"temperature\": TEMPERATURE},\n+        \"response\": response_str,\n+        \"response_tokens\": num_tokens_from_messages(response_str),\n+        \"start_time\": start_time,\n+        \"end_time\": end_time,\n+    }\n+\n+    # 既存のログデータをロード（なければ空リストを使う）\n+    if os.path.exists(log_file):\n+        with open(log_file, \"r\", encoding=\"utf-8\") as f:\n+            try:\n+                existing_logs = json.load(f)\n+            except json.JSONDecodeError:\n+                existing_logs = []\n+    else:\n+        existing_logs = []\n+\n+    # 新しいログデータを追加\n+    existing_logs.append(log_data)\n+    \n+    # ログをJSON形式で保存\n+    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n+        json.dump(existing_logs, f, ensure_ascii=False, indent=4)\n+    return response_str\n+\n+def get_mr_info(mr_iid) -> list:\n+    \"\"\"\n+    GitLabのマージリクエスト情報を取得する。\n+\n+    Args:\n+        mr_iid (int): マージリクエストのIID。\n+\n+    Returns:\n+        list: マージリクエストの差分情報リスト。\n+    \"\"\"\n+    base = f\"{GITLAB_URL}/api/v4\"\n+    # 1. MR基本情報\n+    mr_api_base_url = f\"{base}/projects/{PROJECT_ID}/merge_requests/{mr_iid}\"\n+    # 2. 差分情報（Diff）\n+    #    MRの差分情報を取得するエンドポイント\n+    diffs_endpoint_url = f\"{mr_api_base_url}/diffs\"\n+\n+    all_diffs = []\n+    current_page = 1\n+\n+    while True:\n+        params = {\"page\": current_page, \"per_page\": GIT_API_PER_PAGE}\n+        response = requests.get(diffs_endpoint_url, headers=HEADERS, params=params)\n+        try:\n+            response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n+        except requests.exceptions.HTTPError as e:\n+            print(f\"APIリクエストに失敗しました: {e}\")\n+            break \n+        try:\n+            page_data = response.json()\n+        except requests.exceptions.JSONDecodeError:\n+            print(f\"APIレスポンスのJSONデコードに失敗しました。ステータス: {response.status_code}, 本文: {response.text}\")\n+            break\n+        if not isinstance(page_data, list):\n+            print(f\"APIが差分情報のリストを返しませんでした。レスポンス: {page_data}\")\n+            break\n+        if not page_data:  # 現在のページにデータがなければ、全ページ取得済み\n+            break\n+\n+        all_diffs.extend(page_data)\n+\n+        # x-total-pages ヘッダーで総ページ数を確認 (存在する場合)\n+        total_pages_header = response.headers.get(\"x-total-pages\")\n+        if total_pages_header and current_page >= int(total_pages_header):\n+            break # 最後のページまで到達\n+        current_page += 1\n+    return all_diffs\n+\n+def main():\n+    \"\"\"\n+    コマンドライン引数で指定したマージリクエストのIIDからGitLab APIで差分情報を取得し、\n+    その差分情報をOpenAI APIに送信してレビュー結果をマークダウン形式で出力する。\n+    \"\"\"\n+    # コマンドライン引数を解析\n+    parser = argparse.ArgumentParser(description=\"GitLab MR情報を取得してコードレビューを実行します。\")\n+    parser.add_argument(\"--mr_iid\", type=int, required=True, help=\"マージリクエストのIIDを指定してください。\")\n+    args = parser.parse_args()\n+\n+    # GitLab MR情報を取得\n+    diffs = get_mr_info(args.mr_iid)\n+    # レビュー観点をロード\n+    review_perspectives = load_review_perspectives()\n+    \n+    # 出力ファイルの末尾に付与する現在時刻を取得してフォーマット\n+    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n+    # レビュー結果出力ファイル名とログファイル名を生成\n+    output_file = f\"review_results_{timestamp}.md\"\n+    log_file = f\"api_call_logs_{timestamp}.json\"\n+    \n+    # 各DIFFに対してレビューを実行\n+    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n+        for diff in diffs:\n+            file_name = diff['new_path']\n+            diff_info = diff['diff']\n+            if not diff_info:\n+                continue\n+            f.write(f\"# レビュー対象：{file_name}\\n\")\n+            for perspective in review_perspectives:\n+                f.write(f\"## レビュー観点：{perspective}\\n\")\n+                review_result = review_code(perspective, file_name, diff_info, log_file)\n+                f.write(f\"{review_result}\\n\")\n+\n+if __name__ == \"__main__\":\n+    main()\n",
        "review_perspective": "実行して期待通りの挙動をするか（ロジックのバグがないか）",
        "request_tokens": 3193,
        "api_params": {
            "model": "gpt-4.1-nano",
            "temperature": 0.2
        },
        "response": "この観点では問題ありません。",
        "response_tokens": 13,
        "start_time": "2025/05/26 04:49:49.320009",
        "end_time": "2025/05/26 04:49:49.919412"
    }
]